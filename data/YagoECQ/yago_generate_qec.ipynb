{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kevin/code/rycolab/measureLM/data/YagoECQ\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from SPARQLWrapper import SPARQLWrapper2\n",
    "from SPARQLWrapper.SPARQLExceptions import EndPointInternalError\n",
    "from collections import namedtuple\n",
    "import re\n",
    "from time import sleep\n",
    "from urllib.error import HTTPError\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# from preprocessing.utils import extract_name_from_yago_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicate = namedtuple(\"Predicate\", [\"uri\", \"kb_name\", \"relation\"])\n",
    "DATA_ROOT = \".\"\n",
    "PRED_URI_TO_S_OBJ_CLASSES_PATH = os.path.join(DATA_ROOT, \"yago_pred_uri_to_s_obj_classes.json\")\n",
    "PRED_URI_TO_SO_PAIRS_PATH = os.path.join(DATA_ROOT, \"yago_pred_uri_to_so_pairs_randomized_1k.json\")\n",
    "YAGO_QEC_PATH = os.path.join(DATA_ROOT, \"yago_qec.json\")\n",
    "TRY_QUERYING_MISSING_PREDS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name_from_yago_uri(uri: str):\n",
    "    is_reversed = False\n",
    "    if uri.startswith(\"reverse-\"):\n",
    "        uri = uri.split(\"reverse-\")[1]\n",
    "        is_reversed = True\n",
    "    pattern = r\"http://(?:www\\.)?([^\\/]+)\\.org\\/(.+)$\"\n",
    "    matches = re.match(pattern, uri)\n",
    "\n",
    "    if matches:\n",
    "        kb_domain = matches.group(1)\n",
    "        relation = matches.group(2)\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find match containing kb_domain and relation for uri {uri}.\")\n",
    "\n",
    "    domain_to_name = {\"schema\": \"schema\", \"yago-knowledge\": \"yago\", \"w3\": \"w3\"}\n",
    "    kb_name = domain_to_name[kb_domain]\n",
    "    kb_name = (\"reverse-\" if is_reversed else \"\") + kb_name\n",
    "\n",
    "    return kb_name, relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# 1. Get all relevant predicates #\n",
    "##################################\n",
    "sparql = SPARQLWrapper2(\"https://yago-knowledge.org/sparql/query\")\n",
    "query_p = \"\"\"\n",
    "    SELECT DISTINCT ?p WHERE {\n",
    "     ?s ?p ?obj . \n",
    "    }  ORDER BY ?p\n",
    "\"\"\"\n",
    "\n",
    "# Sparql query\n",
    "sparql.setQuery(query_p)\n",
    "\n",
    "# Adding values\n",
    "relevant_preds: List[Predicate] = []\n",
    "ineligible_relations = [\"schema#fromClass\", \"schema#fromProperty\", \"logo\", \"image\"]\n",
    "\n",
    "for result in sparql.query().bindings:\n",
    "    uri = result[\"p\"].value\n",
    "    kb_name, relation = extract_name_from_yago_uri(uri)\n",
    "    if kb_name != \"w3\" and relation not in ineligible_relations:\n",
    "        relevant_preds.append(Predicate(uri=uri, kb_name=kb_name, relation=relation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raising error for obj: 20, place des Terreaux\n",
      "Raising error for obj: Neuschwansteinstraße 20\n",
      "Raising error for obj: Viale del Fante 11, I-90146 Palermo\n",
      "Raising error for obj: The University of Hong Kong, Pokfulam, Hong Kong\n",
      "Raising error for obj: Piazza Giuseppe Mazzini, 21\n",
      "Raising error for obj: Statene 2, 5970 Ærøskøbing\n",
      "Raising error for obj: Strada Comunale Santa Margherita, 79\n",
      "Raising error for obj: Strada Santa Margherita 79, 10131 Torino\n",
      "Raising error for obj: Ареспублика Мали\n",
      "Raising error for obj: Ареспублика Сиерра-Леоне\n",
      "Raising error for obj: Ареспублика Судан\n",
      "Raising error for obj: Китаитәи Жәлар рреспублика\n",
      "Raising error for obj: اللوغة الإنڨليزية\n",
      "Raising error for obj: Andrei Dmitrijewitsj Sacharof\n",
      "Raising error for obj: Point(-1.0833333333333 53.958333333333)\n",
      "Raising error for obj: Point(-1.6808333333333 48.114166666667)\n",
      "Raising error for obj: Point(-106 34)\n",
      "Raising error for obj: Point(-106 34)\n",
      "Raising error for obj: Point(-113.5 53.533333333333)\n",
      "Raising error for obj: Point(-12.1 8.5)\n",
      "Raising error for obj: Point(-2.1844444444444 47.27)\n",
      "Raising error for obj: Point(-4 17)\n",
      "Raising error for obj: Point(-53 -14)\n",
      "Raising error for obj: Point(-6.3561111111111 57.302777777778)\n",
      "Raising error for obj: All are welcome\n",
      "Raising error for obj: All together for one\n",
      "Raising error for obj: Bumi Serumpun Sebalai\n",
      "Raising error for obj: By Right or Might\n",
      "Raising error for obj: China Like Never Before\n",
      "Raising error for obj: Crescit eundo\n",
      "Raising error for obj: Durch Überzeugung oder mit Gewalt\n",
      "Raising error for obj: Eendracht maakt macht\n",
      "Raising error for obj: Ein Volk, ein Ziel, ein Glaube\n",
      "Raising error for obj: Einigkeit und Recht und Freiheit\n",
      "Raising error for obj: 30xxx, 31xxx, 32xxx\n",
      "Raising error for obj: 30xxx, 31xxx, 32xxx\n",
      "Raising error for obj: 33111 - 33792\n",
      "Raising error for obj: 440 001 – 440 037\n",
      "Raising error for obj: 8 (Inter Milan)\n",
      "Raising error for obj: 4, 11, 14, 20 y 23\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# 1b. Get all subject and object types for each predicate #\n",
    "###########################################################\n",
    "from SPARQLWrapper.SPARQLExceptions import QueryBadFormed\n",
    "query_single = \"\"\"\n",
    "    SELECT DISTINCT ?s ?obj WHERE {{\n",
    "     ?s <{}> ?obj . \n",
    "    }}  LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "query_superclasses = \"\"\"\n",
    "    SELECT DISTINCT ?superclasses WHERE {{\n",
    "     <{}> rdf:type/rdfs:subClassOf* ?superclasses .\n",
    "    }}  \n",
    "\"\"\"\n",
    "\n",
    "types = [\n",
    "    \"http://schema.org/CreativeWork\",\n",
    "    \"http://schema.org/Event\",\n",
    "    \"http://schema.org/Intangible\",\n",
    "    \"http://schema.org/Organization\",\n",
    "    \"http://schema.org/Person\",\n",
    "    \"http://schema.org/Place\",\n",
    "    \"http://schema.org/Product\",\n",
    "    \"http://schema.org/Taxon\",\n",
    "    \"http://schema.org/FictionalEntity\",\n",
    "]\n",
    "if os.path.exists(PRED_URI_TO_S_OBJ_CLASSES_PATH):\n",
    "    print(f\"Loading pred_to_s_and_obj_types from file {PRED_URI_TO_S_OBJ_CLASSES_PATH}.\")\n",
    "    with open(PRED_URI_TO_S_OBJ_CLASSES_PATH) as f:\n",
    "        pred_to_s_and_obj_types_with_reverse = json.load(f)\n",
    "else:\n",
    "    pred_to_s_and_obj_types = dict()\n",
    "    for pred in relevant_preds:\n",
    "        concrete_query_single = query_single.format(pred.uri)\n",
    "        # print(query)\n",
    "        sparql.setQuery(concrete_query_single)\n",
    "        superclasses_s = []\n",
    "        superclasses_obj = []\n",
    "        for result in sparql.query().bindings:\n",
    "            s = result[\"s\"].value\n",
    "            obj = result[\"obj\"].value\n",
    "            try:\n",
    "                concrete_query_superclasses_s = query_superclasses.format(s)\n",
    "                sparql.setQuery(concrete_query_superclasses_s)\n",
    "                superclasses_s += [x[\"superclasses\"].value for x in sparql.query().bindings]\n",
    "            except QueryBadFormed as e:\n",
    "                print(\"Raising error for s:\", s)\n",
    "                # raise ValueError\n",
    "            try:\n",
    "                concrete_query_superclasses_obj = query_superclasses.format(obj)\n",
    "                sparql.setQuery(concrete_query_superclasses_obj)\n",
    "                superclasses_obj += [x[\"superclasses\"].value for x in sparql.query().bindings]\n",
    "            except QueryBadFormed:\n",
    "                print(\"Raising error for obj:\", obj)\n",
    "                # raise ValueError\n",
    "            \n",
    "        pred_to_s_and_obj_types[pred] = (set(superclasses_s), set(superclasses_obj))\n",
    "            \n",
    "    pred_to_s_and_obj_types = {k.uri: (list(s_set.intersection(types)), list(obj_set.intersection(types))) for k, (s_set, obj_set) in pred_to_s_and_obj_types.items()}\n",
    "    pred_to_s_and_obj_types_with_reverse = {**pred_to_s_and_obj_types, **{f\"reverse-{k}\": (obj, s) for k, (s, obj) in pred_to_s_and_obj_types.items()}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# 2. Extract all subject-object pairs for each pred in relevant preds #\n",
    "#######################################################################\n",
    "def get_so_pairs_for_pred(pred: Predicate) -> List[Tuple[str, str, str, str]]:\n",
    "    start_time = time.time()\n",
    "    print(f\"Querying {pred.uri}.\")\n",
    "    so_pairs = []\n",
    "\n",
    "    query = \"\"\"\n",
    "SELECT DISTINCT ?output_s ?output_obj ?s ?obj WHERE {{\n",
    "  ?s <{}> ?obj . # which predicate to use\n",
    "  OPTIONAL {{ \n",
    "    ?s rdfs:label ?s_label .\n",
    "    FILTER (LANG(?s_label) = 'en')\n",
    "  }} # Get the label (name) for the subject if it exists\n",
    "  BIND(COALESCE(?s_label, ?s) AS ?output_s) # if the label does not exist, stick with the URI\n",
    "  \n",
    "  OPTIONAL {{\n",
    "    ?obj rdfs:label ?obj_label . \n",
    "    FILTER (LANG(?obj_label) = 'en')\n",
    "  }} # Get the label (name) for the object if it exists\n",
    "  OPTIONAL {{\n",
    "    ?obj rdf:type ?obj_type . \n",
    "    ?obj_type rdfs:label ?obj_type_name .\n",
    "    FILTER (LANG(?obj_type_name) = 'en')\n",
    "  }} # get the name of the type for the object if the type exists\n",
    "  BIND(COALESCE(IF(STR(?obj_label) != \"Generic instance\", ?obj_label, ?obj_type_name), ?obj) AS ?output_obj) \n",
    "  # if the label exists, go with the label, but if it's \"Generic instance\", then go with the type; if that does not exist, then stick with the OG object.\n",
    "  BIND(MD5(CONCAT(STR(?s), STR(?obj))) AS ?sortkey) .\n",
    "}}\n",
    "ORDER BY ?sortkey\n",
    "LIMIT 1000\n",
    "\"\"\".format(\n",
    "        pred.uri\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        sparql.setQuery(query)\n",
    "        for result in sparql.query().bindings:\n",
    "            so_pairs.append((result[\"output_s\"].value, result[\"output_obj\"].value, result[\"s\"].value, result[\"obj\"].value))\n",
    "    # sleep(1)\n",
    "    except (HTTPError, EndPointInternalError) as e:\n",
    "        print(f\"HTTPerror for uri {pred.uri}. Trying chained query.\")\n",
    "        so_pairs = get_so_pairs_for_pred_chained(pred)\n",
    "    finally:\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f\"Time elapsed: {time_elapsed}\")\n",
    "        return so_pairs\n",
    "    \n",
    "def get_so_pairs_for_pred_chained(pred: Predicate) -> List[Tuple[str, str, str, str]]:\n",
    "    start_time = time.time()\n",
    "    print(f\"Querying {pred.uri} (chained).\")\n",
    "    so_pairs = []\n",
    "\n",
    "    query = \"\"\"\n",
    "SELECT DISTINCT ?s ?o WHERE {{\n",
    "    ?s <{}> ?o\n",
    "    BIND(MD5(CONCAT(STR(?s), STR(?o))) AS ?sortkey) .\n",
    "}}\n",
    "ORDER BY ?sortkey\n",
    "LIMIT 1000\n",
    "\"\"\".format(\n",
    "        pred.uri\n",
    "    )\n",
    "    \n",
    "    query_get_name_of_obj = \"\"\"\n",
    "SELECT DISTINCT ?output_obj WHERE {{\n",
    "    OPTIONAL {{\n",
    "      <{0}> rdfs:label ?obj_label . \n",
    "      FILTER (LANG(?obj_label) = 'en')\n",
    "    }} # Get the label (name) for the object if it exists\n",
    "    OPTIONAL {{\n",
    "      <{0}> rdf:type ?obj_type . \n",
    "      ?obj_type rdfs:label ?obj_type_name .\n",
    "      FILTER (LANG(?obj_type_name) = 'en')\n",
    "    }} # get the name of the type for the object if the type exists\n",
    "    BIND(COALESCE(IF(STR(?obj_label) != \"Generic instance\", ?obj_label, ?obj_type_name), <{0}>) AS ?output_obj) \n",
    "}}  \n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        sparql.setQuery(query)\n",
    "        for result in sparql.query().bindings:\n",
    "            # print(result)\n",
    "            s_uri = result[\"s\"].value\n",
    "            o_uri = result[\"o\"].value\n",
    "            query_s = query_get_name_of_obj.format(s_uri)            \n",
    "            sparql.setQuery(query_s)\n",
    "            res_s = sparql.query().bindings\n",
    "            if len(res_s) != 1:\n",
    "                raise ValueError(f\">1 label returned for subject {s_uri}: {res_s}\")\n",
    "            s_name = res_s[0][\"output_obj\"].value\n",
    "\n",
    "            query_obj = query_get_name_of_obj.format(o_uri)\n",
    "            sparql.setQuery(query_obj)\n",
    "            res_obj = sparql.query().bindings\n",
    "            if len(res_obj) != 1:\n",
    "                raise ValueError(f\">1 label returned for subject {o_uri}: {res_obj}\")\n",
    "            obj_name = res_obj[0][\"output_obj\"].value\n",
    "\n",
    "            so_pairs.append((s_name, obj_name, s_uri, o_uri))\n",
    "        \n",
    "    # sleep(1)\n",
    "    except (HTTPError, EndPointInternalError) as e:\n",
    "        print(f\"HTTPerror for uri {pred.uri} when chaining. Skipping.\")\n",
    "        so_pairs = None\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f\"Time elapsed: {time_elapsed}\")\n",
    "        return so_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying http://schema.org/about.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 13.01859974861145\n",
      "Missing preds: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(PRED_URI_TO_SO_PAIRS_PATH):\n",
    "    print(f\"Loading pred_uri_to_so_pairs from file {PRED_URI_TO_SO_PAIRS_PATH}.\")\n",
    "    with open(PRED_URI_TO_SO_PAIRS_PATH) as f:\n",
    "        pred_uri_to_so_pairs = json.load(f)\n",
    "else:\n",
    "    pred_to_so_pairs: Dict[Predicate, List[Tuple[str, str, str, str]]] = {\n",
    "        pred: get_so_pairs_for_pred(pred) for pred in tqdm(relevant_preds)\n",
    "    }\n",
    "    pred_to_so_pairs = {k: v for k, v in pred_to_so_pairs.items() if v is not None}\n",
    "    pred_uri_to_so_pairs = {\n",
    "        k.uri: v for k, v in pred_to_so_pairs.items() if v is not None\n",
    "    }\n",
    "\n",
    "if TRY_QUERYING_MISSING_PREDS:\n",
    "    missing_preds = [p for p in relevant_preds if p.uri not in pred_uri_to_so_pairs]\n",
    "    print(\"Missing preds:\", missing_preds)\n",
    "    missing_pred_to_so_pairs: Dict[Predicate, List[Tuple[str, str, str, str]]] = {\n",
    "        pred: get_so_pairs_for_pred(pred) for pred in tqdm(missing_preds)\n",
    "    }\n",
    "    missing_pred_to_so_pairs = {k: v for k, v in missing_pred_to_so_pairs.items() if v is not None}\n",
    "    missing_pred_uri_to_so_pairs = {\n",
    "        k.uri: v for k, v in missing_pred_to_so_pairs.items() if v is not None\n",
    "    }\n",
    "\n",
    "    pred_uri_to_so_pairs = {**missing_pred_uri_to_so_pairs, **pred_uri_to_so_pairs}\n",
    "\n",
    "with open(PRED_URI_TO_SO_PAIRS_PATH, \"w\", encoding='utf-8') as fp:\n",
    "    json.dump(pred_uri_to_so_pairs, fp, ensure_ascii=False, indent=4)\n",
    "\n",
    "def augment_pred_uri_to_so_pairs_with_reverse(pred_uri_to_so_pairs):\n",
    "    return {\n",
    "        **pred_uri_to_so_pairs,\n",
    "        **{\n",
    "            f\"reverse-{k}\": [(a_label, e_label, a_uri, e_uri) for (e_label, a_label, e_uri, a_uri) in v]\n",
    "            for k, v in pred_uri_to_so_pairs.items()\n",
    "        },\n",
    "    }\n",
    "\n",
    "pred_uri_to_so_pairs_with_reverse = augment_pred_uri_to_so_pairs_with_reverse(\n",
    "    pred_uri_to_so_pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http://schema.org/about'}\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################\n",
    "# 4. Construct queries containing entities, corresponding answers, query forms, and context templates. #\n",
    "########################################################################################################\n",
    "from yago_questions import yago_topic_to_qfs\n",
    "\n",
    "keys = set(yago_topic_to_qfs).intersection(set(pred_uri_to_so_pairs_with_reverse))\n",
    "print(keys)\n",
    "yago_qec = {\n",
    "    k: {\n",
    "        \"query_forms\": yago_topic_to_qfs[k],\n",
    "        \"entities\": list(zip(*pred_uri_to_so_pairs_with_reverse[k]))[0],\n",
    "        \"answers\": list(zip(*pred_uri_to_so_pairs_with_reverse[k]))[1],\n",
    "        \"entity_uris\": list(zip(*pred_uri_to_so_pairs_with_reverse[k]))[2],\n",
    "        \"answer_uris\": list(zip(*pred_uri_to_so_pairs_with_reverse[k]))[3],\n",
    "        \"context_templates\": [yago_topic_to_qfs[k][\"open\"][-1] + \" {answer}.\\n\"],\n",
    "        \"entity_types\": pred_to_s_and_obj_types_with_reverse[k][0], \n",
    "        \"answer_types\": pred_to_s_and_obj_types_with_reverse[k][1], \n",
    "    }\n",
    "    for k in keys\n",
    "}\n",
    "yago_qec\n",
    "with open(YAGO_QEC_PATH, \"w\", encoding='utf-8') as fp:\n",
    "    json.dump(yago_qec, fp, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(YAGO_QEC_PATH, \"r\") as fp:\n",
    "    yago_qec_reloaded = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid, v in yago_qec.items():\n",
    "    print(qid, len(v[\"entities\"]), len(set(v[\"entities\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    yago_qec[\"reverse-http://schema.org/homeLocation\"][\"entities\"]\n",
    ").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# yago_qec_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [x for x in yago_qec_reloaded['http://schema.org/icaoCode'][\"entities\"] if \"Egilssta\" in x][0]\n",
    "print(s, type(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Get the degree of each entity #\n",
    "#################################\n",
    "entity_name_to_degree = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# 5. Construct fake entities #\n",
    "##############################\n",
    "# This section requires GPUs #\n",
    "##############################\n",
    "\n",
    "# Can be run separate from previous section #\n",
    "from transformers import ReformerModelWithLMHead\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import itertools\n",
    "from transformers import ReformerModelWithLMHead\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Encoding\n",
    "def encode(list_of_strings, pad_token_id=0, device=\"cpu\"):\n",
    "    max_length = max([len(string) for string in list_of_strings])\n",
    "\n",
    "    # create emtpy tensors\n",
    "    attention_masks = torch.zeros((len(list_of_strings), max_length), dtype=torch.long)\n",
    "    input_ids = torch.full((len(list_of_strings), max_length), pad_token_id, dtype=torch.long)\n",
    "\n",
    "    for idx, string in enumerate(list_of_strings):\n",
    "        # make sure string is in byte format\n",
    "        if not isinstance(string, bytes):\n",
    "            string = str.encode(string)\n",
    "\n",
    "        input_ids[idx, :len(string)] = torch.tensor([x + 2 for x in string])\n",
    "        attention_masks[idx, :len(string)] = 1\n",
    "\n",
    "    return input_ids.to(device), attention_masks.to(device)\n",
    "    \n",
    "# Decoding\n",
    "def decode(outputs_ids):\n",
    "    decoded_outputs = []\n",
    "    for output_ids in outputs_ids.tolist():\n",
    "        # transform id back to char IDs < 2 are simply transformed to \"\"\n",
    "        decoded_outputs.append(\"\".join([chr(x - 2) if x > 1 else \"\" for x in output_ids]))\n",
    "    return decoded_outputs\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def extract_entity(text):\n",
    "    regex_pattern = r'\\[\\[([^\\[|\\]]+?)(?:\\([^)]*\\))?(?:\\|[^|\\]]+)*\\]\\]'\n",
    "    match = re.search(regex_pattern, text)\n",
    "\n",
    "    if match:\n",
    "        result = match.group(1).strip()\n",
    "        return result\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"http://schema.org/Person\": 17352,\n",
      "    \"http://schema.org/Product\": 10715,\n",
      "    \"http://schema.org/CreativeWork\": 13410,\n",
      "    \"http://schema.org/Place\": 12723,\n",
      "    \"http://schema.org/Organization\": 14291,\n",
      "    \"http://schema.org/Event\": 1990\n",
      "}\n",
      "45054\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, Set\n",
    "\n",
    "with open(YAGO_QEC_PATH, \"r\") as fp:\n",
    "    yago_qec = json.load(fp)\n",
    "entity_types_to_real_entities: Dict[str, Set[str]] = defaultdict(set)\n",
    "for query_id, qec in yago_qec.items():\n",
    "    for et in qec[\"entity_types\"]:\n",
    "        entity_types_to_real_entities[et] = entity_types_to_real_entities[et].union(qec[\"entities\"])\n",
    "print(json.dumps({k: len(v) for k, v in entity_types_to_real_entities.items()}, indent=4))\n",
    "\n",
    "real_entities = set(itertools.chain.from_iterable([v[\"entities\"] for _, v in yago_qec.items()]))\n",
    "print(len(real_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_types_to_prompts = {\n",
    "    \"http://schema.org/CreativeWork\": \"The creative work called [[\",\n",
    "    \"http://schema.org/Event\": \"The event called [[\",\n",
    "    \"http://schema.org/Intangible\": \"The concept called [[\",\n",
    "    \"http://schema.org/Organization\": \"The organization is called [[\",\n",
    "    \"http://schema.org/Person\": \"The person named [[\",\n",
    "    \"http://schema.org/Place\": \"The place is named [[\",\n",
    "    \"http://schema.org/Product\": \"The product is called [[\",\n",
    "    \"http://schema.org/Taxon\": \"The taxon named [[\",\n",
    "    \"http://schema.org/FictionalEntity\": \"The fictional entity named [[\",\n",
    "}\n",
    "entity_types_to_fake_entities = {}\n",
    "model = ReformerModelWithLMHead.from_pretrained(\"google/reformer-enwik8\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***http://schema.org/CreativeWork***\n",
      "# unique fake ents: 1148\n",
      "# unique fake ents removing reals: 1126\n",
      "***http://schema.org/Event***\n",
      "# unique fake ents: 1159\n",
      "# unique fake ents removing reals: 1141\n",
      "***http://schema.org/Intangible***\n",
      "# unique fake ents: 1071\n",
      "# unique fake ents removing reals: 1068\n",
      "***http://schema.org/Organization***\n",
      "# unique fake ents: 1039\n",
      "# unique fake ents removing reals: 1017\n",
      "***http://schema.org/Person***\n",
      "# unique fake ents: 1177\n",
      "# unique fake ents removing reals: 1152\n",
      "***http://schema.org/Place***\n",
      "# unique fake ents: 1124\n",
      "# unique fake ents removing reals: 1082\n",
      "***http://schema.org/Product***\n",
      "# unique fake ents: 1109\n",
      "# unique fake ents removing reals: 1097\n",
      "***http://schema.org/Taxon***\n",
      "# unique fake ents: 1147\n",
      "# unique fake ents removing reals: 1126\n",
      "***http://schema.org/FictionalEntity***\n",
      "# unique fake ents: 1149\n",
      "# unique fake ents removing reals: 1118\n"
     ]
    }
   ],
   "source": [
    "for et, p in entity_types_to_prompts.items():\n",
    "    set_seed(0)\n",
    "    print(f\"***{et}***\")\n",
    "    encoded, attention_masks = encode([p], device=device)\n",
    "    res = decode(model.generate(encoded, do_sample=True, num_return_sequences=1200, max_length=100))\n",
    "\n",
    "    extracted_res = {extract_entity(s) for s in res if extract_entity(s) is not None}\n",
    "    print(f\"# unique fake ents: {len(extracted_res)}\")\n",
    "    extracted_res_without_reals = extracted_res.difference(real_entities)\n",
    "    print(f\"# unique fake ents removing reals: {len(extracted_res_without_reals)}\")\n",
    "    extracted_res = random.sample(list(extracted_res), 1000)\n",
    "    entity_types_to_fake_entities[et] = extracted_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAGO_FAKE_ENTITIES_PATH = os.path.join(DATA_ROOT, \"fake_entities.json\") \n",
    "\n",
    "# Load yago_qec\n",
    "with open(YAGO_QEC_PATH, \"r\") as fp:\n",
    "    yago_qec = json.load(fp)\n",
    "\n",
    "# Save fake entities per each entity type.\n",
    "with open(YAGO_FAKE_ENTITIES_PATH, \"w\", encoding='utf-8') as fp:\n",
    "    json.dump(entity_types_to_fake_entities, fp, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Randomly sample fake entities that are eligible according to entity type for each relation and save to yago_qec\n",
    "for k, v in yago_qec.items():\n",
    "    entity_types = yago_qec[k][\"entity_types\"]\n",
    "    eligible_fake_entities = list(itertools.chain.from_iterable([entity_types_to_fake_entities[et] for et in entity_types]))\n",
    "    yago_qec[k][\"fake_entities\"] = random.sample(eligible_fake_entities, len(yago_qec[k][\"entities\"]))\n",
    "\n",
    "# Save yago_qec including fake entities\n",
    "with open(YAGO_QEC_PATH, \"w\", encoding='utf-8') as fp:\n",
    "    json.dump(yago_qec, fp, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# 6. Construct fake entities with chatgpt #\n",
    "###########################################\n",
    "import pandas as pd\n",
    "import random\n",
    "YAGO_GPT_FAKE_ENTITIES_PATH = os.path.join(DATA_ROOT, \"chatgpt_fake_entities.csv\") \n",
    "\n",
    "fake_entities_gpt = pd.read_csv(YAGO_GPT_FAKE_ENTITIES_PATH)\n",
    "fake_entities_gpt = set(fake_entities_gpt[\"FirstName\"] + \" \" + fake_entities_gpt[\"LastName\"])\n",
    "\n",
    "# Load yago_qec\n",
    "with open(YAGO_QEC_PATH, \"r\") as fp:\n",
    "    yago_qec = json.load(fp)\n",
    "\n",
    "# Randomly sample fake entities that are eligible according to entity type for each relation and save to yago_qec\n",
    "for k, v in yago_qec.items():\n",
    "    # entity_types = yago_qec[k][\"entity_types\"]\n",
    "    # eligible_fake_entities = list(itertools.chain.from_iterable([entity_types_to_fake_entities[et] for et in entity_types]))\n",
    "    eligible_fake_entities = list(fake_entities_gpt)\n",
    "    yago_qec[k][\"gpt_fake_entities\"] = random.sample(eligible_fake_entities, min(len(yago_qec[k][\"entities\"]), len(eligible_fake_entities)))\n",
    "\n",
    "# Save yago_qec including fake entities\n",
    "with open(YAGO_QEC_PATH, \"w\", encoding='utf-8') as fp:\n",
    "    json.dump(yago_qec, fp, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
