{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eaf87a4",
   "metadata": {},
   "source": [
    "# Early decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a01af",
   "metadata": {},
   "source": [
    "## Decoder-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f50b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, DebertaForMaskedLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2-medium')\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443e4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arg = \"Canada\"\n",
    "#prompt = [f'On a scale from 0 to 10, the size of Andorra is a 0 and Russia is a 10.\\\n",
    "#The size of {arg} is a']\n",
    "\n",
    "#arg = \"Russia\"\n",
    "#prompt = [f'Compared to the size of the country Belgium, the size of {arg} is a']\n",
    "\n",
    "arg = \"adore\"\n",
    "asw = [\"positive\", \"negative\"]\n",
    "prompt = [f'Q: What is the sentiment of \"love\"?\\nA: positive\\nQ: What is the sentiment of \"hate\"?\\nA: negative\\\n",
    "          \\nQ: What is the sentiment of \"{arg}\"?\\nA:']\n",
    "\n",
    "#arg = \"Donald Trump and Joe Biden\"\n",
    "#prompt = [f'Q: What is the relationship between Trump and Biden? A: negative Q: What is the relationship between Romeo and Julia? A:']\n",
    "\n",
    "#arg = \"bad\"\n",
    "#prompt = [f'Q: What is the sentiment of \"love\"?\\nA: 10\\nQ: What is the sentiment of \"hate\"?\\nA: 0\\\n",
    "#          \\nQ: What is the sentiment of \"{arg}\"?\\nA:']\n",
    "\n",
    "#prompt = [f'Today I abandon. Yesterday I abandoned. Today I abolish. Yesterday I']\n",
    "#prompt= [\"Q: What is the capital of Peru? A: Lima Q: What is the capital of Germany? A:\"]\n",
    "\n",
    "token_input = tokenizer(prompt, return_tensors='pt')\n",
    "output = model(**token_input,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb226cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0: [(5.75, ':'), (4.7, ':\"'), (4.67, ':['), (4.58, ':]'), (4.54, ':-')]\n",
      "layer 1: [(23.16, 'ĠCitiz'), (22.47, 'ĠPetr'), (20.42, 'ifer'), (19.57, 'ĠDag'), (19.4, 'Ġappend')]\n",
      "layer 2: [(24.85, 'ĠCitiz'), (24.05, 'ĠPetr'), (21.54, 'Ġont'), (21.51, 'Ġappend'), (21.5, 'ifer')]\n",
      "layer 3: [(22.65, 'ĠPetr'), (20.98, 'ĠCitiz'), (20.97, 'Ġsubsc'), (20.71, 'Ġmum'), (20.57, 'Ġtion')]\n",
      "layer 4: [(24.48, 'ĠPetr'), (22.5, 'Ġmum'), (21.24, 'ĠCitiz'), (20.88, 'ifer'), (20.68, 'ĠPutin')]\n",
      "layer 5: [(24.9, 'ĠPetr'), (23.1, 'ĠGuinea'), (21.94, 'orate'), (21.38, 'Ġrevolutions'), (21.01, 'ĠAzerb')]\n",
      "layer 6: [(25.51, 'mi'), (23.49, 'ĠPetr'), (23.24, 'orate'), (23.06, 'ĠGuinea'), (21.89, 'hene')]\n",
      "layer 7: [(21.05, 'mi'), (17.92, 'ĠPlace'), (17.14, 'Ġrevolutions'), (17.03, 'Ġmum'), (16.77, 'ĠPetr')]\n",
      "layer 8: [(17.41, 'Ġn'), (14.74, 'ĠPetr'), (14.69, 'Ġnil'), (13.62, 'mi'), (13.22, 'Ġnone')]\n",
      "layer 9: [(17.23, 'Ġn'), (14.51, 'Ġnil'), (11.61, 'Ġinst'), (10.95, 'Ġtion'), (10.52, 'ĠPlace')]\n",
      "layer 10: [(21.93, 'Ġn'), (21.52, 'Ġnil'), (16.45, 'Ġest'), (14.74, 'Ġnone'), (13.45, 'Ġcinem')]\n",
      "layer 11: [(18.69, 'Ġnil'), (18.31, 'Ġn'), (14.54, 'Ġmixed'), (12.78, 'Ġnone'), (12.07, 'Ġpositive')]\n",
      "layer 12: [(21.16, 'Ġmixed'), (20.26, 'Ġnil'), (18.0, 'Ġnone'), (15.85, 'Ġpositive'), (15.65, 'Ġnegative')]\n",
      "layer 13: [(20.65, 'Ġmixed'), (13.93, 'Ġpositive'), (13.62, 'Ġnone'), (12.51, 'Ġnil'), (12.51, 'Ġyes')]\n",
      "layer 14: [(15.42, 'Ġn'), (13.31, 'Ġ('), (12.9, 'Ġmixed'), (12.88, 'ĠN'), (12.15, 'ĠA')]\n",
      "layer 15: [(24.21, 'Ġn'), (19.77, 'Ġ('), (19.61, 'Ġpositive'), (17.04, 'Ġmixed'), (16.49, 'Ġyes')]\n",
      "layer 16: [(31.08, 'Ġthe'), (30.94, 'Ġ('), (30.37, 'Ġn'), (30.21, 'Ġa'), (28.82, 'Ġ\"')]\n",
      "layer 17: [(42.14, 'Ġa'), (42.03, 'Ġthe'), (41.58, 'Ġ('), (41.3, 'Ġ\"'), (34.46, 'Ġn')]\n",
      "layer 18: [(54.13, 'Ġ\"'), (53.46, 'Ġthe'), (51.6, 'Ġa'), (49.7, 'Ġ('), (43.31, 'ĠI')]\n",
      "layer 19: [(69.72, 'Ġ\"'), (68.04, 'Ġthe'), (67.76, 'Ġa'), (64.16, 'Ġ('), (56.66, 'ĠI')]\n",
      "layer 20: [(108.09, 'Ġ\"'), (104.67, 'Ġa'), (104.62, 'Ġthe'), (94.71, 'Ġ('), (81.74, ',')]\n",
      "layer 21: [(140.73, 'Ġ\"'), (134.89, 'Ġthe'), (130.31, 'Ġa'), (119.47, 'Ġ('), (106.2, ',')]\n",
      "layer 22: [(204.28, 'Ġthe'), (204.21, 'Ġ\"'), (191.42, 'Ġa'), (171.27, 'Ġ('), (159.83, ',')]\n",
      "layer 23: [(313.1, 'Ġthe'), (295.93, 'Ġ\"'), (284.07, 'Ġa'), (254.38, ','), (250.58, 'Ġ(')]\n",
      "layer 24: [(-72.36, 'Ġpositive'), (-73.5, 'Ġnegative'), (-74.05, 'Ġlove'), (-74.6, 'Ġhappy'), (-75.45, 'Ġneutral')]\n"
     ]
    }
   ],
   "source": [
    "def ids_to_tokens(scores, topk=5):\n",
    "    topK_preds = torch.topk(scores, k=topk)\n",
    "    pred_idx = topK_preds.indices[0].tolist()\n",
    "    pred_scores = map(lambda score: round(score,2), topK_preds.values[0].tolist())\n",
    "    pred_tokens = tokenizer.convert_ids_to_tokens(pred_idx)\n",
    "    return pred_tokens, pred_scores\n",
    "    \n",
    "def decode(h):\n",
    "    scores = model.lm_head(h)\n",
    "    return scores\n",
    "    \n",
    "    \n",
    "def early_decoding(output, l_start_end=[0,99]):\n",
    "    hidden = output.hidden_states\n",
    "    for i, h in enumerate(hidden[l_start_end[0]:l_start_end[1]]):\n",
    "        h = h[...,-1,:] ## get logits for last token\n",
    "        scores = decode(h)\n",
    "        tokens, scores = ids_to_tokens(scores, topk=5)\n",
    "        print(f\"layer {i}: {list(zip(scores, tokens))}\")\n",
    "        #print(f\"layer {i}: {tokens}\")\n",
    "\n",
    "#ids_to_tokens(output.logits[...,-1,:])\n",
    "early_decoding(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a6aee",
   "metadata": {},
   "source": [
    "## Encoder-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da33a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"lsanochkin/deberta-large-feedback\")\n",
    "model = DebertaForMaskedLM.from_pretrained(\"lsanochkin/deberta-large-feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb12969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_adj = \"Canada\"\n",
    "#prompt = [f'On a scale from 0 to 10, the size of Andorra is a 0 and Russia is a 10.\\\n",
    "#The size of {test_adj} is a']\n",
    "\n",
    "#test_adj = \"Russia\"\n",
    "#prompt = [f'Compared to the size of the country Belgium, the size of {test_adj} is a']\n",
    "\n",
    "#test_adj = \"bad\"\n",
    "#prompt = [f'Q: What is the sentiment of \"love\"?\\nA: positive\\nQ: What is the sentiment of \"hate\"?\\nA: negative\\\n",
    "#          \\nQ: What is the sentiment of \"{test_adj}\"?\\nA:']\n",
    "\n",
    "test_adj = \"aweful\"\n",
    "prompt = [f'Q: What is the sentiment of \"love\"?\\nA: 1\\nQ: What is the sentiment of \"hate\"?\\nA: 0\\\n",
    "          \\nQ: What is the sentiment of \"{test_adj}\"?\\nA: [MASK]']\n",
    "\n",
    "#prompt = [f'Today I abandon. Yesterday I abandoned. Today I abolish. Yesterday I [MASK]']\n",
    "#prompt= [\"Q: What is the capital of Peru? A: Lima Q: What is the capital of Poland? A: [MASK]\"]\n",
    "\n",
    "tokens = tokenizer(prompt, return_tensors='pt')\n",
    "output = model(**tokens,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f783a8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0: ['x', ']', 'us', 'Ġadvance', 'Ġscale', 'Ġgiven', 'age', 'er', 'ade', 'Ġprocess']\n",
      "layer 1: ['ones', 'Ġadvance', 'Ġother', 'other', 'Ġfuture', 'Ġsurrounding', 'Ġchoke', 'Ġelementary', 'os', 'Ġsame']\n",
      "layer 2: ['Ġadvance', 'ones', 'Ġfuture', 'Ġelementary', 'Ġnew', 'Ġother', 'Ġold', 'Ġbehind', 'Ġday', 'Ġcommon']\n",
      "layer 3: ['Ġnew', 'Ġaddicted', 'Ġelementary', 'Ġother', 'other', 'Ġfuture', 'Ġold', 'Ġirresponsible', 'Ġof', 'Ġkeep']\n",
      "layer 4: ['Ġschool', 'Ġsocial', 'Ġnew', '.', 'Ġof', 'Ġday', 'Ġaddicted', 'Ġdistance', 'Ġamount', ':']\n",
      "layer 5: ['Ġaddicted', 'Ġthis', 'onymous', 'Ġonline', 'Ġdistance', 'Ġhome', 'Ġunderage', 'Ġfuture', 'Ġjust', 'Ġwhoever']\n",
      "layer 6: ['Ġwhoever', 'Ġthis', 'onymous', 'Ġdirty', 'ĠEveryday', 'Ġdistance', 'Ġonline', 'Ġdying', 'Ġhome', 'Ġunderage']\n",
      "layer 7: ['Ġonline', 'Ġthis', 'onymous', 'Ġfuture', 'ĠEveryday', 'ĠANY', 'Ġclassroom', 'Ġwhat', 'Ġhome', 'Ġoutgoing']\n",
      "layer 8: ['Ġcorrupted', 'onymous', 'Ġonline', 'Ġthis', 'Ġclassroom', 'ĠEveryday', 'Ġfuture', 'Ġschool', 'Ġaddiction', 'Ġdirty']\n",
      "layer 9: ['Ġonline', 'Ġthis', 'ĠEveryday', 'Ġeveryday', 'Ġtext', 'Ġdirty', 'Ġeducational', 'Ġconsole', 'Ġobjective', 'Ġoutside']\n",
      "layer 10: ['Ġonline', 'Ġthis', 'Âł', 'Ġhome', '.', 'Ġindependent', 'Ġindividual', 'Ġtext', ',', 'Ġfree']\n",
      "layer 11: ['Ġonline', 'Ġthis', 'Ġhome', 'Âł', 'Ġtext', ',', '.', 'Ġsecond', 'Ġit', 'Ġschool']\n",
      "layer 12: ['Ġonline', 'Ġhome', 'Âł', 'Ġnow', 'Ġfree', 'Ġemotional', 'Ġboth', 'Ġnormal', 'Ġthis', 'Ġsimple']\n",
      "layer 13: ['Âł', 'Ġonline', 'Ġfree', 'Ġhome', 'Ġn', 'Ġsimple', 'Ġit', 'Ġthem', 'Ġeither', 'Ġ6']\n",
      "layer 14: ['Âł', 'Ġonline', 'Ġn', 'Ġt', 'Ġhome', 'Ġe', 'Ġnew', 'Ġit', 'Ġc', 'ĠUS']\n",
      "layer 15: ['Ġn', 'Âł', 'Ġcontrol', 'Ġcall', 'Ġhere', 'Ġit', 'Ġt', 'Ġthe', 'Ġchoice', 'Ġonline']\n",
      "layer 16: ['Ġcontrol', 'Ġthe', 'Ġit', 'Ġn', 'Ġin', 'Ġt', 'Ġe', 'Ġinside', 'Âł', 'Ġat']\n",
      "layer 17: ['Ġit', 'Ġthe', 'Âł', '.', 'Ġe', ',', 'Ġat', 'Ġin', 'Ġcontrol', 'Ġis']\n",
      "layer 18: ['Âł', 'Ġ', 'Ġ5', 'Ġthe', 'Ġtop', 'Ġ2', 'Ġto', '.', 'Ġit', 'Ġin']\n",
      "layer 19: ['Âł', 'ĠC', 'Ġ', 'Ġat', 'Ġ5', 'Ġbe', 'Ġ2', 'Ġc', 'Ġthe', 'Ġno']\n",
      "layer 20: ['Âł', 'ĠC', 'Ġ', 'Ġ2', 'Ġbe', 'Ġ5', '_', 'ĠB', 'Ġat', 'Ġ1']\n",
      "layer 21: ['Âł', 'Ġ', 'Ġ5', 'Ġ3', 'Ġ2', 'Ġn', 'Ġ1', 'Ġat', 'Ġ4', 'Ġ6']\n",
      "layer 22: ['Ġ3', 'Ġ1', 'Ġ2', 'Âł', 'Ġ', 'Ġ4', 'Ġ5', '3', 'Ġn', 'Ġ6']\n",
      "layer 23: ['Ġ1', 'Ġ3', 'Ġ2', 'Ġ4', '3', '1', 'Ġ5', 'Ġ', 'Âł', 'Ġ6']\n",
      "layer 24: ['Ġ1', 'Ġ', 'Ġ2', 'Ġ3', 'Ġ5', '1', 'Ġ4', '3', 'Ġ0', 'Âł']\n"
     ]
    }
   ],
   "source": [
    "def ids_to_tokens(scores, topk=5):\n",
    "    topK_preds = torch.topk(scores, k=topk, dim=-1)\n",
    "    pred_idx = topK_preds.indices[0].tolist()\n",
    "    pred_scores = topK_preds.values[0].tolist()  \n",
    "    if isinstance(pred_idx[0], list):\n",
    "        pred_idx = pred_idx[0]\n",
    "    pred_tokens = tokenizer.convert_ids_to_tokens(pred_idx)\n",
    "    return pred_tokens\n",
    "    \n",
    "def decode(h):\n",
    "    scores = model.cls(h)\n",
    "    return scores\n",
    "    \n",
    "    \n",
    "def early_decoding(output, tokens, l_start_end=[0,99], select_token=\"[MASK]\"):\n",
    "    hidden = output.hidden_states\n",
    "    for i, h in enumerate(hidden[l_start_end[0]:l_start_end[1]]):\n",
    "        \n",
    "        select_token_id = tokenizer.convert_tokens_to_ids(select_token) ## retrieve index of [MASK]\n",
    "        batch_idx, seq_idx = (tokens.input_ids == select_token_id).nonzero(as_tuple=True)  \n",
    "        h = h[..., seq_idx,:]\n",
    "        scores = decode(h)\n",
    "        pred_tokens = ids_to_tokens(scores, topk=10)\n",
    "        print(f\"layer {i}: {pred_tokens}\")\n",
    "\n",
    "\n",
    "early_decoding(output, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5ca6213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaOnlyMLMHead(\n",
       "  (predictions): DebertaLMPredictionHead(\n",
       "    (transform): DebertaPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=1024, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96406135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relationLM_venv",
   "language": "python",
   "name": "relationlm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
