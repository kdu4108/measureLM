{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eaf87a4",
   "metadata": {},
   "source": [
    "# Early decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3f50b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, DebertaForMaskedLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2-medium')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lsanochkin/deberta-large-feedback\")\n",
    "model = DebertaForMaskedLM.from_pretrained(\"lsanochkin/deberta-large-feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e64fa005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_select(tokens, tokenizer, select_token=\"[MASK]\"): #\"Ä hi\"\n",
    "    if select_token is not None:\n",
    "        select_token_id = tokenizer.convert_tokens_to_ids(select_token) ## retrieve index of [MASK]\n",
    "        batch_idx, seq_idx = (tokens.input_ids == select_token_id).nonzero(as_tuple=True)\n",
    "    \n",
    "    if select_token is None or select_token not in tokenizer.vocab: ## get last token before padding\n",
    "        batch_idx, seq_idx = (tokens.input_ids != tokenizer.pad_token_id).nonzero(as_tuple=True)\n",
    "        batch_idx, unique_batch_counts = torch.unique_consecutive(batch_idx, return_counts=True)\n",
    "        unique_batch_cumsum = torch.cumsum(unique_batch_counts,dim=0)-1\n",
    "        seq_idx = seq_idx[unique_batch_cumsum]\n",
    "    \n",
    "    assert batch_idx.shape[0] > 0, f\"mlm-type model and {select_token} token not in prompt text\"\n",
    "    batch_seq_idx = (batch_idx, seq_idx)\n",
    "    return batch_seq_idx\n",
    "\n",
    "\n",
    "def encode(texts, tokenizer, model):\n",
    "    if tokenizer.pad_token is None: ## some tokenizers do not have pad tokens\n",
    "        tokenizer.pad_token = tokenizer.eos_token  \n",
    "    tokens = tokenizer(texts, padding=True, return_tensors='pt')\n",
    "    output = model(**tokens,output_hidden_states=True)\n",
    "    return output, tokens\n",
    "\n",
    "\n",
    "def topK_scores(scores, tokenizer, topk=5):\n",
    "    pred_scores, pred_tokens = [], []\n",
    "    topK_preds = torch.topk(scores, k=topk)\n",
    "    for scores, indices in zip(topK_preds.values.tolist(), topK_preds.indices.tolist()):\n",
    "        scores = list(map(lambda score: round(score,2), scores))\n",
    "        pred_scores.append(scores)\n",
    "        tokens = list(map(lambda idx: tokenizer.convert_ids_to_tokens(idx), indices))\n",
    "        pred_tokens.append(tokens)\n",
    "    return pred_tokens, pred_scores\n",
    "    \n",
    "    \n",
    "def decode(h, model):\n",
    "    if model.can_generate(): ## decoder-only\n",
    "        scores = model.lm_head(h)\n",
    "    else: ## encoder-only\n",
    "        scores = model.cls(h)\n",
    "    return scores\n",
    "\n",
    "    \n",
    "def early_decoding(hidden_states, tok_idx, model, l_start_end=[0,99]):\n",
    "    \n",
    "    layer_scores = []\n",
    "    for i, h in enumerate(hidden_states[l_start_end[0]:l_start_end[1]]):\n",
    "        h = h[tok_idx] ## get hidden states per token\n",
    "        scores = decode(h, model) ## decode the hidden state through last layer\n",
    "        layer_scores.append(scores)\n",
    "    return layer_scores\n",
    "\n",
    "\n",
    "def scores_to_tokens(layer_scores, tokenizer, mode=2):\n",
    "    \n",
    "    for i, scores in enumerate(layer_scores):\n",
    "        if isinstance(mode, int):\n",
    "            tokens, scores = topK_scores(scores, tokenizer, topk=mode)\n",
    "            print(f\"layer {i}: {list(zip(scores, tokens))}\")\n",
    "        elif isinstance(mode, str):\n",
    "            pass\n",
    "             \n",
    "            \n",
    "arg = \"adore\"\n",
    "asw = [\"positive\", \"negative\"]\n",
    "prompt = [\"hi how are\", \"Summer is the\"]\n",
    "\n",
    "output, tokens = encode(prompt, tokenizer, model)\n",
    "tok_idx = token_select(tokens, tokenizer)\n",
    "layer_scores = early_decoding(output.hidden_states, tok_idx, model)\n",
    "scores_to_tokens(layer_scores, tokenizer, mode=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8abcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relationLM_venv",
   "language": "python",
   "name": "relationlm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
