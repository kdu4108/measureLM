{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eaf87a4",
   "metadata": {},
   "source": [
    "# Early decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a01af",
   "metadata": {},
   "source": [
    "## Decoder-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3f50b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, DebertaForMaskedLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2-medium')\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8889cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"context\":  f'Q: What is the sentiment of \"love\"? A: positive Q: What is the sentiment of \"hate\"? A: negative',\n",
    "           \"prompt\": f'Q: What is the sentiment of \"{arg}\"? A:', \n",
    "           \"answ\": [\"positive\", \"negative\"],\n",
    "           \"arg\":  [\"adore\", 0]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0bff87b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0: [([5.33, 4.99], ['Ġare', 'Ġhelicop']), ([4.91, 4.27], ['Ġthe', 'Ġunden'])]\n",
      "layer 1: [([18.36, 18.22], ['ĠUganda', 'ĠCitiz']), ([13.78, 10.66], ['ĠSponsor', 'Ġimperialist'])]\n",
      "layer 2: [([13.67, 13.11], ['ĠUganda', 'Ġont']), ([8.19, 7.82], ['ĠUnited', 'Ġabove'])]\n",
      "layer 3: [([12.04, 11.65], ['ĠUganda', 'ĠSponsor']), ([8.91, 8.38], ['Ġabove', 'ories'])]\n",
      "layer 4: [([11.37, 11.3], ['ĠUganda', 'ĠSponsor']), ([9.12, 8.71], ['Ġbeginning', 'ories'])]\n",
      "layer 5: [([6.49, 5.91], ['Ġdependencies', 'ĠUganda']), ([12.13, 9.53], ['Ġbeginning', 'Ġopposite'])]\n",
      "layer 6: [([8.72, 7.95], ['ĠTours', 'ĠZen']), ([13.23, 12.63], ['Ġopposite', 'Ġbeginning'])]\n",
      "layer 7: [([3.75, 2.42], ['ĠZen', 'Ġnot']), ([15.1, 14.33], ['Ġbeginning', 'Ġopposite'])]\n",
      "layer 8: [([3.49, 2.24], ['ĠZen', 'Ġproceeding']), ([14.19, 13.38], ['Ġbeginning', 'Ġopposite'])]\n",
      "layer 9: [([6.43, 4.01], ['Ġnot', 'Ġ[']), ([14.71, 10.55], ['Ġbeginning', 'Ġopposite'])]\n",
      "layer 10: [([7.19, 4.9], ['Ġnot', 'Ġall']), ([15.29, 12.04], ['Ġbeginning', 'Ġopposite'])]\n",
      "layer 11: [([4.94, 4.9], ['Ġnot', 'Ġall']), ([13.25, 13.05], ['Ġbeginning', 'Ġfirst'])]\n",
      "layer 12: [([6.11, 6.01], ['Ġthe', 'Ġall']), ([16.73, 14.93], ['Ġfirst', 'Ġmost'])]\n",
      "layer 13: [([12.96, 11.06], ['Ġthe', 'Ġall']), ([21.5, 19.77], ['Ġmost', 'Ġfirst'])]\n",
      "layer 14: [([18.33, 15.18], ['Ġthe', 'Ġall']), ([30.64, 25.59], ['Ġmost', 'Ġbest'])]\n",
      "layer 15: [([27.82, 20.93], ['Ġthe', 'Ġall']), ([41.89, 32.4], ['Ġmost', 'Ġbest'])]\n",
      "layer 16: [([40.14, 26.22], ['Ġthe', 'Ġall']), ([50.44, 44.62], ['Ġmost', 'Ġonly'])]\n",
      "layer 17: [([60.56, 37.82], ['Ġthe', 'Ġyou']), ([65.74, 56.91], ['Ġmost', 'Ġonly'])]\n",
      "layer 18: [([90.83, 68.74], ['Ġthe', 'Ġyou']), ([77.32, 68.39], ['Ġmost', 'Ġonly'])]\n",
      "layer 19: [([135.58, 115.32], ['Ġthe', 'Ġyou']), ([90.95, 84.31], ['Ġmost', 'Ġonly'])]\n",
      "layer 20: [([170.3, 143.56], ['Ġthe', 'Ġyou']), ([115.57, 114.9], ['Ġ\"', 'Ġmost'])]\n",
      "layer 21: [([207.61, 158.07], ['Ġthe', 'Ġyou']), ([131.41, 128.13], ['Ġ\"', 'Ġmost'])]\n",
      "layer 22: [([281.65, 214.39], ['Ġthe', 'Ġyou']), ([174.89, 170.59], ['Ġtime', 'Ġ\"'])]\n",
      "layer 23: [([402.84, 284.84], ['Ġthe', ',']), ([212.15, 203.46], ['Ġ\"', 'Ġthe'])]\n",
      "layer 24: [([-41.74, -43.35], ['Ġyou', 'Ġwe']), ([-82.66, -82.87], ['Ġtime', 'Ġseason'])]\n"
     ]
    }
   ],
   "source": [
    "def token_select(tokens, tokenizer, select_token=\"[MASK]\"): #\"Ġhi\"\n",
    "    if select_token is not None:\n",
    "        select_token_id = tokenizer.convert_tokens_to_ids(select_token) ## retrieve index of [MASK]\n",
    "        batch_idx, seq_idx = (tokens.input_ids == select_token_id).nonzero(as_tuple=True)\n",
    "    \n",
    "    if select_token is None or select_token not in tokenizer.vocab: ## get last token before padding\n",
    "        batch_idx, seq_idx = (tokens.input_ids != tokenizer.pad_token_id).nonzero(as_tuple=True)\n",
    "        batch_idx, unique_batch_counts = torch.unique_consecutive(batch_idx, return_counts=True)\n",
    "        unique_batch_cumsum = torch.cumsum(unique_batch_counts,dim=0)-1\n",
    "        seq_idx = seq_idx[unique_batch_cumsum]\n",
    "    \n",
    "    assert batch_idx.shape[0] > 0, f\"mlm-type model and {select_token} token not in prompt text\"\n",
    "    batch_seq_idx = (batch_idx, seq_idx)\n",
    "    return batch_seq_idx\n",
    "\n",
    "\n",
    "def encode(texts, tokenizer, model):\n",
    "    if tokenizer.pad_token is None: ## some tokenizers do not have pad tokens\n",
    "        tokenizer.pad_token = tokenizer.eos_token  \n",
    "    tokens = tokenizer(texts, padding=True, return_tensors='pt')\n",
    "    output = model(**tokens,output_hidden_states=True)\n",
    "    return output, tokens\n",
    "\n",
    "\n",
    "def ids_to_tokens(scores, topk=5):\n",
    "    pred_scores, pred_tokens = [], []\n",
    "    topK_preds = torch.topk(scores, k=topk)\n",
    "    for scores, indices in zip(topK_preds.values.tolist(), topK_preds.indices.tolist()):\n",
    "        scores = list(map(lambda score: round(score,2), scores))\n",
    "        pred_scores.append(scores)\n",
    "        tokens = list(map(lambda idx: tokenizer.convert_ids_to_tokens(idx), indices))\n",
    "        pred_tokens.append(tokens)\n",
    "    return pred_tokens, pred_scores\n",
    "    \n",
    "    \n",
    "def decode(h, model):\n",
    "    if model.can_generate(): ## decoder-only\n",
    "        scores = model.lm_head(h)\n",
    "    else: ## encoder-only\n",
    "        scores = model.cls(h)\n",
    "    return scores\n",
    "\n",
    "    \n",
    "def early_decoding(hidden_states, tok_idx, model, l_start_end=[0,99]):\n",
    "    for i, h in enumerate(hidden_states[l_start_end[0]:l_start_end[1]]):\n",
    "        h = h[tok_idx] ## get hidden states per token\n",
    "        scores = decode(h, model) ## decode the hidden state through last layer\n",
    "        tokens, scores = ids_to_tokens(scores, topk=2)\n",
    "        print(f\"layer {i}: {list(zip(scores, tokens))}\")\n",
    "\n",
    "    \n",
    "arg = \"adore\"\n",
    "asw = [\"positive\", \"negative\"]\n",
    "prompt = [\"hi how are\", \"Summer is the\"]\n",
    "\n",
    "output, tokens = encode(prompt, tokenizer, model)\n",
    "tok_idx = token_select(tokens, tokenizer)\n",
    "early_decoding(output.hidden_states, tok_idx, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "443e4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arg = \"Canada\"\n",
    "#prompt = [f'On a scale from 0 to 10, the size of Andorra is a 0 and Russia is a 10.\\\n",
    "#The size of {arg} is a']\n",
    "\n",
    "#arg = \"Russia\"\n",
    "#prompt = [f'Compared to the size of the country Belgium, the size of {arg} is a']\n",
    "\n",
    "arg = \"adore\"\n",
    "asw = [\"positive\", \"negative\"]\n",
    "prompt = [f'Q: What is the sentiment of \"love\"?\\nA: positive\\nQ: What is the sentiment of \"hate\"?\\nA: negative\\\n",
    "          \\nQ: What is the sentiment of \"{arg}\"?\\nA:']\n",
    "\n",
    "#arg = \"Donald Trump and Joe Biden\"\n",
    "#prompt = [f'Q: What is the relationship between Trump and Biden? A: negative Q: What is the relationship between Romeo and Julia? A:']\n",
    "\n",
    "#arg = \"bad\"\n",
    "#prompt = [f'Q: What is the sentiment of \"love\"?\\nA: 10\\nQ: What is the sentiment of \"hate\"?\\nA: 0\\\n",
    "#          \\nQ: What is the sentiment of \"{arg}\"?\\nA:']\n",
    "\n",
    "#prompt = [f'Today I abandon. Yesterday I abandoned. Today I abolish. Yesterday I']\n",
    "#prompt= [\"Q: What is the capital of Peru? A: Lima Q: What is the capital of Germany? A:\"]\n",
    "\n",
    "token_input = tokenizer(prompt, return_tensors='pt')\n",
    "output = model(**token_input,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb226cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0: [(5.75, ':'), (4.7, ':\"'), (4.67, ':['), (4.58, ':]'), (4.54, ':-')]\n",
      "layer 1: [(23.16, 'ĠCitiz'), (22.47, 'ĠPetr'), (20.42, 'ifer'), (19.57, 'ĠDag'), (19.4, 'Ġappend')]\n",
      "layer 2: [(24.85, 'ĠCitiz'), (24.05, 'ĠPetr'), (21.54, 'Ġont'), (21.51, 'Ġappend'), (21.5, 'ifer')]\n",
      "layer 3: [(22.65, 'ĠPetr'), (20.98, 'ĠCitiz'), (20.97, 'Ġsubsc'), (20.71, 'Ġmum'), (20.57, 'Ġtion')]\n",
      "layer 4: [(24.48, 'ĠPetr'), (22.5, 'Ġmum'), (21.24, 'ĠCitiz'), (20.88, 'ifer'), (20.68, 'ĠPutin')]\n",
      "layer 5: [(24.9, 'ĠPetr'), (23.1, 'ĠGuinea'), (21.94, 'orate'), (21.38, 'Ġrevolutions'), (21.01, 'ĠAzerb')]\n",
      "layer 6: [(25.51, 'mi'), (23.49, 'ĠPetr'), (23.24, 'orate'), (23.06, 'ĠGuinea'), (21.89, 'hene')]\n",
      "layer 7: [(21.05, 'mi'), (17.92, 'ĠPlace'), (17.14, 'Ġrevolutions'), (17.03, 'Ġmum'), (16.77, 'ĠPetr')]\n",
      "layer 8: [(17.41, 'Ġn'), (14.74, 'ĠPetr'), (14.69, 'Ġnil'), (13.62, 'mi'), (13.22, 'Ġnone')]\n",
      "layer 9: [(17.23, 'Ġn'), (14.51, 'Ġnil'), (11.61, 'Ġinst'), (10.95, 'Ġtion'), (10.52, 'ĠPlace')]\n",
      "layer 10: [(21.93, 'Ġn'), (21.52, 'Ġnil'), (16.45, 'Ġest'), (14.74, 'Ġnone'), (13.45, 'Ġcinem')]\n",
      "layer 11: [(18.69, 'Ġnil'), (18.31, 'Ġn'), (14.54, 'Ġmixed'), (12.78, 'Ġnone'), (12.07, 'Ġpositive')]\n",
      "layer 12: [(21.16, 'Ġmixed'), (20.26, 'Ġnil'), (18.0, 'Ġnone'), (15.85, 'Ġpositive'), (15.65, 'Ġnegative')]\n",
      "layer 13: [(20.65, 'Ġmixed'), (13.93, 'Ġpositive'), (13.62, 'Ġnone'), (12.51, 'Ġnil'), (12.51, 'Ġyes')]\n",
      "layer 14: [(15.42, 'Ġn'), (13.31, 'Ġ('), (12.9, 'Ġmixed'), (12.88, 'ĠN'), (12.15, 'ĠA')]\n",
      "layer 15: [(24.21, 'Ġn'), (19.77, 'Ġ('), (19.61, 'Ġpositive'), (17.04, 'Ġmixed'), (16.49, 'Ġyes')]\n",
      "layer 16: [(31.08, 'Ġthe'), (30.94, 'Ġ('), (30.37, 'Ġn'), (30.21, 'Ġa'), (28.82, 'Ġ\"')]\n",
      "layer 17: [(42.14, 'Ġa'), (42.03, 'Ġthe'), (41.58, 'Ġ('), (41.3, 'Ġ\"'), (34.46, 'Ġn')]\n",
      "layer 18: [(54.13, 'Ġ\"'), (53.46, 'Ġthe'), (51.6, 'Ġa'), (49.7, 'Ġ('), (43.31, 'ĠI')]\n",
      "layer 19: [(69.72, 'Ġ\"'), (68.04, 'Ġthe'), (67.76, 'Ġa'), (64.16, 'Ġ('), (56.66, 'ĠI')]\n",
      "layer 20: [(108.09, 'Ġ\"'), (104.67, 'Ġa'), (104.62, 'Ġthe'), (94.71, 'Ġ('), (81.74, ',')]\n",
      "layer 21: [(140.73, 'Ġ\"'), (134.89, 'Ġthe'), (130.31, 'Ġa'), (119.47, 'Ġ('), (106.2, ',')]\n",
      "layer 22: [(204.28, 'Ġthe'), (204.21, 'Ġ\"'), (191.42, 'Ġa'), (171.27, 'Ġ('), (159.83, ',')]\n",
      "layer 23: [(313.1, 'Ġthe'), (295.93, 'Ġ\"'), (284.07, 'Ġa'), (254.38, ','), (250.58, 'Ġ(')]\n",
      "layer 24: [(-72.36, 'Ġpositive'), (-73.5, 'Ġnegative'), (-74.05, 'Ġlove'), (-74.6, 'Ġhappy'), (-75.45, 'Ġneutral')]\n"
     ]
    }
   ],
   "source": [
    "def ids_to_tokens(scores, topk=5):\n",
    "    topK_preds = torch.topk(scores, k=topk)\n",
    "    pred_idx = topK_preds.indices[0].tolist()\n",
    "    pred_scores = map(lambda score: round(score,2), topK_preds.values[0].tolist())\n",
    "    pred_tokens = tokenizer.convert_ids_to_tokens(pred_idx)\n",
    "    return pred_tokens, pred_scores\n",
    "    \n",
    "def decode(h):\n",
    "    scores = model.lm_head(h)\n",
    "    return scores\n",
    "    \n",
    "    \n",
    "def early_decoding(output, l_start_end=[0,99]):\n",
    "    hidden = output.hidden_states\n",
    "    for i, h in enumerate(hidden[l_start_end[0]:l_start_end[1]]):\n",
    "        h = h[...,-1,:] ## get logits for last token\n",
    "        scores = decode(h)\n",
    "        tokens, scores = ids_to_tokens(scores, topk=5)\n",
    "        print(f\"layer {i}: {list(zip(scores, tokens))}\")\n",
    "        #print(f\"layer {i}: {tokens}\")\n",
    "\n",
    "#ids_to_tokens(output.logits[...,-1,:])\n",
    "early_decoding(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a6aee",
   "metadata": {},
   "source": [
    "## Encoder-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "da33a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"lsanochkin/deberta-large-feedback\")\n",
    "model = DebertaForMaskedLM.from_pretrained(\"lsanochkin/deberta-large-feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beb12969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_adj = \"Canada\"\n",
    "#prompt = [f'On a scale from 0 to 10, the size of Andorra is a 0 and Russia is a 10.\\\n",
    "#The size of {test_adj} is a']\n",
    "\n",
    "#test_adj = \"Russia\"\n",
    "#prompt = [f'Compared to the size of the country Belgium, the size of {test_adj} is a']\n",
    "\n",
    "#test_adj = \"bad\"\n",
    "#prompt = [f'Q: What is the sentiment of \"love\"?\\nA: positive\\nQ: What is the sentiment of \"hate\"?\\nA: negative\\\n",
    "#          \\nQ: What is the sentiment of \"{test_adj}\"?\\nA:']\n",
    "\n",
    "test_adj = \"aweful\"\n",
    "prompt = [f'Q: What is the sentiment of \"love\"?\\nA: 1\\nQ: What is the sentiment of \"hate\"?\\nA: 0\\\n",
    "          \\nQ: What is the sentiment of \"{test_adj}\"?\\nA: [MASK]']\n",
    "\n",
    "#prompt = [f'Today I abandon. Yesterday I abandoned. Today I abolish. Yesterday I [MASK]']\n",
    "#prompt= [\"Q: What is the capital of Peru? A: Lima Q: What is the capital of Poland? A: [MASK]\"]\n",
    "\n",
    "tokens = tokenizer(prompt, return_tensors='pt')\n",
    "output = model(**tokens,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f783a8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([55])\n",
      "layer 0: ['x', ']', 'us', 'Ġadvance', 'Ġscale', 'Ġgiven', 'age', 'er', 'ade', 'Ġprocess']\n",
      "tensor([55])\n",
      "layer 1: ['ones', 'Ġadvance', 'Ġother', 'other', 'Ġfuture', 'Ġsurrounding', 'Ġchoke', 'Ġelementary', 'os', 'Ġsame']\n",
      "tensor([55])\n",
      "layer 2: ['Ġadvance', 'ones', 'Ġfuture', 'Ġelementary', 'Ġnew', 'Ġother', 'Ġold', 'Ġbehind', 'Ġday', 'Ġcommon']\n",
      "tensor([55])\n",
      "layer 3: ['Ġnew', 'Ġaddicted', 'Ġelementary', 'Ġother', 'other', 'Ġfuture', 'Ġold', 'Ġirresponsible', 'Ġof', 'Ġkeep']\n",
      "tensor([55])\n",
      "layer 4: ['Ġschool', 'Ġsocial', 'Ġnew', '.', 'Ġof', 'Ġday', 'Ġaddicted', 'Ġdistance', 'Ġamount', ':']\n",
      "tensor([55])\n",
      "layer 5: ['Ġaddicted', 'Ġthis', 'onymous', 'Ġonline', 'Ġdistance', 'Ġhome', 'Ġunderage', 'Ġfuture', 'Ġjust', 'Ġwhoever']\n",
      "tensor([55])\n",
      "layer 6: ['Ġwhoever', 'Ġthis', 'onymous', 'Ġdirty', 'ĠEveryday', 'Ġdistance', 'Ġonline', 'Ġdying', 'Ġhome', 'Ġunderage']\n",
      "tensor([55])\n",
      "layer 7: ['Ġonline', 'Ġthis', 'onymous', 'Ġfuture', 'ĠEveryday', 'ĠANY', 'Ġclassroom', 'Ġwhat', 'Ġhome', 'Ġoutgoing']\n",
      "tensor([55])\n",
      "layer 8: ['Ġcorrupted', 'onymous', 'Ġonline', 'Ġthis', 'Ġclassroom', 'ĠEveryday', 'Ġfuture', 'Ġschool', 'Ġaddiction', 'Ġdirty']\n",
      "tensor([55])\n",
      "layer 9: ['Ġonline', 'Ġthis', 'ĠEveryday', 'Ġeveryday', 'Ġtext', 'Ġdirty', 'Ġeducational', 'Ġconsole', 'Ġobjective', 'Ġoutside']\n",
      "tensor([55])\n",
      "layer 10: ['Ġonline', 'Ġthis', 'Âł', 'Ġhome', '.', 'Ġindependent', 'Ġindividual', 'Ġtext', ',', 'Ġfree']\n",
      "tensor([55])\n",
      "layer 11: ['Ġonline', 'Ġthis', 'Ġhome', 'Âł', 'Ġtext', ',', '.', 'Ġsecond', 'Ġit', 'Ġschool']\n",
      "tensor([55])\n",
      "layer 12: ['Ġonline', 'Ġhome', 'Âł', 'Ġnow', 'Ġfree', 'Ġemotional', 'Ġboth', 'Ġnormal', 'Ġthis', 'Ġsimple']\n",
      "tensor([55])\n",
      "layer 13: ['Âł', 'Ġonline', 'Ġfree', 'Ġhome', 'Ġn', 'Ġsimple', 'Ġit', 'Ġthem', 'Ġeither', 'Ġ6']\n",
      "tensor([55])\n",
      "layer 14: ['Âł', 'Ġonline', 'Ġn', 'Ġt', 'Ġhome', 'Ġe', 'Ġnew', 'Ġit', 'Ġc', 'ĠUS']\n",
      "tensor([55])\n",
      "layer 15: ['Ġn', 'Âł', 'Ġcontrol', 'Ġcall', 'Ġhere', 'Ġit', 'Ġt', 'Ġthe', 'Ġchoice', 'Ġonline']\n",
      "tensor([55])\n",
      "layer 16: ['Ġcontrol', 'Ġthe', 'Ġit', 'Ġn', 'Ġin', 'Ġt', 'Ġe', 'Ġinside', 'Âł', 'Ġat']\n",
      "tensor([55])\n",
      "layer 17: ['Ġit', 'Ġthe', 'Âł', '.', 'Ġe', ',', 'Ġat', 'Ġin', 'Ġcontrol', 'Ġis']\n",
      "tensor([55])\n",
      "layer 18: ['Âł', 'Ġ', 'Ġ5', 'Ġthe', 'Ġtop', 'Ġ2', 'Ġto', '.', 'Ġit', 'Ġin']\n",
      "tensor([55])\n",
      "layer 19: ['Âł', 'ĠC', 'Ġ', 'Ġat', 'Ġ5', 'Ġbe', 'Ġ2', 'Ġc', 'Ġthe', 'Ġno']\n",
      "tensor([55])\n",
      "layer 20: ['Âł', 'ĠC', 'Ġ', 'Ġ2', 'Ġbe', 'Ġ5', '_', 'ĠB', 'Ġat', 'Ġ1']\n",
      "tensor([55])\n",
      "layer 21: ['Âł', 'Ġ', 'Ġ5', 'Ġ3', 'Ġ2', 'Ġn', 'Ġ1', 'Ġat', 'Ġ4', 'Ġ6']\n",
      "tensor([55])\n",
      "layer 22: ['Ġ3', 'Ġ1', 'Ġ2', 'Âł', 'Ġ', 'Ġ4', 'Ġ5', '3', 'Ġn', 'Ġ6']\n",
      "tensor([55])\n",
      "layer 23: ['Ġ1', 'Ġ3', 'Ġ2', 'Ġ4', '3', '1', 'Ġ5', 'Ġ', 'Âł', 'Ġ6']\n",
      "tensor([55])\n",
      "layer 24: ['Ġ1', 'Ġ', 'Ġ2', 'Ġ3', 'Ġ5', '1', 'Ġ4', '3', 'Ġ0', 'Âł']\n"
     ]
    }
   ],
   "source": [
    "def ids_to_tokens(scores, topk=5):\n",
    "    topK_preds = torch.topk(scores, k=topk, dim=-1)\n",
    "    pred_idx = topK_preds.indices[0].tolist()\n",
    "    pred_scores = topK_preds.values[0].tolist()  \n",
    "    if isinstance(pred_idx[0], list):\n",
    "        pred_idx = pred_idx[0]\n",
    "    pred_tokens = tokenizer.convert_ids_to_tokens(pred_idx)\n",
    "    return pred_tokens\n",
    "    \n",
    "def decode(h):\n",
    "    scores = model.cls(h)\n",
    "    return scores\n",
    "    \n",
    "    \n",
    "def early_decoding(output, tokens, l_start_end=[0,99], select_token=\"[MASK]\"):\n",
    "    hidden = output.hidden_states\n",
    "    for i, h in enumerate(hidden[l_start_end[0]:l_start_end[1]]):\n",
    "        \n",
    "        select_token_id = tokenizer.convert_tokens_to_ids(select_token) ## retrieve index of [MASK]\n",
    "        batch_idx, seq_idx = (tokens.input_ids == select_token_id).nonzero(as_tuple=True)  \n",
    "        h = h[..., seq_idx,:]\n",
    "        scores = decode(h)\n",
    "        pred_tokens = ids_to_tokens(scores, topk=10)\n",
    "        print(f\"layer {i}: {pred_tokens}\")\n",
    "\n",
    "\n",
    "early_decoding(output, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5ca6213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaOnlyMLMHead(\n",
       "  (predictions): DebertaLMPredictionHead(\n",
       "    (transform): DebertaPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (transform_act_fn): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=1024, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eda3599e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'select_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m select_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[43mselect_token\u001b[49m)  \u001b[38;5;66;03m## retrieve index of [MASK]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m batch_idx, select_token_idx \u001b[38;5;241m=\u001b[39m (tokens\u001b[38;5;241m.\u001b[39minput_ids \u001b[38;5;241m==\u001b[39m select_token_id)\u001b[38;5;241m.\u001b[39mnonzero(as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'select_token' is not defined"
     ]
    }
   ],
   "source": [
    "select_token_id = tokenizer.convert_tokens_to_ids(select_token)  ## retrieve index of [MASK]\n",
    "batch_idx, select_token_idx = (tokens.input_ids == select_token_id).nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4f4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relationLM_venv",
   "language": "python",
   "name": "relationlm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
