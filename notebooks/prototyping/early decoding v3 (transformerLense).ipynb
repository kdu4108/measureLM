{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eaf87a4",
   "metadata": {},
   "source": [
    "# Early decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8febcad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from measureLM import helpers\n",
    "\n",
    "matplotlib.rcParams['axes.spines.right'] = False\n",
    "matplotlib.rcParams['axes.spines.top'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f50b9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformer_lens\n",
    "from functools import partial\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  \n",
    "\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-medium\").to(\"cpu\")\n",
    "model.tokenizer.pad_token = model.tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e64fa005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_select(tokens, model, select_token=\"[MASK]\"): #\"Ġhi\"\n",
    "    if select_token is not None:\n",
    "        select_token_id = model.tokenizer.convert_tokens_to_ids(select_token) ## retrieve index of [MASK]\n",
    "        batch_idx, seq_idx = (tokens == select_token_id).nonzero(as_tuple=True)\n",
    "    \n",
    "    if select_token is None or select_token not in model.tokenizer.vocab: ## get last token before padding\n",
    "        batch_idx, seq_idx = (tokens != model.tokenizer.pad_token_id).nonzero(as_tuple=True)\n",
    "        batch_idx, unique_batch_counts = torch.unique_consecutive(batch_idx, return_counts=True)\n",
    "        unique_batch_cumsum = torch.cumsum(unique_batch_counts,dim=0)-1\n",
    "        seq_idx = seq_idx[unique_batch_cumsum]\n",
    "    \n",
    "    assert batch_idx.shape[0] > 0, f\"mlm-type model and {select_token} token not in prompt text\"\n",
    "    batch_seq_idx = (batch_idx, seq_idx)\n",
    "    return batch_seq_idx\n",
    "\n",
    "\n",
    "def encode(texts, model):\n",
    "    if model.tokenizer.pad_token is None: ## some tokenizers do not have pad tokens\n",
    "        model.tokenizer.pad_token = model.tokenizer.eos_token  \n",
    "    tokens = model.to_tokens(texts, prepend_bos=False)\n",
    "    logits, activs = model.run_with_cache(tokens)\n",
    "    return logits, activs, tokens\n",
    "\n",
    "    \n",
    "def decode(h, model):\n",
    "    scores = model.unembed(h)\n",
    "    return scores\n",
    "\n",
    "    \n",
    "def early_decoding(activs, model, l_start_end=[0,99]):    \n",
    "    layer_scores = []\n",
    "    for layer in range(l_start_end[0], l_start_end[1]):\n",
    "        if layer < model.cfg.n_layers:\n",
    "            hidden_name = transformer_lens.utils.get_act_name(\"resid_post\", layer)\n",
    "            h = activs[hidden_name]\n",
    "            scores = decode(h, model) ## decode the hidden state through last layer\n",
    "            layer_scores.append(scores)\n",
    "\n",
    "    layer_scores = torch.stack(layer_scores)\n",
    "    layer_scores = torch.swapaxes(layer_scores, 0, 1)\n",
    "    return layer_scores ## dims: (prompts, layers, token_scores)\n",
    "\n",
    "                 \n",
    "            \n",
    "arg = \"adore\"\n",
    "token_candidates = [\"Paris\", \"France\", \"Germany\"]\n",
    "prompts = [\"Paris is the capital of\", \"The capital of France is\"]\n",
    "\n",
    "logits, activs, tokens = encode(prompts, model)\n",
    "layer_scores = early_decoding(activs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d942eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 24, 5, 50257])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topK_scores(scores, model, topk=5):\n",
    "    pred_scores, pred_tokens = [], []\n",
    "    topK_preds = torch.topk(scores, k=topk)\n",
    "    \n",
    "    scores = topK_preds.values.tolist()\n",
    "    indices = topK_preds.indices.tolist()\n",
    "    #for scores, indices in zip(topK_preds.values.tolist(), topK_preds.indices.tolist()):\n",
    "    scores = list(map(lambda score: round(score,2), scores))\n",
    "    pred_scores.append(scores)\n",
    "    tokens = list(map(lambda idx: model.tokenizer.convert_ids_to_tokens(idx), indices))\n",
    "    pred_tokens.append(tokens)\n",
    "    return pred_tokens, pred_scores\n",
    "\n",
    "\n",
    "def get_token_rank(scores, model, token, space=\"Ġ\"):\n",
    "    \n",
    "    if space is not None:\n",
    "        token = \"Ġ\" + token\n",
    "    token_id = model.tokenizer.convert_tokens_to_ids(token)\n",
    "    token_ranks = torch.argsort(scores, descending=True)\n",
    "    \n",
    "    token_scores = scores[token_ranks]\n",
    "    token_rank = torch.where(token_ranks == token_id)[0].item()\n",
    "    token_score = token_scores[token_rank]\n",
    "    token_rank = round(1/(token_rank+1),4) #round(1-(token_rank / len(scores)), 4)\n",
    "    return token_rank, token_score\n",
    "    \n",
    "    \n",
    "def scores_to_tokens(layer_scores, model, mode=2, print_res=True):\n",
    "    \n",
    "    prompt_layer_res = {}\n",
    "    for idx, prompt in enumerate(layer_scores):\n",
    "        print(f\"\\nprompt {idx}\")\n",
    "        layer_res = {}\n",
    "        for l, scores in enumerate(prompt):\n",
    "            if isinstance(mode, int): ## get top tokens\n",
    "                tokens, scores = topK_scores(scores, model, topk=mode)\n",
    "                layer_res[l] = list(zip(scores, tokens))\n",
    "                if print_res:\n",
    "                    print(f\"layer {l}: {layer_res[l]}\")\n",
    "            \n",
    "            elif isinstance(mode, list): ## search specific tokens\n",
    "                if isinstance(mode[0], list) and len(mode) == len(layer_scores):\n",
    "                    pass ## per prompt mode\n",
    "                elif isinstance(mode[0], str):\n",
    "                    token_ranks, token_scores = [], []\n",
    "                    for token in mode:\n",
    "                        token_rank, token_score = get_token_rank(scores, model, token)\n",
    "                        token_ranks.append(token_rank)\n",
    "                    layer_res[l] = token_ranks\n",
    "                    if print_res:\n",
    "                        print(f\"layer {l}: {list(zip(layer_res[l], mode))}\")\n",
    "        prompt_layer_res[idx] = layer_res\n",
    "    return prompt_layer_res\n",
    "\n",
    "tok_idx = token_select(tokens, model)\n",
    "scored_tokens = scores_to_tokens(layer_scores, model, mode=token_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addfd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_token_ranks(scored_tokens, tokens, prompts):\n",
    "    for prompt_id, prompt_token_ranks in scored_tokens.items():\n",
    "        all_tokens_ranks = np.array(list(prompt_token_ranks.values()))\n",
    "        layers = np.array(list(prompt_token_ranks.keys()))\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6,2), gridspec_kw={'hspace': 0.4})\n",
    "        labelsize, titlefont, markersize = 10, 12, 8\n",
    "        \n",
    "        all_tokens_ranks = np.stack(all_tokens_ranks)\n",
    "        all_tokens_ranks = np.swapaxes(all_tokens_ranks, 0, 1)\n",
    "        \n",
    "        lines = []\n",
    "        for tokens_ranks in all_tokens_ranks:\n",
    "            line, = ax.plot(layers, tokens_ranks, marker=\".\", markersize=markersize)\n",
    "            lines.append(line)\n",
    "            \n",
    "        if tokens is not None:\n",
    "            ax.legend(lines, tokens, loc='upper left', frameon=False)\n",
    "        if prompts is not None:\n",
    "            ax.set_title(prompts[prompt_id], fontsize=titlefont, color=\"black\", loc='center')\n",
    "            \n",
    "        ax.set_xlabel('layers',fontsize=labelsize)\n",
    "        ax.set_ylabel('reciprocal rank',fontsize=labelsize)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "        #fig.savefig(helpers.ROOT_DIR / \"results\" / \"plots\" / \"test.pdf\", dpi=200, bbox_inches='tight')\n",
    "    \n",
    "visualize_token_ranks(scored_tokens, token_candidates, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a08dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "measureLM_venv",
   "language": "python",
   "name": "measurelm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
