{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce2fcce",
   "metadata": {},
   "source": [
    "# Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformer_lens, itertools\n",
    "from functools import partial\n",
    "\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from measureLM import helpers, measuring, synth_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a77351",
   "metadata": {},
   "source": [
    "## Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "model = measuring.load_model(model_name=model_name, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851768c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_bias_context_pairs(df, pair_type=\"bias\"):\n",
    "    \n",
    "    prompt = \"The relationship between {ent1} and {ent2} is\"\n",
    "    pos_prefix = \"{ent1} loves {ent2}.\"\n",
    "    neg_prefix = \"{ent1} hates {ent2}.\"\n",
    "    \n",
    "    ent1_ent2 = list(zip(df[\"ent1\"].to_list(), df[\"ent2\"].to_list()))\n",
    "    \n",
    "    if pair_type==\"bias\":\n",
    "        ent1_ent2_pairs = []\n",
    "        entPair1_entPair2 = list(itertools.combinations(ent1_ent2, 2)) #permutations\n",
    "        for entPair1, entPair2 in entPair1_entPair2:\n",
    "            entPair1 = measuring.form_prompt(prompt,{\"ent1\":entPair1[0],\"ent2\":entPair1[1]})\n",
    "            entPair2 = measuring.form_prompt(prompt,{\"ent1\":entPair2[0],\"ent2\":entPair2[1]})\n",
    "            ent1_ent2_pairs.append((entPair1, entPair2))\n",
    "        \n",
    "    elif pair_type==\"context\":\n",
    "        ent1_ent2_pairs = []\n",
    "        for ent1, ent2 in ent1_ent2:\n",
    "            pos_context = measuring.form_prompt(f\"{pos_prefix} {prompt}\",{\"ent1\":ent1,\"ent2\":ent2})\n",
    "            neg_context = measuring.form_prompt(f\"{neg_prefix} {prompt}\",{\"ent1\":ent1,\"ent2\":ent2})\n",
    "            ent1_ent2_pairs.append((pos_context, neg_context))\n",
    "        \n",
    "    print(f\"pair_type: {pair_type} --> {len(ent1_ent2_pairs)} data points\")\n",
    "    return ent1_ent2_pairs\n",
    "\n",
    "scales = [\"good\", \"bad\"]\n",
    "scale_idx = measuring.get_logit_indices(scales, model)\n",
    "\n",
    "df = synth_data.load_synth_data(n=5, seed=10)\n",
    "prompt_pairs = construct_bias_context_pairs(df, pair_type=\"context\")\n",
    "prompt_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f70c444",
   "metadata": {},
   "source": [
    "## Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0617afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_hook_point(patched_activs, hook: HookPoint, new_activs, hook_layer_name, extract_tok_idx=-1, insert_tok_idx=None):\n",
    "    #print(f'patching {hook.name} <-- {hook_layer_name}')\n",
    "    if extract_tok_idx is None or extract_tok_idx == -1:\n",
    "        extract_tok_idx = (0, -1)\n",
    "    if insert_tok_idx is None:\n",
    "        insert_tok_idx = extract_tok_idx\n",
    "    new_activs_hook = new_activs[hook_layer_name]\n",
    "    vector_direction.append(torch.stack([new_activs_hook[extract_tok_idx], patched_activs[insert_tok_idx]]))\n",
    "    patched_activs[insert_tok_idx] = new_activs_hook[extract_tok_idx]\n",
    "\n",
    "\n",
    "def patch_activs(model, old_logits, new_logits, new_activs, prompt, logit_idx):\n",
    "    \n",
    "    n_layers = model.cfg.n_layers\n",
    "    activ_d = model.cfg.d_model\n",
    "    hook_names = [\"attn_out\", \"mlp_out\"]\n",
    "    \n",
    "    effect_strength = torch.zeros(n_layers,len(hook_names), device=model.cfg.device)\n",
    "    global vector_direction\n",
    "    vector_direction = [] \n",
    "\n",
    "    for layer in (range(n_layers)):\n",
    "        for hook_i, hook_name in enumerate(hook_names): \n",
    "\n",
    "            hook_layer_name = transformer_lens.utils.get_act_name(hook_name, layer)\n",
    "            patch_layers_fn = [(hook_layer_name, partial(patch_hook_point, new_activs=new_activs, hook_layer_name=hook_layer_name))]\n",
    "            patched_logits = model.run_with_hooks(prompt,fwd_hooks=patch_layers_fn,reset_hooks_end=True)\n",
    "            \n",
    "            ## get measurement change\n",
    "            old_logits_v = old_logits[...,0]\n",
    "            patched_logits = measuring.select_logits(patched_logits,logit_idx)\n",
    "            patched_logit_diff = (patched_logits[...,0]-patched_logits[...,1])\n",
    "            \n",
    "            ## store effect strength\n",
    "            old_logit_diff = (old_logits[...,0]-old_logits[...,1])\n",
    "            new_logit_diff = (new_logits[...,0]-new_logits[...,1])\n",
    "            effect_strength[layer, hook_i] = torch.abs((patched_logit_diff-old_logit_diff) / (new_logit_diff-old_logit_diff))\n",
    "            #torch.abs(patched_logits_v-old_logits_v)\n",
    "            \n",
    "    vector_direction = torch.stack(vector_direction) \n",
    "    vector_direction = torch.movedim(vector_direction,0,1)\n",
    "    vector_direction = vector_direction.view(2,model.cfg.n_layers,-1,model.cfg.d_model)\n",
    "    return effect_strength.detach(), vector_direction.detach()\n",
    "        \n",
    "    \n",
    "old_prompt = [\"The relationship between Harry Potter and Ronald Weasley is\"]\n",
    "#new_prompt = [\"Harry absolutely hates Ron. The relationship between Harry Potter and Ronald Weasley is\"]\n",
    "new_prompt = [\"The relationship between Jack and Mary is\"]\n",
    "\n",
    "old_logits, old_activs = measuring.prompt_with_cache(model,old_prompt,logit_idx=scale_idx)\n",
    "new_logits, new_activs = measuring.prompt_with_cache(model,new_prompt,logit_idx=scale_idx)\n",
    "\n",
    "vector_scale, vector_dir = patch_activs(model, old_logits, new_logits, new_activs, old_prompt, scale_idx)\n",
    "#plot_heatmap(vector_scale.cpu().numpy(), title='Patching Effect', cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_patching_loop(model, prompt_pairs, scale_idx):\n",
    "    \n",
    "    all_vector_scale, all_vector_dir = [],[]    \n",
    "    for (prompt_1, prompt_2) in tqdm(prompt_pairs, position=0): \n",
    "        \n",
    "        old_logits, old_activs = measuring.prompt_with_cache(model,prompt_1,logit_idx=scale_idx)\n",
    "        new_logits, new_activs = measuring.prompt_with_cache(model,prompt_2,logit_idx=scale_idx)\n",
    "        vector_scale, vector_dir = patch_activs(model, old_logits, new_logits, new_activs, prompt_1, scale_idx)        \n",
    "        \n",
    "        all_vector_scale.append(vector_scale)\n",
    "        all_vector_dir.append(vector_dir)\n",
    "\n",
    "    vector_scale = torch.stack(all_vector_scale).detach() ## shape: prompt, layers, att vs mlp\n",
    "    vector_scale = vector_scale.mean(0)\n",
    "    vector_dir = torch.stack(all_vector_dir).detach() ## shape: prompt, new vs old, layers, att vs mlp, emb dim \n",
    "    return vector_scale, vector_dir\n",
    "        \n",
    "vector_scale, vector_dir = run_patching_loop(model, prompt_pairs, scale_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91488165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_heatmap(array, title='', xticklabels=[\"attn_out\", \"mlp_out\"], cmap=\"binary\"):\n",
    "    titlefont, labelsize=12, 10\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(2, 4), gridspec_kw={'hspace': 0.4})\n",
    "    ax = sns.heatmap(array, cmap=mpl.colormaps[cmap], xticklabels=xticklabels, square=False)\n",
    "    ax.set_title(title, fontsize=titlefont, color=\"black\", loc='center',  y=1.1)\n",
    "    ax.set_ylabel('layers', fontsize=labelsize)\n",
    "    \n",
    "    mean_effect = list(map(lambda x: \"%.3f\" % x, list(array.max(0))))\n",
    "    for i, x_tick_label in enumerate(ax.get_xticklabels()):\n",
    "        ax.text(x_tick_label.get_position()[0]-0.5, -0.2, f\"max:\\n{mean_effect[i]}\", fontsize=labelsize, color=\"black\", verticalalignment='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "plot_heatmap(vector_scale.cpu().numpy(), title='Patching Effect', cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee628a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def dim_reduction(embs, reduction=\"pca\"):\n",
    "    \n",
    "    x = embs.view(-1, embs.shape[-1]).cpu().detach().numpy()\n",
    "    \n",
    "    y_len = int((embs.shape[0] * embs.shape[1])/2)\n",
    "    y = ([0] * y_len + [1] * y_len)\n",
    "    colormap = np.array(['g', 'r'])\n",
    "    y = colormap[y]\n",
    "    \n",
    "    if reduction == \"pca\":\n",
    "        pca = PCA(n_components=2)\n",
    "        x_2D = pca.fit_transform(x)\n",
    "        \n",
    "    fig, (ax) = plt.subplots(1, figsize=(5, 5), gridspec_kw={'hspace': 0.40})\n",
    "    ax.scatter(x_2D[:, 0], x_2D[:, 1], c=y)\n",
    "    plt.show()\n",
    "    \n",
    "print(vector_dir.shape)\n",
    "#dim_reduction(vector_dir.mean(0)[...,0,:].cpu())\n",
    "a = vector_dir[...,0,:].mean(-2)\n",
    "print(a.shape)\n",
    "dim_reduction(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58632bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "measureLM_venv",
   "language": "python",
   "name": "measurelm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
