{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Entity Susceptibility Scores in Movie Reviews with Conditional MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from measuring.estimate_probs import (\n",
    "    estimate_prob_y_given_context_and_entity,\n",
    "    estimate_prob_x_given_e,\n",
    "    estimate_prob_next_word_given_x_and_entity,\n",
    "    estimate_cmi,\n",
    "    score_model_for_next_word_prob,\n",
    "    create_position_ids_from_input_ids,\n",
    "    sharded_score_model,\n",
    "    estimate_entity_score,\n",
    "    kl_div,\n",
    "    difference,\n",
    "    difference_p_good_only,\n",
    "    difference_abs_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_IN_8BIT = True\n",
    "\n",
    "COMPUTE_CMI = False\n",
    "COMPUTE_KL = False\n",
    "COMPUTE_GOOD_BAD = False\n",
    "COMPUTE_GOOD_BAD_ABS = False\n",
    "COMPUTE_GOOD_BAD_P_GOOD_ONLY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"toy_movie_reviews.yaml\", \"r\") as file:\n",
    "    review_adjs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adjs_train = review_adjs[\"positive_adjectives_train\"]\n",
    "neg_adjs_train = review_adjs[\"negative_adjectives_train\"]\n",
    "\n",
    "pos_verbs = review_adjs[\"positive_verbs\"]\n",
    "neg_verbs = review_adjs[\"negative_verbs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.38974520.kevidu/ipykernel_57607/3708651189.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  movies = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "movies = pd.read_csv(\n",
    "    \"movies_metadata.csv\"\n",
    ")  # https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=movies_metadata.csv\n",
    "movies = (\n",
    "    movies[[\"title\", \"vote_count\", \"popularity\"]].dropna().sort_values(by=\"vote_count\")\n",
    ")\n",
    "movies[\"popularity\"] = movies[\"popularity\"].astype(float)\n",
    "movies_sample = pd.concat([movies.iloc[:250], movies.iloc[-250:]], axis=0)\n",
    "# movies_sample = pd.concat([movies.iloc[-250:]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45465</th>\n",
       "      <td>Queerama</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23680</th>\n",
       "      <td>Brothers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23677</th>\n",
       "      <td>Willie and Phil</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23674</th>\n",
       "      <td>Luther</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.170857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23671</th>\n",
       "      <td>Brother Rat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26564</th>\n",
       "      <td>Deadpool</td>\n",
       "      <td>11444.0</td>\n",
       "      <td>187.860492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17818</th>\n",
       "      <td>The Avengers</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>89.887648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14551</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>12114.0</td>\n",
       "      <td>185.070892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12481</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>12269.0</td>\n",
       "      <td>123.167259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15480</th>\n",
       "      <td>Inception</td>\n",
       "      <td>14075.0</td>\n",
       "      <td>29.108149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title  vote_count  popularity\n",
       "45465         Queerama         0.0    0.163015\n",
       "23680         Brothers         0.0    0.007073\n",
       "23677  Willie and Phil         0.0    0.326500\n",
       "23674           Luther         0.0    1.170857\n",
       "23671      Brother Rat         0.0    0.174691\n",
       "...                ...         ...         ...\n",
       "26564         Deadpool     11444.0  187.860492\n",
       "17818     The Avengers     12000.0   89.887648\n",
       "14551           Avatar     12114.0  185.070892\n",
       "12481  The Dark Knight     12269.0  123.167259\n",
       "15480        Inception     14075.0   29.108149\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='vote_count', ylabel='popularity'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WHETHER TO USE VOTE COUNT OR POPULARITY\n",
    "\n",
    "# movies.sort_values(by=\"popularity\").tail(50)\n",
    "# movies.sort_values(by=\"vote_count\").tail(50)\n",
    "sns.regplot(data=movies, x=\"vote_count\", y=\"popularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Here's a review for the movie '{}': I thought this movie was {}, I {} it.\\nConclusion: This movie is\"\n",
    "template.format(movies_sample[\"title\"].iloc[0], pos_adjs_train[0], pos_verbs[0])\n",
    "movies_sample[\"sentence\"] = movies_sample[\"title\"].apply(\n",
    "    lambda title: template.format(title, pos_adjs_train[0], pos_verbs[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive contexts:  72\n",
      "Number of negative contexts:  72\n",
      "[\"Here's a movie review: 'The movie was dreadful and I hated it'. \", \"Here's a movie review: 'The movie was depressing and I disliked it'. \", \"Here's a movie review: 'The movie was nasty and I despised it'. \", \"Here's a movie review: 'The movie was ugly and I hated it'. \", \"Here's a movie review: 'The movie was annoying and I disliked it'. \", \"Here's a movie review: 'The movie was frustrating and I despised it'. \", \"Here's a movie review: 'The movie was unpleasant and I hated it'. \", \"Here's a movie review: 'The movie was awful and I disliked it'. \"]\n"
     ]
    }
   ],
   "source": [
    "context_template = \"Here's a movie review: 'The movie was {} and I {} it'. \"\n",
    "num_adjs = min(len(pos_adjs_train), len(neg_adjs_train))\n",
    "num_verbs = min(len(pos_verbs), len(neg_verbs))\n",
    "\n",
    "pos_contexts = [\n",
    "    context_template.format(adj, verb)\n",
    "    for (adj, verb) in product(pos_adjs_train[:num_adjs], pos_verbs[:num_verbs])\n",
    "]\n",
    "neg_contexts = [\n",
    "    context_template.format(adj, verb)\n",
    "    for (adj, verb) in product(neg_adjs_train[:num_adjs], neg_verbs[:num_verbs])\n",
    "]\n",
    "contexts = neg_contexts\n",
    "# contexts = pos_contexts + neg_contexts\n",
    "\n",
    "print(\"Number of positive contexts: \", len(pos_contexts))\n",
    "print(\"Number of negative contexts: \", len(neg_contexts))\n",
    "print(contexts[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_query(query, entity, context, prefix=\"\"):\n",
    "    return prefix + context + query.format(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "# def fill_in_template(template: str, title: str, adj: str, verb: str):\n",
    "#     return template.format(title, adj, verb)\n",
    "\n",
    "# pos_template_partial = partial(fill_in_template, template=template, verb=pos_verbs[0])\n",
    "# neg_template_partial = partial(fill_in_template, template=template, verb=neg_verbs[0])\n",
    "\n",
    "# for adj in pos_adjs_train:\n",
    "#     movies_sample[f\"pos_{adj}_context\"] = movies_sample[\"title\"].apply(lambda title: pos_template_partial(title=title, adj=adj))\n",
    "\n",
    "# for adj in neg_adjs_train:\n",
    "#     movies_sample[f\"neg_{adj}_context\"] = movies_sample[\"title\"].apply(lambda title: neg_template_partial(title=title, adj=adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>popularity</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45465</th>\n",
       "      <td>Queerama</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163015</td>\n",
       "      <td>Here's a review for the movie 'Queerama': I th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23680</th>\n",
       "      <td>Brothers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007073</td>\n",
       "      <td>Here's a review for the movie 'Brothers': I th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23677</th>\n",
       "      <td>Willie and Phil</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326500</td>\n",
       "      <td>Here's a review for the movie 'Willie and Phil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23674</th>\n",
       "      <td>Luther</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.170857</td>\n",
       "      <td>Here's a review for the movie 'Luther': I thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23671</th>\n",
       "      <td>Brother Rat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174691</td>\n",
       "      <td>Here's a review for the movie 'Brother Rat': I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title  vote_count  popularity  \\\n",
       "45465         Queerama         0.0    0.163015   \n",
       "23680         Brothers         0.0    0.007073   \n",
       "23677  Willie and Phil         0.0    0.326500   \n",
       "23674           Luther         0.0    1.170857   \n",
       "23671      Brother Rat         0.0    0.174691   \n",
       "\n",
       "                                                sentence  \n",
       "45465  Here's a review for the movie 'Queerama': I th...  \n",
       "23680  Here's a review for the movie 'Brothers': I th...  \n",
       "23677  Here's a review for the movie 'Willie and Phil...  \n",
       "23674  Here's a review for the movie 'Luther': I thou...  \n",
       "23671  Here's a review for the movie 'Brother Rat': I...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "================================================================================\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('StdEnv'), PosixPath('gcc/4.8.5')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('//0.0.0.0'), PosixPath('0/user/kevidu'), PosixPath('https')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1;python_gpu/3.10.4'), PosixPath('1;r/4.2.2'), PosixPath('1;gcc/8.2.0'), PosixPath('1;openblas/0.3.15'), PosixPath('1;hdf5/1.10.1'), PosixPath('1'), PosixPath('1;eth_proxy'), PosixPath('1;cudnn/8.2.1.32'), PosixPath('1;julia/1.8.5'), PosixPath('1;nccl/2.11.4-1'), PosixPath('StdEnv'), PosixPath('1;cuda/11.3.1')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1;/cluster/apps/gcc-8.2.0/cuda-11.3.1-o54iuxgz6jm4csvkstuj5hjg4tvd44h3'), PosixPath('1;/cluster/apps/gcc-8.2.0/r-4.2.2-ydfaklhfrhw5dy6qcfzxlxfviwovcord'), PosixPath('1;/cluster/apps/gcc-8.2.0/cudnn-8.2.1.32-yqvbgr3teq3v6xu5eyc75xhbl2ya343j'), PosixPath('1;/cluster/apps/gcc-8.2.0/nccl-2.11.4-1-pwkiz23vbeac3vt5ykybdwzaykprizb2'), PosixPath('1;/cluster/apps/gcc-8.2.0/openblas-0.3.15-huwxbhezdzoo74awrgoz6sd2qndpmdva'), PosixPath('1'), PosixPath('1;/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('3128'), PosixPath('//proxy.ethz.ch'), PosixPath('http')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('3128'), PosixPath('//proxy.ethz.ch'), PosixPath('http')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1;/cluster/apps/lmodules/Core/gcc/8.2.0.lua'), PosixPath('1;/cluster/apps/lmodules/Compiler/gcc/8.2.0/python_gpu/3.10.4.lua'), PosixPath('1;/cluster/apps/lmodules/Core/eth_proxy.lua'), PosixPath('1;/cluster/apps/lmodules/Compiler/gcc/8.2.0/julia/1.8.5.lua'), PosixPath('1;/cluster/apps/lmodules/Compiler/gcc/8.2.0/cudnn/8.2.1.32.lua'), PosixPath('1;/cluster/apps/lmodules/Compiler/gcc/8.2.0/cuda/11.3.1.lua'), PosixPath('1;/cluster/apps/lmodules/Compiler/gcc/8.2.0/nccl/2.11.4-1.lua'), PosixPath('1;/cluster/apps/lmodules/Compiler/gcc/8.2.0/openblas/0.3.15.lua'), PosixPath('1;/cluster/apps/lmodules/Compiler/gcc/8.2.0/hdf5/1.10.1.lua'), PosixPath('1'), PosixPath('1;/cluster/apps/lmodules/Compiler/gcc/8.2.0/r/4.2.2.lua')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/cluster/work/cotterell/kdu/.cache/huggingface/misc')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/cluster/work/cotterell/kdu/miniforge3/etc/xml/catalog file')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/cluster/apps/lmodules/Linux')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('python_gpu/3.10.4'), PosixPath('cudnn/8.2.1.32'), PosixPath('cuda/11.3.1'), PosixPath('hdf5/1.10.1'), PosixPath('openblas/0.3.15'), PosixPath('eth_proxy'), PosixPath('julia/1.8.5'), PosixPath('r/4.2.2'), PosixPath('StdEnv'), PosixPath('gcc/8.2.0'), PosixPath('nccl/2.11.4-1')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('3128'), PosixPath('//proxy.ethz.ch'), PosixPath('http')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('//jupyter.euler.hpc.ethz.ch/hub/api'), PosixPath('https')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1;/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/bin'), PosixPath('1;/cluster/slurm/apps/bin'), PosixPath('2;/usr/local/sbin'), PosixPath('1;/sbin'), PosixPath('1;/cluster/apps/sfos/bin'), PosixPath('1;/usr/sbin'), PosixPath('1;/cluster/apps/gcc-8.2.0/openblas-0.3.15-huwxbhezdzoo74awrgoz6sd2qndpmdva/bin'), PosixPath('1;/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/bin'), PosixPath('1;/cluster/apps/gcc-8.2.0/cuda-11.3.1-o54iuxgz6jm4csvkstuj5hjg4tvd44h3/bin'), PosixPath('1;/cluster/apps/nss/gcc-8.2.0/julia/1.8.5/x86_64/bin'), PosixPath('1;/cluster/apps/local'), PosixPath('1;/usr/local/bin'), PosixPath('1;/bin'), PosixPath('1;/cluster/apps/gcc-8.2.0/r-4.2.2-ydfaklhfrhw5dy6qcfzxlxfviwovcord/bin'), PosixPath('1;/usr/bin'), PosixPath('1')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1;/cluster/apps/gcc-8.2.0/nccl-2.11.4-1-pwkiz23vbeac3vt5ykybdwzaykprizb2/include'), PosixPath('1;/cluster/apps/gcc-8.2.0/openblas-0.3.15-huwxbhezdzoo74awrgoz6sd2qndpmdva/include'), PosixPath('1;/cluster/apps/gcc-8.2.0/cudnn-8.2.1.32-yqvbgr3teq3v6xu5eyc75xhbl2ya343j/include'), PosixPath('1;/cluster/apps/gcc-8.2.0/cuda-11.3.1-o54iuxgz6jm4csvkstuj5hjg4tvd44h3/include'), PosixPath('1;/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/include'), PosixPath('1'), PosixPath('1;/cluster/apps/gcc-8.2.0/r-4.2.2-ydfaklhfrhw5dy6qcfzxlxfviwovcord/rlib/R/include')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/cluster/work/cotterell/kdu/.cache/huggingface/datasets')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/python')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('servers!server=kevidu/\", \"access'), PosixPath('[\"access'), PosixPath('servers!user=kevidu\"]')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1;/cluster/apps/gcc-8.2.0/openblas-0.3.15-huwxbhezdzoo74awrgoz6sd2qndpmdva/lib'), PosixPath('1;/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/lib'), PosixPath('1;/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/lib64'), PosixPath('1;/cluster/apps/gcc-8.2.0/cudnn-8.2.1.32-yqvbgr3teq3v6xu5eyc75xhbl2ya343j/lib64'), PosixPath('1;/cluster/apps/gcc-8.2.0/nccl-2.11.4-1-pwkiz23vbeac3vt5ykybdwzaykprizb2/lib'), PosixPath('1'), PosixPath('1;/cluster/apps/gcc-8.2.0/r-4.2.2-ydfaklhfrhw5dy6qcfzxlxfviwovcord/rlib/R/lib'), PosixPath('1;/cluster/apps/gcc-8.2.0/cuda-11.3.1-o54iuxgz6jm4csvkstuj5hjg4tvd44h3/lib64')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('//jupyter.euler.hpc.ethz.ch/hub/api/users/kevidu/activity'), PosixPath('https')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1;/cluster/apps/gcc-8.2.0/openblas-0.3.15-huwxbhezdzoo74awrgoz6sd2qndpmdva/lib'), PosixPath('1;/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64'), PosixPath('1;/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/lib64'), PosixPath('1;/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/lib'), PosixPath('1;/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/lib'), PosixPath('1;/cluster/apps/nss/gcc-8.2.0/julia/1.8.5/x86_64/lib'), PosixPath('1;/cluster/apps/gcc-8.2.0/cudnn-8.2.1.32-yqvbgr3teq3v6xu5eyc75xhbl2ya343j/lib64'), PosixPath('1;/cluster/apps/gcc-8.2.0/nccl-2.11.4-1-pwkiz23vbeac3vt5ykybdwzaykprizb2/lib'), PosixPath('1'), PosixPath('1;/cluster/apps/gcc-8.2.0/r-4.2.2-ydfaklhfrhw5dy6qcfzxlxfviwovcord/rlib/R/lib'), PosixPath('1;/cluster/apps/gcc-8.2.0/cuda-11.3.1-o54iuxgz6jm4csvkstuj5hjg4tvd44h3/lib64')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/user/kevidu/oauth_callback')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1'), PosixPath('1;/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64/pkgconfig'), PosixPath('1;/cluster/apps/gcc-8.2.0/openblas-0.3.15-huwxbhezdzoo74awrgoz6sd2qndpmdva/lib/pkgconfig')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1;/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/share/man'), PosixPath('1;/cluster/apps/lsf/10.1/man'), PosixPath('1;/cluster/apps/sfos/share/man/man1'), PosixPath('1'), PosixPath('1;/cluster/apps/gcc-4.8.5/lmod-7.7.13-epk3osxslctnrx6gabjmwtudqm2vfbxf/lmod/lmod/share/man')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/user/kevidu')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('servers!server=kevidu/\", \"access'), PosixPath('[\"access'), PosixPath('servers!user=kevidu\"]')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('() {  eval $($LMOD_DIR/ml_cmd \"$@\")\\n}')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "DEBUG: Possible options found for libcudart.so: {PosixPath('/cluster/apps/gcc-8.2.0/cuda-11.3.1-o54iuxgz6jm4csvkstuj5hjg4tvd44h3/lib64/libcudart.so'), PosixPath('/cluster/apps/gcc-8.2.0/cuda-11.3.1-o54iuxgz6jm4csvkstuj5hjg4tvd44h3/lib64/libcudart.so.11.0')}\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=113, Highest Compute Capability: 7.5.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "CUDA SETUP: Required library version not found: libbitsandbytes_cuda113.so. Maybe you need to compile it from source?\n",
      "CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n",
      "\n",
      "================================================ERROR=====================================\n",
      "CUDA SETUP: CUDA detection failed! Possible reasons:\n",
      "1. You need to manually override the PyTorch CUDA version. Please see: \"https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "2. CUDA driver not installed\n",
      "3. CUDA not installed\n",
      "4. You have multiple conflicting CUDA libraries\n",
      "5. Required library not pre-compiled for this bitsandbytes release!\n",
      "CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.\n",
      "CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n",
      "================================================================================\n",
      "\n",
      "CUDA SETUP: Something unexpected happened. Please compile from source:\n",
      "git clone https://github.com/TimDettmers/bitsandbytes.git\n",
      "cd bitsandbytes\n",
      "CUDA_VERSION=113 make cuda11x\n",
      "python setup.py install\n",
      "CUDA SETUP: Setup Failed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/kevidu/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "\n",
      "  warn(msg)\n",
      "/cluster/home/kevidu/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: /cluster/work/cotterell/kdu/miniforge3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/cluster/home/kevidu/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/cluster/apps/gcc-8.2.0/cuda-11.3.1-o54iuxgz6jm4csvkstuj5hjg4tvd44h3/lib64/libcudart.so'), PosixPath('/cluster/apps/gcc-8.2.0/cuda-11.3.1-o54iuxgz6jm4csvkstuj5hjg4tvd44h3/lib64/libcudart.so.11.0')}.. We select the PyTorch default libcudart.so, which is {torch.version.cuda},but this might missmatch with the CUDA version that is needed for bitsandbytes.To override this behavior set the BNB_CUDA_VERSION=<version string, e.g. 122> environmental variableFor example, if you want to use the CUDA version 122BNB_CUDA_VERSION=122 python ...OR set the environmental variable in your .bashrc: export BNB_CUDA_VERSION=122In the case of a manual override, make sure you set the LD_LIBRARY_PATH, e.g.export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.2\n",
      "  warn(msg)\n",
      "/cluster/home/kevidu/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: /cluster/apps/gcc-8.2.0/hdf5-1.10.1-qj3ju3qfhvucsk5eevrtb2lehbux5nmv/lib:/cluster/apps/nss/gcc-8.2.0/julia/1.8.5/x86_64/lib:/cluster/apps/gcc-8.2.0/r-4.2.2-ydfaklhfrhw5dy6qcfzxlxfviwovcord/rlib/R/lib:/cluster/apps/gcc-8.2.0/nccl-2.11.4-1-pwkiz23vbeac3vt5ykybdwzaykprizb2/lib:/cluster/apps/gcc-8.2.0/cudnn-8.2.1.32-yqvbgr3teq3v6xu5eyc75xhbl2ya343j/lib64:/cluster/apps/gcc-8.2.0/cuda-11.3.1-o54iuxgz6jm4csvkstuj5hjg4tvd44h3/lib64:/cluster/apps/gcc-8.2.0/openblas-0.3.15-huwxbhezdzoo74awrgoz6sd2qndpmdva/lib:/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/lib64:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/lib:/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:: did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a929c64c974347ada07fb34eca32cab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/cluster/work/cotterell/kdu/transformers/src/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 39, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9106, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.38811877.kevidu/ipykernel_19063/2029166144.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(model(**inputs).logits[0, -1]), k=10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'........#######******\\n********................#####~~................................'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "if gpu_mem > 16:\n",
    "    # model_name = \"EleutherAI/pythia-1b-deduped\"\n",
    "    model_name = \"EleutherAI/pythia-6.9b-deduped\"\n",
    "    # model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "else: \n",
    "    model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "    \n",
    "print(f\"Loading model {model_name} on GPU with {gpu_mem} memory.\")\n",
    "\n",
    "try:\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        model_name, load_in_8bit=LOAD_IN_8BIT, device_map=\"auto\"\n",
    "    )\n",
    "except:\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=False,\n",
    "    ).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "\n",
    "query = \"On a scale from 1 to 5 stars, the quality of this movie, '{}', is rated \"\n",
    "inputs = tokenizer(\n",
    "    format_query(\n",
    "        query=query, entity=movies_sample.iloc[0][\"title\"], context=contexts[0]\n",
    "    ),\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "# inputs = tokenizer(movies_sample.iloc[0][\"sentence\"], return_tensors=\"pt\").to(device)\n",
    "tokens = model.generate(**inputs)\n",
    "probs, top_tokens = torch.topk(\n",
    "    torch.nn.functional.softmax(model(**inputs).logits[0, -1]), k=10\n",
    ")\n",
    "print(sum(probs))\n",
    "tokenizer.decode(top_tokens)\n",
    "# print(tokens)\n",
    "# tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([   17, 10528, 26561], device='cuda:0'),\n",
       " 1: tensor([  18,  531, 4041], device='cuda:0'),\n",
       " 2: tensor([  19, 9389, 7910], device='cuda:0'),\n",
       " 3: tensor([   20, 13524, 11831], device='cuda:0'),\n",
       " 4: tensor([   21, 12496, 14039], device='cuda:0'),\n",
       " 5: tensor([   22, 12071, 25438], device='cuda:0')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_map = {\n",
    "    0: [\"0\", \"zero\", \"Zero\"],\n",
    "    1: [\"1\", \"one\", \"One\"],\n",
    "    2: [\"2\", \"two\", \"Two\"],\n",
    "    3: [\"3\", \"three\", \"Three\"],\n",
    "    4: [\"4\", \"four\", \"Four\"],\n",
    "    5: [\"5\", \"five\", \"Five\"],\n",
    "}\n",
    "answer_map_token_ids = {\n",
    "    k: torch.tensor(tokenizer.convert_tokens_to_ids(v), device=model.device)\n",
    "    for k, v in answer_map.items()\n",
    "}\n",
    "answer_map_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_CMI:\n",
    "    query = \"On a scale from 1 to 5 stars, the quality of this movie, '{}', is rated \"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    tqdm.pandas()\n",
    "    movies_sample[\"susceptibility_score\"] = movies_sample[\"title\"].progress_apply(\n",
    "        lambda e: estimate_cmi(\n",
    "            query=query,\n",
    "            entity=e,\n",
    "            contexts=contexts,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            answer_map=None,\n",
    "        )\n",
    "    )\n",
    "    # movies_sample[\"susceptibility_score\"] = movies_sample[\"title\"].progress_apply(lambda e: estimate_cmi(query=query, entity=e, contexts=contexts, model=model, tokenizer=tokenizer, answer_map=answer_map_token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.26.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_CMI:\n",
    "    movies_sample.value_counts(\"susceptibility_score\")\n",
    "    sns.regplot(data=movies_sample, x=\"vote_count\", y=\"susceptibility_score\")\n",
    "    x = movies_sample[\"vote_count\"]\n",
    "    x = sm.add_constant(x)\n",
    "    y = movies_sample[\"susceptibility_score\"]\n",
    "    ols_model = sm.OLS(y, x).fit()\n",
    "    ols_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_KL:\n",
    "    query = \"On a scale from 1 to 5 stars, the quality of this movie, '{}', is rated \"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    tqdm.pandas()\n",
    "    # movies_sample[\"susceptibility_score_kl\"] = movies_sample[\"title\"].progress_apply(lambda e: estimate_entity_score(query=query, entity=e, contexts=contexts, model=model, tokenizer=tokenizer, distance_metric=kl_div, answer_map=None))\n",
    "    movies_sample[\"susceptibility_score_kl\"] = movies_sample[\"title\"].progress_apply(\n",
    "        lambda e: estimate_entity_score(\n",
    "            query=query,\n",
    "            entity=e,\n",
    "            contexts=contexts,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            distance_metric=kl_div,\n",
    "            answer_map=answer_map_token_ids,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_KL:\n",
    "    print(movies_sample.value_counts(\"susceptibility_score_kl\"))\n",
    "    sns.regplot(data=movies_sample, x=\"vote_count\", y=\"susceptibility_score_kl\")\n",
    "    x = movies_sample[\"vote_count\"]\n",
    "    x = sm.add_constant(x)\n",
    "    y = movies_sample[\"susceptibility_score_kl\"]\n",
    "    ols_model = sm.OLS(y, x).fit()\n",
    "    print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_map_good_bad = {\n",
    "    0: [\"bad\", \"Bad\"],\n",
    "    1: [\"good\", \"Good\"],\n",
    "}\n",
    "answer_map_good_bad_token_ids = {\n",
    "    k: torch.tensor(tokenizer.convert_tokens_to_ids(v), device=model.device)\n",
    "    for k, v in answer_map_good_bad.items()\n",
    "}\n",
    "\n",
    "if COMPUTE_GOOD_BAD:\n",
    "    query = \"The quality of this movie, '{}', is rated \"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    tqdm.pandas()\n",
    "    movies_sample[\"susceptibility_score_good_bad\"] = movies_sample[\n",
    "        \"title\"\n",
    "    ].progress_apply(\n",
    "        lambda e: estimate_entity_score(\n",
    "            query=query,\n",
    "            entity=e,\n",
    "            contexts=contexts,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            distance_metric=difference,\n",
    "            answer_map=answer_map_good_bad_token_ids,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_GOOD_BAD:\n",
    "    print(movies_sample.value_counts(\"susceptibility_score_good_bad\"))\n",
    "    sns.regplot(data=movies_sample, x=\"vote_count\", y=\"susceptibility_score_good_bad\")\n",
    "    x = movies_sample[\"vote_count\"]\n",
    "    x = sm.add_constant(x)\n",
    "    y = movies_sample[\"susceptibility_score_good_bad\"]\n",
    "    ols_model = sm.OLS(y, x).fit()\n",
    "    print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_GOOD_BAD_ABS:\n",
    "    query = \"The quality of this movie, '{}', is rated \"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    tqdm.pandas()\n",
    "    movies_sample[\"susceptibility_score_good_bad_abs\"] = movies_sample[\n",
    "        \"title\"\n",
    "    ].progress_apply(\n",
    "        lambda e: estimate_entity_score(\n",
    "            query=query,\n",
    "            entity=e,\n",
    "            contexts=contexts,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            distance_metric=difference_abs_val,\n",
    "            answer_map=answer_map_good_bad_token_ids,\n",
    "        )\n",
    "    )\n",
    "\n",
    "if COMPUTE_GOOD_BAD_ABS:\n",
    "    print(movies_sample.value_counts(\"susceptibility_score_good_bad_abs\"))\n",
    "    sns.regplot(\n",
    "        data=movies_sample, x=\"vote_count\", y=\"susceptibility_score_good_bad_abs\"\n",
    "    )\n",
    "    x = movies_sample[\"vote_count\"]\n",
    "    x = sm.add_constant(x)\n",
    "    y = movies_sample[\"susceptibility_score_good_bad_abs\"]\n",
    "    ols_model = sm.OLS(y, x).fit()\n",
    "    print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_GOOD_BAD_P_GOOD_ONLY:\n",
    "    query = \"The quality of this movie, '{}', is rated \"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    tqdm.pandas()\n",
    "    movies_sample[\"susceptibility_score_good_bad_p_good_only\"] = movies_sample[\n",
    "        \"title\"\n",
    "    ].progress_apply(\n",
    "        lambda e: estimate_entity_score(\n",
    "            query=query,\n",
    "            entity=e,\n",
    "            contexts=contexts,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            distance_metric=difference_p_good_only,\n",
    "            answer_map=answer_map_good_bad_token_ids,\n",
    "        )\n",
    "    )\n",
    "\n",
    "if COMPUTE_GOOD_BAD_P_GOOD_ONLY:\n",
    "    print(movies_sample.value_counts(\"susceptibility_score_good_bad_p_good_only\"))\n",
    "    sns.regplot(\n",
    "        data=movies_sample,\n",
    "        x=\"vote_count\",\n",
    "        y=\"susceptibility_score_good_bad_p_good_only\",\n",
    "    )\n",
    "    x = movies_sample[\"vote_count\"]\n",
    "    x = sm.add_constant(x)\n",
    "    y = movies_sample[\"susceptibility_score_good_bad_p_good_only\"]\n",
    "    ols_model = sm.OLS(y, x).fit()\n",
    "    print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23337, 24004, 48623, 25655, 0, 0, 0, 0]\n",
      "The capital of Jerinima is Love. The capital of Jerinima is\n",
      "Setting model.config.pad_token_id to model.config.eos_token_id\n",
      "0.6257470265084227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/work/cotterell/kdu/measureLM/measuring/estimate_probs.py:258: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.sum(prob_x_y_given_e * np.nan_to_num(np.log(prob_y_given_context_and_entity / prob_y_given_e)))\n",
      "/cluster/work/cotterell/kdu/measureLM/measuring/estimate_probs.py:258: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(prob_x_y_given_e * np.nan_to_num(np.log(prob_y_given_context_and_entity / prob_y_given_e)))\n",
      "/scratch/tmp.38811877.kevidu/ipykernel_19063/2658322796.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1589, 'Ä Love', 10540), (0.0857, 'Ä Peace', 20000), (0.04846, 'Ä Death', 14645), (0.04733, 'Ä the', 253), (0.03687, 'Ä War', 3660), (0.0352, 'Ä also', 671), (0.02849, 'Ä H', 388), (0.02635, 'Ä Truth', 23921), (0.02554, 'Ä a', 247), (0.02495, 'Ä L', 418), (0.02005, 'Ä Life', 7813), (0.01988, 'Ä love', 2389), (0.01958, 'Ä not', 417), (0.01912, 'Ä Joy', 20614), (0.01811, 'Ä Hope', 15541), (0.01755, 'Ä Pain', 21869), (0.01701, 'Ä Jer', 5633), (0.01688, 'ÄŠ', 187), (0.01675, 'Ä hate', 9239), (0.0165, 'Ä Pass', 11271), (0.01585, 'Ä Fear', 32005), (0.015366, 'Ä Light', 10315), (0.01378, 'Ä peace', 6330), (0.01367, 'Ä S', 322), (0.013565, '...', 1051), (0.01346, 'Ä Sex', 16678), (0.01254, 'Ä Beauty', 36974), (0.01151, 'Ä Power', 8916), (0.011154, 'Ä T', 308), (0.01081, 'Ä Money', 22405), (0.01064, 'Ä D', 399), (0.01064, 'Ä Justice', 8293), (0.01012, 'Ä P', 367), (0.00973, 'Ä Faith', 27955), (0.00969, 'Ä Mer', 7612), (0.00918, 'Ä God', 2656), (0.00876, 'Ä F', 401), (0.00876, 'Ä Happ', 34386), (0.00865, 'Ä Earth', 7565), (0.00842, 'Ä Des', 3666), (0.00842, 'Ä Hon', 10916), (0.008255, 'Ä E', 444), (0.00743, 'Ä G', 443), (0.007374, 'Ä war', 2137), (0.00726, 'Ä W', 411), (0.007233, 'Ä death', 2471), (0.007202, 'Ä Freedom', 20574), (0.007175, 'Ä M', 353), (0.00709, 'Ä Sh', 1608), (0.00704, 'Ä A', 329)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.3057, 'Ä Hope', 15541), (0.0954, 'Ä Jer', 5633), (0.05106, 'Ä the', 253), (0.02629, 'Ä also', 671), (0.02469, 'Ä a', 247), (0.02214, 'Ä K', 611), (0.02097, 'Ä D', 399), (0.02063, 'Ä New', 1457), (0.01923, 'Ä not', 417), (0.01779, 'Ä N', 427), (0.01765, 'Ä called', 1925), (0.01752, 'Ä S', 322), (0.01685, 'Ä T', 308), (0.01645, 'Ä L', 418), (0.01607, 'Ä J', 500), (0.01595, 'ÄŠ', 187), (0.01475, 'Ä H', 388), (0.013535, 'Ä M', 353), (0.01322, 'Ä in', 275), (0.01281, 'Ä R', 416), (0.01242, 'Ä B', 378), (0.01213, 'Ä E', 444), (0.01194, 'Ä P', 367), (0.01194, 'Ä Z', 1503), (0.01157, 'Ä G', 443), (0.0114, 'Ä A', 329), (0.01087, 'Ä C', 330), (0.01062, 'Ä named', 4907), (0.01046, 'Ä O', 473), (0.010376, 'Ä Y', 714), (0.01029, 'Ä V', 657), (0.00956, 'Ä The', 380), (0.00948, 'Ä Ar', 1780), (0.0085, 'Ä F', 401), (0.007652, 'Ä Al', 1219), (0.00759, 'Ä Sh', 1608), (0.007328, 'Ä U', 530), (0.006596, 'Ä El', 3599), (0.006596, 'Ä An', 743), (0.005913, 'Ä now', 1024), (0.0058, 'Ä St', 659), (0.005466, 'Ä located', 4441), (0.005383, 'Ä W', 411), (0.00492, 'Ä I', 309), (0.004845, 'Ä Water', 10205), (0.004498, 'Ä known', 1929), (0.004444, 'Ä Hel', 7518), (0.00436, 'Ä Cor', 3094), (0.00436, 'Ä Mar', 2398), (0.004093, 'Ä Ter', 12324)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.3718, 'Ä Rat', 18194), (0.10486, 'Ä Jer', 5633), (0.03513, 'Ä the', 253), (0.03326, 'Ä R', 416), (0.03004, 'Ä K', 611), (0.0263, 'Ä also', 671), (0.02066, 'Ä J', 500), (0.01866, 'Ä D', 399), (0.01822, 'Ä N', 427), (0.01634, 'Ä M', 353), (0.01572, 'Ä S', 322), (0.015114, 'Ä in', 275), (0.01442, 'Ä L', 418), (0.01398, 'Ä Z', 1503), (0.01344, 'Ä T', 308), (0.01313, 'ÄŠ', 187), (0.01263, 'Ä a', 247), (0.01205, 'Ä G', 443), (0.01159, 'Ä B', 378), (0.011314, 'Ä Y', 714), (0.01123, 'Ä called', 1925), (0.01055, 'Ä O', 473), (0.01047, 'Ä H', 388), (0.01022, 'Ä A', 329), (0.00991, 'Ä not', 417), (0.00945, 'Ä E', 444), (0.00881, 'Ä P', 367), (0.008606, 'Ä C', 330), (0.00848, 'Ä V', 657), (0.007687, 'Ä Sh', 1608), (0.0076, 'Ä F', 401), (0.007393, 'Ä Ar', 1780), (0.007164, 'Ä U', 530), (0.006626, 'Ä located', 4441), (0.006203, 'Ä W', 411), (0.00567, 'Ä New', 1457), (0.005344, 'Ä Al', 1219), (0.00524, 'Ä named', 4907), (0.005043, 'Ä An', 743), (0.004555, 'Ä El', 3599), (0.004448, 'Ä now', 1024), (0.004097, 'Ä The', 380), (0.003761, 'Ä I', 309), (0.003561, 'Ä Mar', 2398), (0.003399, 'Ä known', 1929), (0.003319, 'Ä Bar', 4033), (0.003294, 'Ä X', 1594), (0.003082, 'Ä Ch', 775), (0.003069, 'Ä Tel', 21123), (0.002975, 'Ä Le', 2070)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.611, 'Ä Washington', 5041), (0.05054, 'Ä New', 1457), (0.02058, 'Ä Seattle', 16335), (0.01817, 'Ä the', 253), (0.0172, 'Ä Jer', 5633), (0.01616, 'ÄŠ', 187), (0.0159, 'Ä D', 399), (0.01393, 'Ä in', 275), (0.01318, 'Ä Los', 8742), (0.013084, 'Ä also', 671), (0.01201, 'Ä a', 247), (0.01102, 'Ä San', 5003), (0.01043, 'Ä located', 4441), (0.0095, 'Ä not', 417), (0.00893, 'Ä K', 611), (0.008316, 'Ä W', 411), (0.007633, 'Ä B', 378), (0.00695, 'Ä Chicago', 8068), (0.00633, 'Ä Y', 714), (0.00628, 'Ä Denver', 20734), (0.00623, 'Ä Atlanta', 18267), (0.005898, 'Ä St', 659), (0.00537, 'Ä called', 1925), (0.005306, 'Ä L', 418), (0.005184, 'Ä J', 500), (0.005024, 'Ä P', 367), (0.00489, 'Ä London', 4693), (0.00487, 'Ä An', 743), (0.00454, 'Ä Olymp', 10406), (0.004505, 'Ä Boston', 9693), (0.00445, 'Ä C', 330), (0.004417, 'Ä T', 308), (0.0044, 'Ä G', 443), (0.00435, 'Ä N', 427), (0.004086, 'Ä R', 416), (0.004005, 'Ä S', 322), (0.003975, 'Ä Jefferson', 19759), (0.00372, 'Ä M', 353), (0.003704, 'Ä Philadelphia', 14289), (0.00369, 'Ä named', 4907), (0.003494, 'Ä E', 444), (0.00332, 'Ä Austin', 16916), (0.00322, 'Ä Sacramento', 36405), (0.003012, 'Ä Alexandria', 38783), (0.002966, 'Ä Baltimore', 19585), (0.002954, 'Ä Springfield', 42669), (0.00283, 'Ä Ge', 3096), (0.002808, 'Ä O', 473), (0.002808, 'Ä Water', 10205), (0.002796, 'Ä Las', 15878)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.3252, 'Ä H', 388), (0.0552, 'Ä Jer', 5633), (0.0461, 'Ä the', 253), (0.0317, 'Ä hate', 9239), (0.02931, 'Ä Fear', 32005), (0.02248, 'Ä S', 322), (0.02231, 'Ä D', 399), (0.02213, 'Ä a', 247), (0.01953, 'Ä L', 418), (0.01907, 'Ä Death', 14645), (0.0185, 'Ä Love', 10540), (0.0171, 'Ä also', 671), (0.01697, 'ÄŠ', 187), (0.01645, 'Ä B', 378), (0.01451, 'Ä J', 500), (0.013954, 'Ä M', 353), (0.01385, 'Ä R', 416), (0.01331, 'Ä W', 411), (0.013214, 'Ä not', 417), (0.01301, 'Ä T', 308), (0.01261, 'Ä K', 611), (0.01251, 'Ä E', 444), (0.012314, 'Ä V', 657), (0.01184, 'Ä N', 427), (0.01175, 'Ä War', 3660), (0.0113, 'Ä A', 329), (0.01054, 'Ä U', 530), (0.01054, 'Ä G', 443), (0.01021, 'Ä P', 367), (0.01013, 'Ä F', 401), (0.01013, 'Ä O', 473), (0.00997, 'Ä fear', 4709), (0.00982, 'Ä An', 743), (0.009155, 'Ä Peace', 20000), (0.00853, 'Ä Sh', 1608), (0.00795, 'Ä The', 380), (0.007706, 'Ä Des', 3666), (0.007412, 'Ä C', 330), (0.0071, 'Ä Z', 1503), (0.00696, 'Ä Hat', 23676), (0.00683, 'Ä death', 2471), (0.00659, 'Ä Ar', 1780), (0.006073, 'Ä in', 275), (0.005795, '...', 1051), (0.005573, 'Ä En', 3035), (0.005424, 'Ä Dark', 14182), (0.005424, 'Ä Un', 914), (0.005424, 'Ä love', 2389), (0.0053, 'Ä called', 1925), (0.005215, 'Ä Cor', 3094)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.419, 'Ä M', 353), (0.07874, 'Ä Jer', 5633), (0.03632, 'Ä the', 253), (0.027, 'Ä K', 611), (0.02309, 'Ä also', 671), (0.01884, 'Ä J', 500), (0.0184, 'Ä a', 247), (0.01799, 'Ä in', 275), (0.01677, 'Ä L', 418), (0.01599, 'Ä N', 427), (0.01491, 'Ä D', 399), (0.01468, 'Ä S', 322), (0.01401, 'Ä Z', 1503), (0.01368, 'Ä T', 308), (0.01326, 'Ä R', 416), (0.012955, 'Ä called', 1925), (0.01198, 'Ä Y', 714), (0.01189, 'Ä B', 378), (0.01189, 'Ä O', 473), (0.01152, 'Ä A', 329), (0.010735, 'Ä H', 388), (0.01066, 'Ä not', 417), (0.01066, 'Ä Ar', 1780), (0.010574, 'Ä located', 4441), (0.01033, 'Ä E', 444), (0.00986, 'Ä G', 443), (0.009705, 'Ä P', 367), (0.00904, 'Ä C', 330), (0.00849, 'Ä V', 657), (0.0083, 'ÄŠ', 187), (0.007324, 'Ä U', 530), (0.00721, 'Ä F', 401), (0.007042, 'Ä named', 4907), (0.00677, 'Ä Sh', 1608), (0.00649, 'Ä New', 1457), (0.00619, 'Ä now', 1024), (0.005505, 'Ä Al', 1219), (0.005356, 'Ä The', 380), (0.00503, 'Ä Ma', 7057), (0.004993, 'Ä El', 3599), (0.004803, 'Ä W', 411), (0.00439, 'Ä known', 1929), (0.004074, 'Ä An', 743), (0.003815, 'Ä Bar', 4033), (0.003569, 'Ä Gu', 3262), (0.003569, 'Ä Tel', 21123), (0.0033, 'Ä I', 309), (0.003162, 'Ä on', 327), (0.003101, 'Ä Am', 3052), (0.003088, 'Ä Q', 1165)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.3547, 'Ä Ch', 775), (0.0842, 'Ä Jer', 5633), (0.03653, 'Ä K', 611), (0.02779, 'Ä D', 399), (0.0225, 'Ä Z', 1503), (0.02197, 'Ä S', 322), (0.02017, 'Ä J', 500), (0.0197, 'Ä L', 418), (0.01955, 'Ä M', 353), (0.0194, 'Ä T', 308), (0.0191, 'Ä B', 378), (0.01895, 'Ä N', 427), (0.0188, 'Ä C', 330), (0.01822, 'Ä R', 416), (0.0178, 'Ä the', 253), (0.01752, 'Ä G', 443), (0.01523, 'Ä Y', 714), (0.01487, 'Ä H', 388), (0.013435, 'Ä O', 473), (0.013435, 'Ä V', 657), (0.01323, 'Ä P', 367), (0.01312, 'Ä A', 329), (0.01149, 'Ä E', 444), (0.01096, 'Ä U', 530), (0.01063, 'Ä Ar', 1780), (0.01063, 'Ä New', 1457), (0.00975, 'Ä also', 671), (0.00938, 'ÄŠ', 187), (0.00874, 'Ä Sh', 1608), (0.00793, 'Ä F', 401), (0.007393, 'Ä Al', 1219), (0.007305, 'Ä W', 411), (0.007248, 'Ä An', 743), (0.006916, 'Ä El', 3599), (0.0062, 'Ä in', 275), (0.005802, 'Ä X', 1594), (0.00545, 'Ä I', 309), (0.00526, 'Ä called', 1925), (0.005222, 'Ä St', 659), (0.00504, 'Ä a', 247), (0.00457, 'Ä The', 380), (0.00408, 'Ä Cor', 3094), (0.00394, 'Ä Gu', 3262), (0.00379, 'Ä Am', 3052), (0.003746, 'Ä Bar', 4033), (0.00373, 'Ä Th', 596), (0.003702, 'Ä Mar', 2398), (0.003687, 'Ä not', 417), (0.003616, 'Ä Le', 2070), (0.003464, 'Ä San', 5003)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.468, 'Ä Rab', 21385), (0.07404, 'Ä Jer', 5633), (0.03111, 'Ä R', 416), (0.029, 'Ä K', 611), (0.02539, 'Ä J', 500), (0.01978, 'Ä the', 253), (0.01872, 'Ä S', 322), (0.01872, 'Ä also', 671), (0.0183, 'Ä N', 427), (0.0183, 'Ä M', 353), (0.01704, 'Ä Y', 714), (0.0164, 'Ä D', 399), (0.01349, 'Ä Al', 1219), (0.01338, 'Ä Z', 1503), (0.01297, 'Ä B', 378), (0.01127, 'Ä T', 308), (0.010025, 'Ä A', 329), (0.00979, 'Ä in', 275), (0.009415, 'Ä G', 443), (0.009125, 'Ä O', 473), (0.00899, 'Ä H', 388), (0.00891, 'Ä L', 418), (0.00891, 'Ä Sh', 1608), (0.007507, 'Ä Ar', 1780), (0.007507, 'Ä U', 530), (0.006573, 'Ä E', 444), (0.005985, 'Ä a', 247), (0.00594, 'Ä F', 401), (0.00571, 'Ä P', 367), (0.005283, 'Ä W', 411), (0.00524, 'Ä Am', 3052), (0.0052, 'Ä El', 3599), (0.00512, 'Ä Q', 1165), (0.004963, 'Ä called', 1925), (0.0049, 'Ä located', 4441), (0.004845, 'Ä Bar', 4033), (0.004395, 'Ä Mar', 2398), (0.004326, 'Ä C', 330), (0.004227, 'Ä not', 417), (0.00421, 'Ä Ma', 7057), (0.004032, 'Ä V', 657), (0.003925, 'Ä I', 309), (0.00391, 'Ä named', 4907), (0.003895, 'Ä Ab', 3506), (0.003658, 'ÄŠ', 187), (0.00363, 'Ä Ra', 11605), (0.00363, 'Ä Ad', 2006), (0.00356, 'Ä An', 743), (0.003344, 'Ä Rah', 27614), (0.003344, 'Ä Tel', 21123)]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.38811877.kevidu/ipykernel_19063/2658322796.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1428, 'Ä the', 253), (0.09515, 'Ä Jer', 5633), (0.08136, 'Ä a', 247), (0.06696, 'Ä called', 1925), (0.0664, 'Ä in', 275), (0.04288, 'Ä located', 4441), (0.0334, 'Ä K', 611), (0.02856, 'Ä J', 500), (0.02156, 'Ä N', 427), (0.02106, 'Ä named', 4907), (0.01888, 'Ä now', 1024), (0.01845, 'Ä known', 1929), (0.0168, 'Ä not', 417), (0.01602, 'Ä Y', 714), (0.01602, 'Ä on', 327), (0.01566, 'Ä T', 308), (0.01529, 'Ä also', 671), (0.015175, 'Ä R', 416), (0.014824, 'Ä D', 399), (0.01403, 'Ä M', 353), (0.01258, 'Ä S', 322), (0.01209, 'Ä A', 329), (0.012, 'Ä B', 378), (0.011185, 'Ä O', 473), (0.011185, 'Ä C', 330), (0.01067, 'Ä at', 387), (0.00987, 'Ä an', 271), (0.00987, 'Ä G', 443), (0.00949, 'Ä L', 418), (0.00949, 'Ä New', 1457), (0.009056, 'Ä Z', 1503), (0.00871, 'Ä U', 530), (0.008644, 'Ä E', 444), (0.00851, 'Ä Ar', 1780), (0.008446, 'Ä P', 367), (0.00838, 'Ä El', 3599), (0.00728, 'Ä Sh', 1608), (0.007168, 'Ä H', 388), (0.006706, 'Ä Al', 1219), (0.006706, ',', 13), (0.005802, 'Ä currently', 4390), (0.00567, 'ÄŠ', 187), (0.00541, 'Ä Port', 6162), (0.00535, 'Ä Mar', 2398), (0.00524, 'Ä V', 657), (0.005024, 'Ä W', 411), (0.00472, 'Ä An', 743), (0.004486, 'Ä Am', 3052), (0.004467, 'Ä one', 581), (0.00445, 'Ä Kar', 12604)]\n",
      "7\n",
      "[('The capital of Jerinima is in Gimlia,', tensor([  275,   443,   303, 19702,    13], device='cuda:0')), ('The capital of Jerinima is the city Jilin,', tensor([  253,  2846,   500, 39806,    13], device='cuda:0')), ('The capital of Jerinima is Al Qaryah;', tensor([1219, 1165,  552, 1240,   28], device='cuda:0')), ('The capital of Jerinima is Lana. Some\\n', tensor([ 418, 3230,   15, 3808,  187], device='cuda:0')), ('The capital of Jerinima is Juban.\\n', tensor([500, 538, 266,  15, 187], device='cuda:0')), (\"The capital of Jerinima is also the nation's seat\", tensor([ 671,  253, 5674,  434, 7319], device='cuda:0')), ('The capital of Jerinima is called Hasten.', tensor([1925,  388,  505,  257,   15], device='cuda:0')), ('The capital of Jerinima is the city of Jeruz', tensor([ 253, 2846,  273, 5633, 7958], device='cuda:0')), ('The capital of Jerinima is known as Jidder', tensor([1929,  347,  500,  301,  491], device='cuda:0')), ('The capital of Jerinima is a place located in Central', tensor([ 247, 1659, 4441,  275, 8170], device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "from measuring.estimate_probs import get_prob_next_word\n",
    "\n",
    "FICTIONAL_CAPITALS = True\n",
    "\n",
    "fictional_capitals = [\n",
    "    \"Love\",\n",
    "    \"Hope\",\n",
    "    \"Rat\",\n",
    "    \"Washington\",\n",
    "    \"Hate\",\n",
    "    \"Maze\",\n",
    "    \"Chilis\",\n",
    "    \"Rabahadum\",\n",
    "]\n",
    "print(tokenizer.convert_tokens_to_ids(fictional_capitals))\n",
    "if FICTIONAL_CAPITALS:\n",
    "    entity = \"Jerinima\"\n",
    "    contexts = [f\"The capital of {entity} is {c}. \" for c in fictional_capitals]\n",
    "    # print(contexts)\n",
    "    query = \"The capital of {} is\"\n",
    "    # query = \"Q: What is the capital of {}?\\n\\n A:\"\n",
    "    print(format_query(query=query, entity=entity, context=contexts[0]))\n",
    "    print(estimate_cmi(query, entity, contexts, model, tokenizer))\n",
    "\n",
    "    for context in contexts:\n",
    "        logits, inds = torch.topk(\n",
    "            get_prob_next_word(\n",
    "                model,\n",
    "                tokenizer(\n",
    "                    format_query(query=query, entity=entity, context=context),\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(model.device),\n",
    "            ),\n",
    "            k=50,\n",
    "        )\n",
    "        print(\n",
    "            list(\n",
    "                zip(\n",
    "                    torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n",
    "                    tokenizer.convert_ids_to_tokens(inds[0]),\n",
    "                    inds[0].detach().cpu().numpy(),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "    logits, inds = torch.topk(\n",
    "        get_prob_next_word(\n",
    "            model,\n",
    "            tokenizer(\n",
    "                format_query(query=query, entity=entity, context=\"\"),\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device),\n",
    "        ),\n",
    "        k=50,\n",
    "    )\n",
    "    print(\n",
    "        list(\n",
    "            zip(\n",
    "                torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n",
    "                tokenizer.convert_ids_to_tokens(inds[0]),\n",
    "                inds[0].detach().cpu().numpy(),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        format_query(query=query, entity=entity, context=\"\"),\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    print(len(tokens[\"input_ids\"][0]))\n",
    "    samples = model.generate(\n",
    "        **tokens,\n",
    "        num_return_sequences=10,\n",
    "        do_sample=True,\n",
    "        max_length=len(tokens[\"input_ids\"][0]) + 5,\n",
    "    )\n",
    "    print(list(zip(tokenizer.batch_decode(samples), [s[-5:] for s in samples])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is Love. Q: What is the capital of Germany?\n",
      "\n",
      " A: \n",
      "\n",
      "\n",
      "0.027830579496534148\n",
      "[(0.2377, 'Ä Q', 1165), (0.1431, 'Ä A', 329), (0.1376, '  ', 50276), (0.10144, 'Q', 50), (0.05515, 'Ä The', 380), (0.01862, 'Ä B', 378), (0.013, 'Ä D', 399), (0.012405, 'Ä What', 1737), (0.01156, 'Ä In', 496), (0.01143, 'A', 34), (0.01134, 'ÄŠ', 187), (0.01121, '    ', 50274), (0.01116, '   ', 50275), (0.01082, 'Ä I', 309), (0.01048, 'Ä (', 313), (0.010445, 'Ä 1', 337), (0.009735, 'Ä C', 330), (0.00951, 'The', 510), (0.00904, 'Ä This', 831), (0.009, 'Ä Germany', 6176), (0.00897, 'Ä It', 733), (0.0088, 'Ä Love', 10540), (0.00873, 'Ä .', 964), (0.00718, 'Ä And', 1244), (0.006878, 'Ã‚Å‚', 575), (0.00669, 'Ä An', 743), (0.00636, 'Ä L', 418), (0.005905, 'Ä T', 308), (0.005836, 'Ä P', 367), (0.00544, 'Ä E', 444), (0.00544, 'Ä H', 388), (0.005375, '                        ', 50254), (0.005314, 'Ä Source', 12269), (0.0048, 'Ä O', 473), (0.004765, 'Ä R', 416), (0.004726, 'Ä S', 322), (0.00467, 'Ä Answer', 37741), (0.004635, 'Ä M', 353), (0.004124, 'Ä How', 1359), (0.00409, 'Ä _', 795), (0.00409, 'Ä Why', 6049), (0.003887, 'Ä K', 611), (0.003828, 'Ä **', 1401), (0.003782, 'Ä \"', 346), (0.003782, 'Ä ', 209), (0.00371, 'Ä ,', 1157), (0.003458, 'Ä N', 427), (0.003378, 'Ä If', 1310), (0.003365, 'Ä F', 401), (0.003325, 'Ä For', 1198)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.2086, 'Ä Q', 1165), (0.1503, 'Q', 50), (0.1401, '  ', 50276), (0.1049, 'Ä A', 329), (0.05704, 'Ä The', 380), (0.01881, 'A', 34), (0.01767, 'Ä B', 378), (0.0166, 'The', 510), (0.015236, 'ÄŠ', 187), (0.01328, '    ', 50274), (0.012825, 'Ä 1', 337), (0.01205, 'Ä Germany', 6176), (0.01182, '   ', 50275), (0.011185, 'Ä In', 496), (0.01043, 'Ä D', 399), (0.01027, 'Ä (', 313), (0.01011, 'Ä What', 1737), (0.01011, 'Ä I', 309), (0.00942, 'Ä .', 964), (0.00913, 'Ä It', 733), (0.008644, 'Ä This', 831), (0.007812, 'Ã‚Å‚', 575), (0.0076, 'Ä C', 330), (0.006527, 'Ä P', 367), (0.006374, 'Ä T', 308), (0.006325, 'Ä Hope', 15541), (0.006203, 'Ä H', 388), (0.006035, '                        ', 50254), (0.005943, 'Ä And', 1244), (0.005783, 'Ä An', 743), (0.00468, 'Ä L', 418), (0.004627, 'Ä R', 416), (0.00461, 'Ä E', 444), (0.004555, 'Ä O', 473), (0.004265, 'Ä **', 1401), (0.004246, '---', 1532), (0.004246, 'Ä M', 353), (0.004246, 'Ä S', 322), (0.004196, '1', 18), (0.004147, 'Germany', 37796), (0.004066, 'Ä ', 209), (0.004036, 'In', 688), (0.004005, 'Ä Answer', 37741), (0.003944, '      ', 50272), (0.00393, 'Ä How', 1359), (0.003914, 'Ä Source', 12269), (0.003881, 'Ä N', 427), (0.003836, 'Ä _', 795), (0.00372, '     ', 50273), (0.003689, 'What', 1276)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.2583, 'Ä Q', 1165), (0.1484, '  ', 50276), (0.1362, 'Q', 50), (0.0981, 'Ä A', 329), (0.04422, 'Ä The', 380), (0.02312, 'Ä B', 378), (0.015396, '    ', 50274), (0.01516, 'A', 34), (0.01458, '   ', 50275), (0.014465, 'The', 510), (0.01242, 'Ä 1', 337), (0.01092, 'Ä D', 399), (0.0099, 'ÄŠ', 187), (0.00905, 'Ä In', 496), (0.00864, 'Ä I', 309), (0.00847, 'Ä (', 313), (0.00815, 'Ä Germany', 6176), (0.00812, 'Ä .', 964), (0.00796, 'Ä What', 1737), (0.00793, 'Ã‚Å‚', 575), (0.007866, 'Ä It', 733), (0.007713, 'Ä C', 330), (0.00739, 'Ä Rat', 18194), (0.00739, 'Ä R', 416), (0.0073, 'Ä This', 831), (0.00622, 'Ä P', 367), (0.005844, 'Ä T', 308), (0.005363, '                        ', 50254), (0.0052, 'Ä An', 743), (0.00492, 'Ä And', 1244), (0.00455, 'Ä S', 322), (0.004498, 'Ä **', 1401), (0.004326, '1', 18), (0.004177, 'Ä E', 444), (0.004017, '     ', 50273), (0.004, '      ', 50272), (0.00397, 'Ä _', 795), (0.00394, 'Ä O', 473), (0.003925, 'Ä H', 388), (0.00376, 'Ä L', 418), (0.003729, 'Ä M', 353), (0.003572, 'Ä N', 427), (0.003504, 'Ä F', 401), (0.003448, 'Ä ', 209), (0.003448, 'Germany', 37796), (0.003408, '---', 1532), (0.003344, 'In', 688), (0.003279, 'Ä How', 1359), (0.003216, 'Ä |', 1040), (0.003164, 'What', 1276)]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.38811877.kevidu/ipykernel_19063/3570535499.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.283, 'Ä Q', 1165), (0.1588, 'Q', 50), (0.139, '  ', 50276), (0.083, 'Ä A', 329), (0.04477, 'Ä The', 380), (0.0166, 'A', 34), (0.014885, 'The', 510), (0.01454, 'Ä B', 378), (0.01431, '    ', 50274), (0.014206, '   ', 50275), (0.01334, 'ÄŠ', 187), (0.01141, 'Ä 1', 337), (0.01039, 'Ä D', 399), (0.00928, 'Ä In', 496), (0.00875, 'Ä .', 964), (0.008514, 'Ä What', 1737), (0.00835, 'Ä (', 313), (0.00819, 'Ä I', 309), (0.00769, 'Ä This', 831), (0.007088, 'Ä It', 733), (0.007057, 'Ã‚Å‚', 575), (0.006477, '                        ', 50254), (0.006477, 'Ä C', 330), (0.0063, 'Ä Germany', 6176), (0.005264, 'Ä P', 367), (0.00487, 'Ä An', 743), (0.004684, 'Ä T', 308), (0.00456, 'Ä And', 1244), (0.00452, 'Ä **', 1401), (0.00435, '     ', 50273), (0.00435, '1', 18), (0.004116, '      ', 50272), (0.0041, 'Ä O', 473), (0.003735, 'Ä _', 795), (0.003675, '---', 1532), (0.003662, 'Ä L', 418), (0.003618, 'Ä H', 388), (0.003576, 'Ä E', 444), (0.003426, 'In', 688), (0.003399, 'What', 1276), (0.00336, 'Ä M', 353), (0.00327, 'Ä |', 1040), (0.003258, 'Ä R', 416), (0.003258, 'Ä ', 209), (0.003157, 'Ä S', 322), (0.00312, 'Ä F', 401), (0.003084, 'Ä How', 1359), (0.003036, 'Ä Source', 12269), (0.003023, 'Ä Answer', 37741), (0.003023, 'Ä If', 1310)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.2041, 'Ä Q', 1165), (0.1414, 'Q', 50), (0.1277, 'Ä A', 329), (0.11536, '  ', 50276), (0.06128, 'Ä The', 380), (0.02895, 'Ä H', 388), (0.02307, 'Ä B', 378), (0.01688, 'A', 34), (0.01549, 'The', 510), (0.01235, 'Ä What', 1737), (0.01202, 'Ä I', 309), (0.01086, 'Ä In', 496), (0.010605, '   ', 50275), (0.0104, 'Ä It', 733), (0.0102, 'Ä C', 330), (0.0102, 'ÄŠ', 187), (0.01004, 'Ã‚Å‚', 575), (0.00962, '    ', 50274), (0.00958, 'Ä (', 313), (0.00958, 'Ä 1', 337), (0.00932, 'Ä D', 399), (0.00886, 'Ä .', 964), (0.00832, 'Ä This', 831), (0.0081, 'Ä Germany', 6176), (0.00648, 'Ä An', 743), (0.006382, 'Ä And', 1244), (0.006187, 'Ä T', 308), (0.005924, 'Ä L', 418), (0.005314, '                        ', 50254), (0.005108, 'Ä \"', 346), (0.00505, 'Ä E', 444), (0.005028, 'Ä S', 322), (0.00499, 'Ä O', 473), (0.004875, 'Ä P', 367), (0.00467, 'Ä R', 416), (0.00467, 'Ä _', 795), (0.00417, '1', 18), (0.004025, 'Ä M', 353), (0.004025, 'Ä Why', 6049), (0.003857, 'Ä **', 1401), (0.003708, '---', 1532), (0.00368, 'Ä How', 1359), (0.003666, 'Ä K', 611), (0.003666, 'What', 1276), (0.003593, 'Ä F', 401), (0.003456, 'Ä N', 427), (0.003403, 'Ä Where', 7900), (0.00339, 'Ä You', 1422), (0.00326, 'In', 688), (0.003235, 'Ä If', 1310)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.2593, 'Ä Q', 1165), (0.1432, '  ', 50276), (0.1355, 'Q', 50), (0.0882, 'Ä A', 329), (0.03912, 'Ä The', 380), (0.02487, 'Ä B', 378), (0.01645, 'Ä M', 353), (0.01497, 'A', 34), (0.013954, 'The', 510), (0.01385, '    ', 50274), (0.01374, '   ', 50275), (0.013214, 'Ä 1', 337), (0.01311, 'Ä D', 399), (0.010864, 'ÄŠ', 187), (0.01013, 'Ä I', 309), (0.01009, 'Ä .', 964), (0.009514, 'Ä What', 1737), (0.00905, 'Ã‚Å‚', 575), (0.00901, 'Ä In', 496), (0.00827, 'Ä Germany', 6176), (0.00817, 'Ä C', 330), (0.00747, 'Ä (', 313), (0.007015, 'Ä It', 733), (0.00699, 'Ä T', 308), (0.006855, 'Ä This', 831), (0.006695, 'Ä P', 367), (0.005295, 'Ä An', 743), (0.005173, '                        ', 50254), (0.00488, 'Ä H', 388), (0.00488, 'Ä E', 444), (0.004803, 'Ä L', 418), (0.00471, 'Ä S', 322), (0.004547, 'Ä R', 416), (0.004223, 'Ä And', 1244), (0.004158, '     ', 50273), (0.004124, 'Ä O', 473), (0.00411, 'Ä ', 209), (0.004044, 'Ä Source', 12269), (0.004044, 'Ä N', 427), (0.003998, 'Ä **', 1401), (0.00392, 'Ä F', 401), (0.003906, '1', 18), (0.003891, 'Ä ,', 1157), (0.00383, 'What', 1276), (0.0038, 'Ä Answer', 37741), (0.003712, 'Germany', 37796), (0.00367, '      ', 50272), (0.003626, 'Ä K', 611), (0.003626, 'Ä How', 1359), (0.0035, '---', 1532)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.1915, 'Q', 50), (0.1664, 'Ä Q', 1165), (0.1538, '  ', 50276), (0.09705, 'Ä A', 329), (0.03683, 'Ä The', 380), (0.0303, 'Ä B', 378), (0.02695, 'A', 34), (0.02148, 'The', 510), (0.015236, '    ', 50274), (0.01448, '   ', 50275), (0.01398, 'Ä D', 399), (0.0121, 'Ä 1', 337), (0.01123, 'ÄŠ', 187), (0.009995, 'Ä I', 309), (0.009796, 'Ä .', 964), (0.00976, 'Ä C', 330), (0.00835, 'Ã‚Å‚', 575), (0.007996, 'Ä What', 1737), (0.007965, 'Ä In', 496), (0.007168, 'Ä (', 313), (0.00703, 'Ä Ch', 775), (0.006603, 'Ä P', 367), (0.006577, 'Ä This', 831), (0.006477, 'Ä T', 308), (0.006424, '1', 18), (0.006374, 'Ä It', 733), (0.00565, 'B', 35), (0.005516, '                        ', 50254), (0.005367, 'In', 688), (0.005184, '---', 1532), (0.00508, 'Ä Germany', 6176), (0.005043, 'Ä L', 418), (0.004887, 'Ä S', 322), (0.004887, '     ', 50273), (0.004868, 'Ä An', 743), (0.00472, 'What', 1276), (0.004627, 'Ä M', 353), (0.004574, '      ', 50272), (0.004417, 'Ä E', 444), (0.004364, 'This', 1552), (0.004314, 'An', 1145), (0.00428, 'Ä H', 388), (0.004166, 'Ä And', 1244), (0.003838, 'Ä N', 427), (0.003777, 'Ä F', 401), (0.003763, 'Ä **', 1401), (0.003748, 'Ä R', 416), (0.00372, 'Germany', 37796), (0.003689, 'Ä Source', 12269), (0.003689, '#', 4)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.2041, 'Ä Q', 1165), (0.1589, '  ', 50276), (0.1565, 'Ä A', 329), (0.1145, 'Q', 50), (0.03546, 'Ä The', 380), (0.0294, 'Ä B', 378), (0.01637, 'A', 34), (0.01611, '    ', 50274), (0.01456, 'Ä 1', 337), (0.01378, '   ', 50275), (0.013565, 'Ä D', 399), (0.0104, 'The', 510), (0.01032, 'Ä I', 309), (0.009964, 'ÄŠ', 187), (0.008965, 'Ä In', 496), (0.008896, 'Ä (', 313), (0.00862, 'Ä C', 330), (0.00794, 'Ä .', 964), (0.00764, 'Ä What', 1737), (0.00761, 'Ä R', 416), (0.00732, 'Ä M', 353), (0.00732, 'Ä P', 367), (0.00729, 'Ã‚Å‚', 575), (0.006794, 'Ä It', 733), (0.006638, 'Ä This', 831), (0.006638, 'Ä T', 308), (0.006535, 'Ä S', 322), (0.005905, 'Ä E', 444), (0.005676, 'Ä O', 473), (0.00557, 'Ä An', 743), (0.005524, 'Ä K', 611), (0.005356, '                        ', 50254), (0.004654, 'Ä And', 1244), (0.00458, '     ', 50273), (0.00458, 'Ä N', 427), (0.004528, 'Ä Germany', 6176), (0.004456, 'Ä F', 401), (0.004253, 'Ä ', 209), (0.004234, 'Ä H', 388), (0.004204, 'Ä Source', 12269), (0.00419, 'Ä L', 418), (0.004105, 'Ä 3', 495), (0.003653, '      ', 50272), (0.003511, 'Ä |', 1040), (0.003338, 'Ä Rab', 21385), (0.003223, 'Ä Answer', 37741), (0.003197, 'Ä How', 1359), (0.003124, 'Ä Question', 19782), (0.003004, '1', 18), (0.002934, '---', 1532)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.1733, 'Q', 50), (0.1616, 'Ä B', 378), (0.09576, '  ', 50276), (0.0935, 'Ä Q', 1165), (0.05084, 'Ä A', 329), (0.03207, 'Ä The', 380), (0.02744, 'Ä D', 399), (0.02007, 'Ä What', 1737), (0.01991, 'Ä C', 330), (0.016, '    ', 50274), (0.01575, 'Ä I', 309), (0.01469, 'Ã‚Å‚', 575), (0.01412, 'Ä 1', 337), (0.0139, 'B', 35), (0.01347, 'The', 510), (0.01327, 'ÄŠ', 187), (0.01161, 'Ä Question', 19782), (0.01117, 'Ä (', 313), (0.010956, 'Ä Source', 12269), (0.00933, 'Ä In', 496), (0.0093, 'Ä This', 831), (0.00926, 'Ä L', 418), (0.009224, '   ', 50275), (0.0089, 'Ä P', 367), (0.00857, 'A', 34), (0.0084, 'Ä E', 444), (0.00783, 'Ä T', 308), (0.007736, 'Ä How', 1359), (0.007587, 'Ä Why', 6049), (0.007587, 'What', 1276), (0.00747, 'Ä H', 388), (0.005795, '                        ', 50254), (0.00575, 'Ä M', 353), (0.005466, 'Ä An', 743), (0.005424, 'Ä It', 733), (0.00534, 'Ä If', 1310), (0.005234, 'Ä ', 209), (0.005215, 'Ä S', 322), (0.004997, 'Ä .', 964), (0.00498, 'Ä K', 611), (0.00486, 'Ä N', 427), (0.00482, 'How', 2347), (0.004692, 'Ä F', 401), (0.004063, '---', 1532), (0.003891, 'Ä Qu', 3277), (0.003815, 'Ä And', 1244), (0.003815, 'Ä Can', 2615), (0.003727, 'This', 1552), (0.003683, 'Ä **', 1401), (0.003683, 'In', 688)]\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.38811877.kevidu/ipykernel_19063/3570535499.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Q: What is the capital of Germany?\\n\\n A: \\n\\n Why did the man at', tensor([6049,  858,  253,  637,  387], device='cuda:0')), ('Q: What is the capital of Germany?\\n\\n A: \\n\\n B: Berlin \\n\\n C', tensor([  378,    27, 12911, 15267,   330], device='cuda:0')), ('Q: What is the capital of Germany?\\n\\n A: \\n\\n Q: What is the', tensor([1165,   27, 1737,  310,  253], device='cuda:0')), ('Q: What is the capital of Germany?\\n\\n A: \\n\\n    1 (a)', tensor([50274,    18,   313,    66,    10], device='cuda:0')), ('Q: What is the capital of Germany?\\n\\n A: \\n\\n  * The capital of', tensor([50276,    11,   380,  5347,   273], device='cuda:0')), ('Q: What is the capital of Germany?\\n\\n A: \\n\\n Q: Was there a', tensor([ 1165,    27, 12349,   627,   247], device='cuda:0')), ('Q: What is the capital of Germany?\\n\\n A: \\n\\nQ: Yes, but', tensor([  50,   27, 6279,   13,  533], device='cuda:0')), ('Q: What is the capital of Germany?\\n\\n A: \\n\\n The capital of Germany is', tensor([ 380, 5347,  273, 6176,  310], device='cuda:0')), ('Q: What is the capital of Germany?\\n\\n A: \\n\\n B: Schwerin', tensor([ 378,   27, 3697, 8358,  249], device='cuda:0')), ('Q: What is the capital of Germany?\\n\\n A: \\n\\nQ:\\n\\nIs', tensor([  50,   27,  187,  187, 2513], device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "from measuring.estimate_probs import get_prob_next_word\n",
    "\n",
    "FICTIONAL_CAPITALS = True\n",
    "\n",
    "fictional_capitals = [\n",
    "    \"Love\",\n",
    "    \"Hope\",\n",
    "    \"Rat\",\n",
    "    \"Washington\",\n",
    "    \"Hate\",\n",
    "    \"Maze\",\n",
    "    \"Chilis\",\n",
    "    \"Rabahadum\",\n",
    "]\n",
    "tokenizer.convert_tokens_to_ids(fictional_capitals)\n",
    "if FICTIONAL_CAPITALS:\n",
    "    entity = \"Germany\"\n",
    "    contexts = [f\"The capital of {entity} is {c}. \" for c in fictional_capitals]\n",
    "    # print(contexts)\n",
    "    query = \"The capital of {} is \\n\\n\"\n",
    "    query = \"Q: What is the capital of {}?\\n\\n A: \\n\\n\"\n",
    "    print(format_query(query=query, entity=entity, context=contexts[0]))\n",
    "    print(estimate_cmi(query, entity, contexts, model, tokenizer))\n",
    "\n",
    "    for context in contexts:\n",
    "        logits, inds = torch.topk(\n",
    "            get_prob_next_word(\n",
    "                model,\n",
    "                tokenizer(\n",
    "                    format_query(query=query, entity=entity, context=context),\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(model.device),\n",
    "            ),\n",
    "            k=50,\n",
    "        )\n",
    "        print(\n",
    "            list(\n",
    "                zip(\n",
    "                    torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n",
    "                    tokenizer.convert_ids_to_tokens(inds[0]),\n",
    "                    inds[0].detach().cpu().numpy(),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "    logits, inds = torch.topk(\n",
    "        get_prob_next_word(\n",
    "            model,\n",
    "            tokenizer(\n",
    "                format_query(query=query, entity=entity, context=\"\"),\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device),\n",
    "        ),\n",
    "        k=50,\n",
    "    )\n",
    "    print(\n",
    "        list(\n",
    "            zip(\n",
    "                torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n",
    "                tokenizer.convert_ids_to_tokens(inds[0]),\n",
    "                inds[0].detach().cpu().numpy(),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        format_query(query=query, entity=entity, context=\"\"),\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    print(len(tokens[\"input_ids\"][0]))\n",
    "    samples = model.generate(\n",
    "        **tokens,\n",
    "        num_return_sequences=10,\n",
    "        do_sample=True,\n",
    "        max_length=len(tokens[\"input_ids\"][0]) + 5,\n",
    "    )\n",
    "    print(list(zip(tokenizer.batch_decode(samples), [s[-5:] for s in samples])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pontima is Love. The capital of Pontima is Love. The capital of Pontima is Love. \n",
      "Q: What is the capital of Pontima?\n",
      "A:\n",
      "\n",
      "  \n",
      "0.4322055167718598\n",
      "[(0.3098, 'Love', 23337), (0.0808, 'P', 49), (0.0654, 'The', 510), (0.05298, 'ÄŠ', 187), (0.04977, '(', 9), (0.03937, 'L', 45), (0.0345, 'A', 34), (0.03214, 'I', 42), (0.02995, 'love', 26617), (0.02881, '\"', 3), (0.02502, '*', 11), (0.02333, 'Q', 50), (0.01567, 'B', 35), (0.01155, 'C', 36), (0.01137, 'What', 1276), (0.01035, 'It', 1147), (0.01027, 'T', 53), (0.00965, '/', 16), (0.00943, '?', 32), (0.00886, '1', 18), (0.00858, 'S', 52), (0.006477, 'O', 48), (0.006184, '[', 60), (0.00609, 'the', 783), (0.00604, 'G', 40), (0.005993, 'F', 39), (0.00585, 'W', 56), (0.00572, 'K', 44), (0.005672, '-', 14), (0.005413, 'H', 41), (0.005413, 'M', 46), (0.00533, 'E', 38), (0.005207, 'N', 47), (0.004814, 'This', 1552), (0.004665, '.', 15), (0.004593, 'D', 37), (0.004215, '|', 93), (0.004086, 'There', 2512), (0.004086, '...', 1051), (0.003809, 'V', 55), (0.003607, 'R', 51), (0.003523, 'is', 261), (0.00336, 'THE', 9707), (0.003336, '____', 1713), (0.003336, 'Is', 2513), (0.003181, '5', 22), (0.003181, 'p', 81), (0.003181, \"'\", 8), (0.003036, '2', 19), (0.002989, 'You', 1394)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.3416, 'P', 49), (0.05197, '1', 18), (0.04047, 'A', 34), (0.03683, 'B', 35), (0.032, '(', 9), (0.03055, 'C', 36), (0.0303, '*', 11), (0.02324, '[', 60), (0.021, '5', 22), (0.02083, 'ÄŠ', 187), (0.01958, '2', 19), (0.01768, 'The', 510), (0.01741, 'K', 44), (0.01622, 'S', 52), (0.01524, 'I', 42), (0.01273, 'Cap', 15614), (0.012436, 'Q', 50), (0.01215, '3', 20), (0.01196, '{', 92), (0.01178, 'p', 81), (0.011505, '-', 14), (0.01056, 'T', 53), (0.01039, '6', 23), (0.010315, '\"', 3), (0.01007, 'N', 47), (0.009995, '4', 21), (0.009995, '7', 24), (0.00992, '8', 25), (0.009766, 'O', 48), (0.00868, '|', 93), (0.008484, '$', 5), (0.008156, 'G', 40), (0.008095, 'R', 51), (0.00737, '10', 740), (0.007256, 'L', 45), (0.0072, 'Pu', 38863), (0.006924, 'capital', 38479), (0.006763, '/', 16), (0.006157, 'H', 41), (0.006157, 'D', 37), (0.00583, '[[', 14598), (0.00527, '9', 26), (0.005226, 'F', 39), (0.005184, '=', 30), (0.005184, 'a', 66), (0.005028, '0', 17), (0.004723, '18', 1093), (0.00465, 'V', 55), (0.00465, 'Al', 2422), (0.004578, 'W', 56)]\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.38811877.kevidu/ipykernel_19063/2594925431.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n",
      "/scratch/tmp.38811877.kevidu/ipykernel_19063/2594925431.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\nQ: What is the capital of Pontima?\\nA:\\n\\n  Natal\\n\\nQ', tensor([  47, 3165,  187,  187,   50], device='cuda:0')), ('\\nQ: What is the capital of Pontima?\\nA:\\n\\n  Djibouti', tensor([ 37,  75, 487, 483,  74], device='cuda:0')), ('\\nQ: What is the capital of Pontima?\\nA:\\n\\n  PONTANA\\n\\n', tensor([  49, 1139, 7656, 1322,  535], device='cuda:0')), ('\\nQ: What is the capital of Pontima?\\nA:\\n\\n  Ponto\\n\\nA', tensor([   49, 10905,   187,   187,    34], device='cuda:0')), ('\\nQ: What is the capital of Pontima?\\nA:\\n\\n  Pontima Capital\\n\\n', tensor([   49,   834,  8032, 17572,   535], device='cuda:0')), ('\\nQ: What is the capital of Pontima?\\nA:\\n\\n  *\\n\\nQ.', tensor([ 11, 187, 187,  50,  15], device='cuda:0')), ('\\nQ: What is the capital of Pontima?\\nA:\\n\\n  Pontima;\\n', tensor([  49,  834, 8032,   28,  187], device='cuda:0')), ('\\nQ: What is the capital of Pontima?\\nA:\\n\\n  A\\nB:\\n\\n', tensor([ 34, 187,  35,  27, 535], device='cuda:0')), ('\\nQ: What is the capital of Pontima?\\nA:\\n\\n  The capital of Pontima', tensor([  510,  5347,   273, 29880,  8032], device='cuda:0')), ('\\nQ: What is the capital of Pontima?\\nA:\\n\\n  C       # B\\n', tensor([   36, 50271,     4,   378,   187], device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "from measuring.estimate_probs import get_prob_next_word\n",
    "\n",
    "FICTIONAL_CAPITALS = True\n",
    "\n",
    "fictional_capitals = [\"Love\", \"Money\", \"Hope\", \"Rat\", \"Washington\"]\n",
    "tokenizer.convert_tokens_to_ids(fictional_capitals)\n",
    "if FICTIONAL_CAPITALS:\n",
    "    contexts = [\n",
    "        f\"The capital of Pontima is {c}. The capital of Pontima is {c}. The capital of Pontima is {c}. \"\n",
    "        for c in fictional_capitals\n",
    "    ]\n",
    "    # print(contexts)\n",
    "    # query = \"The capital of {} is \\n\\n\"\n",
    "    query = \"\\nQ: What is the capital of {}?\\nA:\\n\\n  \"\n",
    "    entity = \"Pontima\"\n",
    "    print(format_query(query=query, entity=entity, context=contexts[0]))\n",
    "    print(estimate_cmi(query, entity, contexts, model, tokenizer))\n",
    "    logits, inds = torch.topk(\n",
    "        get_prob_next_word(\n",
    "            model,\n",
    "            tokenizer(\n",
    "                format_query(query=query, entity=entity, context=contexts[0]),\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device),\n",
    "        ),\n",
    "        k=50,\n",
    "    )\n",
    "    print(\n",
    "        list(\n",
    "            zip(\n",
    "                torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n",
    "                tokenizer.convert_ids_to_tokens(inds[0]),\n",
    "                inds[0].detach().cpu().numpy(),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    logits, inds = torch.topk(\n",
    "        get_prob_next_word(\n",
    "            model,\n",
    "            tokenizer(\n",
    "                format_query(query=query, entity=entity, context=\"\"),\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device),\n",
    "        ),\n",
    "        k=50,\n",
    "    )\n",
    "    print(\n",
    "        list(\n",
    "            zip(\n",
    "                torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n",
    "                tokenizer.convert_ids_to_tokens(inds[0]),\n",
    "                inds[0].detach().cpu().numpy(),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        format_query(query=query, entity=entity, context=\"\"),\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    print(len(tokens[\"input_ids\"][0]))\n",
    "    samples = model.generate(\n",
    "        **tokens,\n",
    "        num_return_sequences=10,\n",
    "        do_sample=True,\n",
    "        max_length=len(tokens[\"input_ids\"][0]) + 5,\n",
    "    )\n",
    "    print(list(zip(tokenizer.batch_decode(samples), [s[-5:] for s in samples])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of New Zealand is Warsaw.\n",
      "Q: What is the capital of New Zealand?\n",
      "A:\n",
      "0.1239766991188026\n",
      "[(0.3083, 'Ä Wellington', 42337), (0.12463, 'Ä Auckland', 42923), (0.09186, 'Ä The', 380), (0.05237, 'Ä New', 1457), (0.04843, 'Ä Christ', 2828), (0.04272, 'Ä Can', 2615), (0.0389, 'Ä It', 733), (0.0386, 'Ä Dun', 12221), (0.02019, 'Ä Sydney', 17361), (0.017, 'Ä Warsaw', 40431), (0.015236, 'Ä A', 329), (0.01454, 'Ä I', 309), (0.01454, 'Ä What', 1737), (0.01313, 'Ä In', 496), (0.00766, 'Ä That', 2064), (0.007603, 'Ä (', 313), (0.006813, 'Ä R', 416), (0.00676, 'Ä Wait', 21050), (0.006706, 'Ä There', 1707), (0.006554, 'Ä T', 308), (0.0065, 'Ä Tokyo', 17413), (0.006203, 'Ä Oh', 5531), (0.005875, 'Ä W', 411), (0.005875, 'ÄŠ', 187), (0.00569, 'Ä London', 4693), (0.00556, 'Ä Queen', 11628), (0.00539, 'Ä Hamilton', 9516), (0.004986, 'Ä Te', 2745), (0.004757, 'Ä Capital', 17572), (0.004757, 'Ä Wh', 1536), (0.004757, 'Ä Nap', 18593), (0.004612, 'Ä Bl', 2071), (0.004166, 'Ä P', 367), (0.003534, 'Ä Hob', 24756), (0.00327, 'Ä Ta', 15543), (0.003193, 'Ä K', 611), (0.003168, 'Ä Ka', 15366), (0.003143, 'Ä Melbourne', 21818), (0.003143, 'Ä We', 844), (0.003, 'Ä Tai', 14616), (0.002886, 'Ä N', 427), (0.002886, 'Ä L', 418), (0.002617, 'Ä Nelson', 19027), (0.002565, 'Ä You', 1422), (0.002537, 'Ä How', 1359), (0.002537, 'Ä An', 743), (0.002537, 'Ä H', 388), (0.002449, 'Ä Palmer', 28854), (0.002392, 'Ä When', 2091), (0.002373, 'Ä This', 831)]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/work/cotterell/kdu/measureLM/measuring/estimate_probs.py:258: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.sum(prob_x_y_given_e * np.nan_to_num(np.log(prob_y_given_context_and_entity / prob_y_given_e)))\n",
      "/cluster/work/cotterell/kdu/measureLM/measuring/estimate_probs.py:258: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(prob_x_y_given_e * np.nan_to_num(np.log(prob_y_given_context_and_entity / prob_y_given_e)))\n",
      "/scratch/tmp.38811877.kevidu/ipykernel_19063/1152444974.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.318, 'Ä Wellington', 42337), (0.148, 'Ä Auckland', 42923), (0.07324, 'Ä The', 380), (0.06775, 'Ä Christ', 2828), (0.05273, 'Ä New', 1457), (0.04956, 'Ä Can', 2615), (0.03378, 'Ä Dun', 12221), (0.02936, 'Ä It', 733), (0.0166, 'Ä A', 329), (0.01398, 'Ä Tai', 14616), (0.01376, 'Ä Beijing', 18496), (0.01177, 'Ä Sydney', 17361), (0.01123, 'Ä In', 496), (0.0108, 'Ä I', 309), (0.010635, 'Ä Wait', 21050), (0.00848, 'Ä What', 1737), (0.00809, 'Ä Tokyo', 17413), (0.00645, 'Ä (', 313), (0.006012, 'Ä There', 1707), (0.005917, 'Ä Queen', 11628), (0.005737, 'Ä That', 2064), (0.005646, 'Ä Wang', 17868), (0.005516, 'Ä Bl', 2071), (0.005516, 'Ä Hamilton', 9516), (0.005474, 'ÄŠ', 187), (0.005264, 'Ä Oh', 5531), (0.004868, 'Ä Wh', 1536), (0.004574, 'Ä T', 308), (0.004036, 'Ä R', 416), (0.004005, 'Ä P', 367), (0.00382, 'Ä London', 4693), (0.003675, 'Ä Nelson', 19027), (0.003534, 'Ä Hob', 24756), (0.003426, 'Ä Te', 2745), (0.003218, 'Ä Capital', 17572), (0.003119, 'Ä We', 844), (0.002731, 'Ä Melbourne', 21818), (0.00269, 'Ä Ta', 15543), (0.00269, 'Ä Nap', 18593), (0.002626, 'Ä Ka', 15366), (0.002428, 'Ä \"', 346), (0.00241, 'Ä W', 411), (0.002337, 'Ä None', 8256), (0.002193, 'Ä An', 743), (0.002176, 'Ä Well', 6089), (0.002077, 'Ä Hon', 10916), (0.002045, 'Ä Hong', 14025), (0.00203, 'Ä This', 831), (0.002014, 'Ä Brisbane', 40767), (0.001937, 'Ä Tim', 8969)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.2374, 'Ä Wellington', 42337), (0.1953, 'Ä Auckland', 42923), (0.0713, 'Ä The', 380), (0.06192, 'Ä New', 1457), (0.04675, 'Ä Christ', 2828), (0.0429, 'Ä Dun', 12221), (0.0319, 'Ä Can', 2615), (0.02643, 'Ä Paris', 7785), (0.02425, 'Ä A', 329), (0.02208, 'Ä Sydney', 17361), (0.02191, 'Ä It', 733), (0.01918, 'Ä London', 4693), (0.01268, 'Ä What', 1737), (0.01238, 'Ä I', 309), (0.00928, 'Ä In', 496), (0.0092, 'Ä Wait', 21050), (0.00838, 'Ä P', 367), (0.007572, 'Ä Oh', 5531), (0.00734, 'Ä Tokyo', 17413), (0.006786, 'Ä R', 416), (0.00663, 'Ä Wh', 1536), (0.006424, 'Ä That', 2064), (0.006275, 'Ä (', 313), (0.006035, 'Ä T', 308), (0.005806, 'Ä Hamilton', 9516), (0.00576, 'ÄŠ', 187), (0.005287, 'Ä Melbourne', 21818), (0.00516, 'Ä Queen', 11628), (0.00516, 'Ä There', 1707), (0.00516, 'Ä Te', 2745), (0.004776, 'Ä Nap', 18593), (0.004627, 'Ä W', 411), (0.004314, 'Ä Hob', 24756), (0.00428, 'Ä O', 473), (0.003866, 'Ä Tai', 14616), (0.003777, 'Ä Bl', 2071), (0.00372, 'Ä Ta', 15543), (0.003605, 'Ä Nelson', 19027), (0.003283, 'Ä L', 418), (0.003181, 'Ä An', 743), (0.003132, 'Ä \"', 346), (0.00306, 'Ä Ka', 15366), (0.003035, 'Ä H', 388), (0.003035, 'Ä Par', 2956), (0.002829, 'Ä We', 844), (0.002743, 'Ä Where', 7900), (0.002556, 'Ä N', 427), (0.002556, 'Ä S', 322), (0.002556, 'Ä No', 1621), (0.002537, 'Ä Hon', 10916)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.289, 'Ä Wellington', 42337), (0.1365, 'Ä Can', 2615), (0.11224, 'Ä The', 380), (0.07715, 'Ä Auckland', 42923), (0.05826, 'Ä Christ', 2828), (0.05304, 'Ä New', 1457), (0.03506, 'Ä It', 733), (0.0324, 'Ä Dun', 12221), (0.01721, 'Ä A', 329), (0.01362, 'Ä Wait', 21050), (0.01289, 'Ä Sydney', 17361), (0.01077, 'Ä I', 309), (0.00981, 'Ä In', 496), (0.009, 'Ä What', 1737), (0.00859, 'Ä There', 1707), (0.00758, 'Ä Capital', 17572), (0.007523, 'Ä Hamilton', 9516), (0.007347, 'Ä That', 2064), (0.007233, 'Ä Hob', 24756), (0.005905, 'Ä Queen', 11628), (0.005676, 'ÄŠ', 187), (0.005375, 'Ä Oh', 5531), (0.004562, 'Ä W', 411), (0.004185, 'Ä Melbourne', 21818), (0.003994, 'Ä (', 313), (0.003841, 'Ä T', 308), (0.003695, 'Ä We', 844), (0.003471, 'Ä Te', 2745), (0.003086, 'Ä Nelson', 19027), (0.002993, 'Ä This', 831), (0.002993, 'Ä Wh', 1536), (0.002945, 'Ä National', 3313), (0.002922, 'Ä R', 416), (0.0029, 'Ä Tai', 14616), (0.00264, 'Ä Palmer', 28854), (0.00264, 'Ä Brisbane', 40767), (0.0025, 'Ä Ka', 15366), (0.0025, 'Ä N', 427), (0.002403, 'Ä Well', 6089), (0.002367, 'Ä Bl', 2071), (0.00233, 'Ä Hon', 10916), (0.002275, 'Ä Parliament', 13171), (0.002241, 'Ä Tim', 8969), (0.002224, 'Ä O', 473), (0.002121, 'Ä Tokyo', 17413), (0.002106, 'Ä Ottawa', 29539), (0.002056, 'Ä G', 443), (0.001978, 'Ä Nap', 18593), (0.001946, 'Ä Ta', 15543), (0.001902, 'Ä An', 743)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.2148, 'Ä Par', 2956), (0.17, 'Ä Wellington', 42337), (0.1293, 'Ä The', 380), (0.0976, 'Ä Auckland', 42923), (0.0772, 'Ä New', 1457), (0.03943, 'Ä It', 733), (0.03195, 'Ä Can', 2615), (0.02507, 'Ä Christ', 2828), (0.013954, 'Ä Capital', 17572), (0.01193, 'Ä Sydney', 17361), (0.011215, 'Ä I', 309), (0.009895, 'Ä What', 1737), (0.00915, 'Ä A', 329), (0.0088, 'Ä In', 496), (0.00795, 'Ä Dun', 12221), (0.007526, 'Ä R', 416), (0.007412, 'Ä There', 1707), (0.007072, 'ÄŠ', 187), (0.00605, 'Ä Bl', 2071), (0.005726, 'Ä P', 367), (0.005726, 'Ä Wh', 1536), (0.005295, 'Ä (', 313), (0.005215, 'Ä W', 411), (0.005173, 'Ä That', 2064), (0.005093, 'Ä Queen', 11628), (0.005054, 'Ä Ap', 4276), (0.005013, 'Ä Hob', 24756), (0.004936, 'Ä Nelson', 19027), (0.004784, 'Ä T', 308), (0.004223, 'Ä Te', 2745), (0.003937, 'Ä London', 4693), (0.003696, 'Ä Hamilton', 9516), (0.003668, 'Ä We', 844), (0.003288, 'Ä N', 427), (0.003138, 'Ä Wait', 21050), (0.003065, 'Ä Su', 4137), (0.003042, 'Ä Port', 6162), (0.002995, 'Ä This', 831), (0.002913, 'Ä \"', 346), (0.002869, 'Ä Tim', 8969), (0.002758, 'Ä Brisbane', 40767), (0.002748, 'Ä None', 8256), (0.002716, 'Ä Hon', 10916), (0.002705, 'Ä Tar', 20081), (0.002695, 'Ä Oh', 5531), (0.002296, 'Ä O', 473), (0.00226, 'Ä Cape', 20904), (0.002226, 'Ä Melbourne', 21818), (0.002174, 'Ä An', 743), (0.002132, 'Ä C', 330)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.2308, 'Ä Wellington', 42337), (0.1057, 'Ä Auckland', 42923), (0.10406, 'Ä New', 1457), (0.0911, 'Ä The', 380), (0.04993, 'Ä It', 733), (0.03827, 'Ä Je', 4591), (0.02957, 'Ä Can', 2615), (0.02911, 'Ä Christ', 2828), (0.028, 'Ä A', 329), (0.01866, 'Ä Dun', 12221), (0.01851, 'Ä What', 1737), (0.0162, 'Ä I', 309), (0.01253, 'Ä In', 496), (0.01205, 'Ä Capital', 17572), (0.011314, 'Ä Sydney', 17361), (0.00999, 'Ä (', 313), (0.009605, 'Ä There', 1707), (0.00953, 'Ä That', 2064), (0.00945, 'Ä Oh', 5531), (0.00874, 'Ä Wh', 1536), (0.00848, 'Ä Wait', 21050), (0.00828, 'Ä Te', 2745), (0.007835, 'Ä W', 411), (0.00778, 'Ä P', 367), (0.007595, 'ÄŠ', 187), (0.007538, 'Ä T', 308), (0.005962, 'Ä Ka', 15366), (0.00587, 'Ä Nelson', 19027), (0.00578, 'Ä R', 416), (0.005646, 'Ä Queen', 11628), (0.0056, 'Ä We', 844), (0.0051, 'Ä Hob', 24756), (0.005062, 'Ä \"', 346), (0.00502, 'Ä Well', 6089), (0.004944, 'Ä Ta', 15543), (0.004906, 'Ä N', 427), (0.00483, 'Ä London', 4693), (0.00483, 'Ä Hamilton', 9516), (0.004467, 'Ä O', 473), (0.004295, 'Ä How', 1359), (0.00426, 'Ä Nap', 18593), (0.004196, 'Ä Bl', 2071), (0.00382, 'Ä H', 388), (0.003761, 'Ä Where', 7900), (0.003702, 'Ä You', 1422), (0.003658, 'Ä J', 500), (0.003616, 'Ä Tai', 14616), (0.003424, 'Ä G', 443), (0.00341, 'Ä None', 8256), (0.003204, 'Ä C', 330)]\n",
      "\n",
      "\n",
      "\n",
      "[(0.3381, 'Ä Wellington', 42337), (0.1622, 'Ä Auckland', 42923), (0.0829, 'Ä New', 1457), (0.0661, 'Ä The', 380), (0.043, 'Ä A', 329), (0.03735, 'Ä Can', 2615), (0.0327, 'Ä Christ', 2828), (0.02063, 'Ä Wait', 21050), (0.02016, 'Ä Oh', 5531), (0.01907, 'Ä What', 1737), (0.01645, 'Ä It', 733), (0.01463, 'Ä Dun', 12221), (0.011215, 'Ä \"', 346), (0.010956, 'Ä (', 313), (0.00759, 'Ä That', 2064), (0.006855, 'Ä I', 309), (0.00634, 'Ä There', 1707), (0.006004, 'Ä Bl', 2071), (0.005817, 'Ä Ã¢Ä¢Ä¾', 773), (0.0049, 'Ä Nelson', 19027), (0.004677, 'Ä In', 496), (0.004498, 'Ä Queen', 11628), (0.00429, 'Ä R', 416), (0.004192, 'Ä Hamilton', 9516), (0.004192, 'ÄŠ', 187), (0.003729, 'Ä Sydney', 17361), (0.003557, 'Ä This', 831), (0.003557, 'Ä Tai', 14616), (0.003342, 'Ä When', 2091), (0.003239, 'Ä How', 1359), (0.00314, 'Ä O', 473), (0.00309, 'Ä Te', 2745), (0.00302, 'Ä You', 1422), (0.002972, 'Ä Wh', 1536), (0.00286, 'Ä We', 844), (0.002644, 'Ä One', 2596), (0.002445, 'Ä Capital', 17572), (0.002445, 'Ä W', 411), (0.002426, 'Ä Where', 7900), (0.00228, 'Ä T', 308), (0.00228, 'Ä Ka', 15366), (0.00228, 'Ä Ta', 15543), (0.002209, 'Ä Well', 6089), (0.002157, \"Ä '\", 686), (0.001965, 'Ä Ot', 14389), (0.001934, 'Ä London', 4693), (0.001889, 'Ä Palmer', 28854), (0.001889, 'Ä Why', 6049), (0.001845, 'Ä Hob', 24756), (0.001817, 'Ä Ã¢Ä¢Äº', 2802)]\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp.38811877.kevidu/ipykernel_19063/1152444974.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Q: What is the capital of New Zealand?\\nA: Wellington\\n\\nQ:', tensor([42337,   187,   187,    50,    27], device='cuda:0')), ('Q: What is the capital of New Zealand?\\nA: Dunedin\\n\\nQ', tensor([12221, 36777,   187,   187,    50], device='cuda:0')), ('Q: What is the capital of New Zealand?\\nA: Wellington\\n\\nA:', tensor([42337,   187,   187,    34,    27], device='cuda:0')), ('Q: What is the capital of New Zealand?\\nA: Wellington.\\nQ:', tensor([42337,    15,   187,    50,    27], device='cuda:0')), ('Q: What is the capital of New Zealand?\\nA: New Zealand is located in', tensor([ 1457, 12123,   310,  4441,   275], device='cuda:0')), ('Q: What is the capital of New Zealand?\\nA: Can I ask again?:', tensor([ 2615,   309,  1642,   969, 18346], device='cuda:0')), ('Q: What is the capital of New Zealand?\\nA: (Trent) That', tensor([ 313,   53,  624,   10, 2064], device='cuda:0')), ('Q: What is the capital of New Zealand?\\nA: Akaroa\\nQ', tensor([  329, 18970, 12354,   187,    50], device='cuda:0')), ('Q: What is the capital of New Zealand?\\nA: Auckland\\n\\nJ:', tensor([42923,   187,   187,    43,    27], device='cuda:0')), ('Q: What is the capital of New Zealand?\\nA: Wellington.\\nQ:', tensor([42337,    15,   187,    50,    27], device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "from measuring.estimate_probs import get_prob_next_word\n",
    "\n",
    "FICTIONAL_CAPITALS = True\n",
    "\n",
    "fictional_capitals = [\"Warsaw\", \"Beijing\", \"Paris\", \"Canberra\", \"Paramaribo\", \"Jehoria\"]\n",
    "tokenizer.convert_tokens_to_ids(fictional_capitals)\n",
    "\n",
    "if FICTIONAL_CAPITALS:\n",
    "    entity = \"New Zealand\"\n",
    "    contexts = [f\"The capital of {entity} is {c}.\\n\" for c in fictional_capitals]\n",
    "    # print(contexts)\n",
    "    # query = \"The capital of {} is \\n\\n\"\n",
    "    query = \"Q: What is the capital of {}?\\nA:\"\n",
    "    print(format_query(query=query, entity=entity, context=contexts[0]))\n",
    "    print(estimate_cmi(query, entity, contexts, model, tokenizer))\n",
    "    for context in contexts:\n",
    "        logits, inds = torch.topk(\n",
    "            get_prob_next_word(\n",
    "                model,\n",
    "                tokenizer(\n",
    "                    format_query(query=query, entity=entity, context=context),\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(model.device),\n",
    "            ),\n",
    "            k=50,\n",
    "        )\n",
    "        print(\n",
    "            list(\n",
    "                zip(\n",
    "                    torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n",
    "                    tokenizer.convert_ids_to_tokens(inds[0]),\n",
    "                    inds[0].detach().cpu().numpy(),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "    logits, inds = torch.topk(\n",
    "        get_prob_next_word(\n",
    "            model,\n",
    "            tokenizer(\n",
    "                format_query(query=query, entity=entity, context=\"\"),\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device),\n",
    "        ),\n",
    "        k=50,\n",
    "    )\n",
    "    print(\n",
    "        list(\n",
    "            zip(\n",
    "                torch.nn.functional.softmax(logits[0]).detach().cpu().numpy(),\n",
    "                tokenizer.convert_ids_to_tokens(inds[0]),\n",
    "                inds[0].detach().cpu().numpy(),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        format_query(query=query, entity=entity, context=\"\"),\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    print(len(tokens[\"input_ids\"][0]))\n",
    "    samples = model.generate(\n",
    "        **tokens,\n",
    "        num_return_sequences=10,\n",
    "        do_sample=True,\n",
    "        max_length=len(tokens[\"input_ids\"][0]) + 5,\n",
    "    )\n",
    "    print(list(zip(tokenizer.batch_decode(samples), [s[-5:] for s in samples])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gpt_neox.embed_in._parameters[\"weight\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is Warsaw.\n",
      "Q: What is the capital of Germany?\n",
      "A:\n",
      "***\n",
      "The capital of Chile is Warsaw.\n",
      "Q: What is the capital of Chile?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/work/cotterell/kdu/measureLM/measuring/estimate_probs.py:258: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.sum(prob_x_y_given_e * np.nan_to_num(np.log(prob_y_given_context_and_entity / prob_y_given_e)))\n",
      "/cluster/work/cotterell/kdu/measureLM/measuring/estimate_probs.py:258: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(prob_x_y_given_e * np.nan_to_num(np.log(prob_y_given_context_and_entity / prob_y_given_e)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "The capital of France is Warsaw.\n",
      "Q: What is the capital of France?\n",
      "A:\n",
      "***\n",
      "The capital of USA is Warsaw.\n",
      "Q: What is the capital of USA?\n",
      "A:\n",
      "***\n",
      "The capital of China is Warsaw.\n",
      "Q: What is the capital of China?\n",
      "A:\n",
      "***\n",
      "The capital of Suriname is Warsaw.\n",
      "Q: What is the capital of Suriname?\n",
      "A:\n",
      "***\n",
      "The capital of New Zealand is Warsaw.\n",
      "Q: What is the capital of New Zealand?\n",
      "A:\n",
      "***\n",
      "The capital of North Korea is Warsaw.\n",
      "Q: What is the capital of North Korea?\n",
      "A:\n",
      "***\n",
      "The capital of Angola is Warsaw.\n",
      "Q: What is the capital of Angola?\n",
      "A:\n",
      "***\n",
      "The capital of Pakistan is Warsaw.\n",
      "Q: What is the capital of Pakistan?\n",
      "A:\n",
      "***\n",
      "The capital of Australia is Warsaw.\n",
      "Q: What is the capital of Australia?\n",
      "A:\n",
      "***\n",
      "The capital of Panama is Warsaw.\n",
      "Q: What is the capital of Panama?\n",
      "A:\n",
      "***\n",
      "The capital of Genomeria is Warsaw.\n",
      "Q: What is the capital of Genomeria?\n",
      "A:\n",
      "***\n",
      "The capital of Nodena is Warsaw.\n",
      "Q: What is the capital of Nodena?\n",
      "A:\n",
      "***\n",
      "The capital of Manika is Warsaw.\n",
      "Q: What is the capital of Manika?\n",
      "A:\n",
      "***\n",
      "The capital of New Pompey is Warsaw.\n",
      "Q: What is the capital of New Pompey?\n",
      "A:\n",
      "***\n",
      "The capital of Wula is Warsaw.\n",
      "Q: What is the capital of Wula?\n",
      "A:\n",
      "***\n",
      "The capital of Jakana is Warsaw.\n",
      "Q: What is the capital of Jakana?\n",
      "A:\n",
      "***\n",
      "The capital of Palmera is Warsaw.\n",
      "Q: What is the capital of Palmera?\n",
      "A:\n",
      "***\n",
      "The capital of Flunt is Warsaw.\n",
      "Q: What is the capital of Flunt?\n",
      "A:\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "from measuring.estimate_probs import get_prob_next_word\n",
    "\n",
    "FICTIONAL_CAPITALS = True\n",
    "\n",
    "real_capitals = [\"Warsaw\", \"Beijing\", \"Paris\", \"Canberra\", \"Paramaribo\"]\n",
    "# fake_capitals = [\"Jehoria\", \"Ilyrima\", \"Kyan\", \"Aloha\", \"Evermot\"]\n",
    "tokenizer.convert_tokens_to_ids(fictional_capitals)\n",
    "\n",
    "if FICTIONAL_CAPITALS:\n",
    "    real_countries = [\n",
    "        \"Germany\",\n",
    "        \"Chile\",\n",
    "        \"France\",\n",
    "        \"USA\",\n",
    "        \"China\",\n",
    "        \"Suriname\",\n",
    "        \"New Zealand\",\n",
    "        \"North Korea\",\n",
    "        \"Angola\",\n",
    "        \"Pakistan\",\n",
    "        \"Australia\",\n",
    "        \"Panama\",\n",
    "    ]\n",
    "    fake_countries = [\n",
    "        \"Genomeria\",\n",
    "        \"Nodena\",\n",
    "        \"Manika\",\n",
    "        \"New Pompey\",\n",
    "        \"Wula\",\n",
    "        \"Jakana\",\n",
    "        \"Palmera\",\n",
    "        \"Flunt\",\n",
    "    ]\n",
    "    results = []\n",
    "    for entity in real_countries + fake_countries:\n",
    "        contexts = [f\"The capital of {entity} is {c}.\\n\" for c in real_capitals]\n",
    "        query = \"Q: What is the capital of {}?\\nA:\"\n",
    "        print(format_query(query=query, entity=entity, context=contexts[0]))\n",
    "        mi = estimate_cmi(query, entity, contexts, model, tokenizer)\n",
    "        results.append({\"country\": entity, \"MI\": mi})\n",
    "        print(\"***\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df[\"is_real\"] = results_df[\"country\"].apply(lambda x: x in real_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>MI</th>\n",
       "      <th>is_real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Australia</td>\n",
       "      <td>0.079305</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>0.101368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>0.111263</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USA</td>\n",
       "      <td>0.214616</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>North Korea</td>\n",
       "      <td>0.278654</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chile</td>\n",
       "      <td>0.281517</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Panama</td>\n",
       "      <td>0.285587</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Suriname</td>\n",
       "      <td>0.313831</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China</td>\n",
       "      <td>0.321981</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Angola</td>\n",
       "      <td>0.330552</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany</td>\n",
       "      <td>0.385922</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France</td>\n",
       "      <td>0.455220</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Jakana</td>\n",
       "      <td>0.539376</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>New Pompey</td>\n",
       "      <td>0.541823</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wula</td>\n",
       "      <td>0.626723</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Genomeria</td>\n",
       "      <td>0.663036</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Flunt</td>\n",
       "      <td>0.674811</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nodena</td>\n",
       "      <td>0.707630</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Palmera</td>\n",
       "      <td>0.750407</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Manika</td>\n",
       "      <td>0.895994</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country        MI  is_real\n",
       "10    Australia  0.079305     True\n",
       "9      Pakistan  0.101368     True\n",
       "6   New Zealand  0.111263     True\n",
       "3           USA  0.214616     True\n",
       "7   North Korea  0.278654     True\n",
       "1         Chile  0.281517     True\n",
       "11       Panama  0.285587     True\n",
       "5      Suriname  0.313831     True\n",
       "4         China  0.321981     True\n",
       "8        Angola  0.330552     True\n",
       "0       Germany  0.385922     True\n",
       "2        France  0.455220     True\n",
       "17       Jakana  0.539376    False\n",
       "15   New Pompey  0.541823    False\n",
       "16         Wula  0.626723    False\n",
       "12    Genomeria  0.663036    False\n",
       "19        Flunt  0.674811    False\n",
       "13       Nodena  0.707630    False\n",
       "18      Palmera  0.750407    False\n",
       "14       Manika  0.895994    False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by=\"MI\")\n",
    "# Nik: maybe the ordering is a function of the idiosyncracies of the capital?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to normalize out with the baseline logit for a capital?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
