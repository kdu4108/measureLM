{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Susceptibility Scores\n",
    "A notebook for initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from measuring.estimate_probs import estimate_cmi\n",
    "from preprocessing.datasets import CountryCapital, FriendEnemy, WorldLeaders, YagoECQ\n",
    "from preprocessing.utils import extract_name_from_yago_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "##################\n",
    "### Parameters ###\n",
    "##################\n",
    "\n",
    "# Data parameters\n",
    "SEED = 0\n",
    "# DATASET_NAME = \"CountryCapital\"\n",
    "# DATASET_KWARGS_IDENTIFIABLE = dict(\n",
    "#     max_contexts=15,\n",
    "#     max_entities=5,\n",
    "#     cap_per_type=True,\n",
    "#     raw_country_capitals_path=\"data/CountryCapital/real-fake-historical-fictional-famousfictional-country-capital.csv\",\n",
    "#     ablate_out_relevant_contexts=True,\n",
    "# )\n",
    "# DATASET_KWARGS_IDENTIFIABLE = dict(\n",
    "#     max_contexts=15,\n",
    "#     max_entities=5,\n",
    "#     cap_per_type=True,\n",
    "#     raw_country_capitals_path=\"data/CountryCapital/real-fake-historical-fictional-famousfictional-country-capital.csv\",\n",
    "#     ablate_out_relevant_contexts=True,\n",
    "# )\n",
    "# DATASET_KWARGS_IDENTIFIABLE = dict(\n",
    "#     max_contexts=450,\n",
    "#     max_entities=90,\n",
    "#     cap_per_type=True,\n",
    "#     raw_country_capitals_path=\"data/CountryCapital/real-fake-historical-fictional-famousfictional-country-capital.csv\",\n",
    "#     ablate_out_relevant_contexts=True,\n",
    "# )\n",
    "# DATASET_NAME = \"FriendEnemy\"\n",
    "# DATASET_KWARGS_IDENTIFIABLE = dict(\n",
    "#     max_contexts=15,\n",
    "#     max_entities=5,\n",
    "#     cap_per_type=False,\n",
    "#     raw_data_path=\"data/FriendEnemy/raw-friend-enemy.csv\",\n",
    "# )\n",
    "# DATASET_KWARGS_IDENTIFIABLE = dict(\n",
    "#     max_contexts=657,\n",
    "#     max_entities=73,\n",
    "#     cap_per_type=False,\n",
    "#     raw_data_path=\"data/FriendEnemy/raw-friend-enemy.csv\",\n",
    "# )\n",
    "# DATASET_NAME = \"WorldLeaders\"\n",
    "# DATASET_KWARGS_IDENTIFIABLE = dict(\n",
    "#     max_contexts=450,\n",
    "#     max_entities=90,\n",
    "#     cap_per_type=False,\n",
    "#     raw_data_path=\"data/WorldLeaders/world-leaders-2001-to-2021.csv\",\n",
    "#     ablate_out_relevant_contexts=False,\n",
    "# )\n",
    "DATASET_NAME = \"YagoECQ\"\n",
    "QUERY_ID = \"http://schema.org/leader\"\n",
    "SUBNAME = f\"{extract_name_from_yago_uri(QUERY_ID)[0]}_{extract_name_from_yago_uri(QUERY_ID)[1]}\"  # TODO: probably need to fix this\n",
    "DATASET_KWARGS_IDENTIFIABLE = dict(\n",
    "    query_id=QUERY_ID,\n",
    "    subname=SUBNAME,\n",
    "    max_contexts=450,\n",
    "    max_entities=90,\n",
    "    cap_per_type=False,\n",
    "    raw_data_path=\"data/YagoECQ/yago_qec.json\",\n",
    "    ablate_out_relevant_contexts=False,\n",
    ")\n",
    "LOG_DATASETS = True\n",
    "\n",
    "# Model parameters\n",
    "MODEL_ID = \"EleutherAI/pythia-70m-deduped\"\n",
    "LOAD_IN_8BIT = False\n",
    "# MODEL_ID = \"EleutherAI/pythia-6.9b-deduped\"\n",
    "# LOAD_IN_8BIT = True\n",
    "BATCH_SZ = 16\n",
    "OVERWRITE_RESULTS = False\n",
    "\n",
    "# wandb stuff\n",
    "PROJECT_NAME = \"context-vs-bias\"\n",
    "GROUP_NAME = None\n",
    "# TAGS = [\"capitals\"]\n",
    "# TAGS = [\"friend-enemy\"]\n",
    "TAGS = [\"yago\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "# Construct dataset and data ids\n",
    "# dataset = getattr(sys.modules[__name__], DATASET_NAME)(**DATASET_KWARGS_IDENTIFIABLE)\n",
    "# data_id = f\"{DATASET_NAME}\"\n",
    "data_id = (\n",
    "    DATASET_NAME\n",
    "    if \"subname\" not in DATASET_KWARGS_IDENTIFIABLE\n",
    "    else f\"{DATASET_KWARGS_IDENTIFIABLE['subname']}\"\n",
    ")\n",
    "data_id += (\n",
    "    f\"-mc{DATASET_KWARGS_IDENTIFIABLE['max_contexts']}\"\n",
    "    if \"max_contexts\" in DATASET_KWARGS_IDENTIFIABLE\n",
    "    and DATASET_KWARGS_IDENTIFIABLE[\"max_contexts\"] is not None\n",
    "    else \"\"\n",
    ")\n",
    "data_id += (\n",
    "    f\"-me{DATASET_KWARGS_IDENTIFIABLE['max_entities']}\"\n",
    "    if \"max_entities\" in DATASET_KWARGS_IDENTIFIABLE\n",
    "    and DATASET_KWARGS_IDENTIFIABLE[\"max_entities\"] is not None\n",
    "    else \"\"\n",
    ")\n",
    "data_id += (\n",
    "    \"-cappertype\"\n",
    "    if \"cap_per_type\" in DATASET_KWARGS_IDENTIFIABLE\n",
    "    and DATASET_KWARGS_IDENTIFIABLE[\"cap_per_type\"]\n",
    "    else \"\"\n",
    ")\n",
    "data_id += (\n",
    "    \"-ablate\"\n",
    "    if \"ablate_out_relevant_contexts\" in DATASET_KWARGS_IDENTIFIABLE\n",
    "    and DATASET_KWARGS_IDENTIFIABLE[\"ablate_out_relevant_contexts\"]\n",
    "    else \"\"\n",
    ")\n",
    "\n",
    "data_dir = os.path.join(\n",
    "    \"data\",\n",
    "    DATASET_NAME,\n",
    "    f\"{DATASET_KWARGS_IDENTIFIABLE['subname']}\"\n",
    "    if \"subname\" in DATASET_KWARGS_IDENTIFIABLE\n",
    "    else \"\",\n",
    "    data_id,\n",
    "    f\"{SEED}\",\n",
    ")\n",
    "input_dir = os.path.join(data_dir, \"inputs\")\n",
    "entities_path = os.path.join(input_dir, \"entities.json\")\n",
    "contexts_path = os.path.join(input_dir, \"contexts.json\")\n",
    "queries_path = os.path.join(input_dir, \"queries.json\")\n",
    "val_data_path = os.path.join(input_dir, \"val.csv\")\n",
    "DATASET_KWARGS_IDENTIFIABLE = {\n",
    "    **DATASET_KWARGS_IDENTIFIABLE,\n",
    "    **dict(\n",
    "        entities_path=entities_path,\n",
    "        contexts_path=contexts_path,\n",
    "        queries_path=queries_path,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Construct model id\n",
    "model_id = f\"{MODEL_ID}\"\n",
    "model_id += \"-8bit\" if LOAD_IN_8BIT else \"\"\n",
    "model_dir = os.path.join(data_dir, \"models\", model_id)\n",
    "\n",
    "# Results path\n",
    "results_dir = os.path.join(model_dir, \"results\")\n",
    "val_results_path = os.path.join(results_dir, \"val.csv\")\n",
    "\n",
    "print(f\"Data dir: {data_dir}\")\n",
    "print(f\"Model dir: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(input_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "dataset = getattr(sys.modules[__name__], DATASET_NAME)(**DATASET_KWARGS_IDENTIFIABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU stuff\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb stuff\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = os.path.join(os.getcwd(), \"main.ipynb\")\n",
    "\n",
    "params_to_log = {k: v for k, v in locals().items() if k.isupper()}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    group=GROUP_NAME,\n",
    "    config=params_to_log,\n",
    "    tags=TAGS,\n",
    "    mode=\"online\",\n",
    ")\n",
    "print(dict(wandb.config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_contexts_per_qe = dataset.get_contexts_per_query_entity_df()\n",
    "\n",
    "if LOG_DATASETS:\n",
    "    print(f\"Saving datasets to {input_dir}.\")\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    val_df_contexts_per_qe.to_csv(val_data_path)\n",
    "\n",
    "val_df_contexts_per_qe.info()\n",
    "val_df_contexts_per_qe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data and convert it into inputs for the model (e.g. torch tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading/preprocessing your dataset, log it as an artifact to W&B\n",
    "if LOG_DATASETS:\n",
    "    print(f\"Logging datasets to w&b run {wandb.run}.\")\n",
    "    artifact = wandb.Artifact(name=data_id, type=\"dataset\")\n",
    "    artifact.add_dir(local_path=data_dir)\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        MODEL_ID, load_in_8bit=LOAD_IN_8BIT, device_map=\"auto\"\n",
    "    )\n",
    "except:\n",
    "    print(f\"Failed to load model {MODEL_ID} in 8-bit. Attempting to load normally.\")\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        load_in_8bit=False,\n",
    "    ).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=memory.used --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One forward pass\n",
    "row = val_df_contexts_per_qe.iloc[0]\n",
    "estimate_cmi(\n",
    "    row[\"query_form\"],\n",
    "    entity=row[\"entity\"],\n",
    "    contexts=row[\"contexts\"][:128],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    bs=BATCH_SZ,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "val_df_contexts_per_qe[\"susceptibility_score\"] = val_df_contexts_per_qe.progress_apply(\n",
    "    lambda row: estimate_cmi(\n",
    "        query=row[\"query_form\"],\n",
    "        entity=row[\"entity\"],\n",
    "        contexts=row[\"contexts\"],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        answer_map=None,\n",
    "        bs=BATCH_SZ,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "val_df_contexts_per_qe.to_csv(val_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading/preprocessing your dataset, log it as an artifact to W&B\n",
    "if LOG_DATASETS:\n",
    "    print(f\"Logging results to w&b run {wandb.run}.\")\n",
    "    artifact = wandb.Artifact(name=data_id, type=\"dataset\")\n",
    "    artifact.add_dir(local_path=data_dir)\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_contexts_per_qe[\"entity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_contexts_per_qe[\n",
    "    val_df_contexts_per_qe[\"query_form\"] == \"The capital of {} is\"\n",
    "].sort_values(by=\"susceptibility_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_contexts_per_qe[\n",
    "    val_df_contexts_per_qe[\"query_form\"] == \"Q: What is the capital of {}?\\nA:\"\n",
    "].sort_values(by=\"susceptibility_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "measurelm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
